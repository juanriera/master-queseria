[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notas para el master de quesería",
    "section": "",
    "text": "1 Introducción general\nEste documento trata sobre el análisis de datos industriales utilizando tanto la hoja de cálculo como un lenguaje de programación moderno como es Python . El uso de las técnicas estadísticas se ha reducido al mínimo, orientando el análisis al uso de métodos gráficos descriptivos sencillos.\n¿Por qué usar Python como herramienta? Hay varias razones que parecen oportunas:\n\nPython es actualmente el lenguaje de programación más extendido y el más utilizado en los nuevos campos de la inteligencia artificial (IA). Su conocimiento, aunque sea de forma básica, aportará un plus a los alumnos en su curriculum.\nEl Principado de Asturias ha establecido una línea prioritaria de desarrollo de conocimientos en las áreas relacionadas con la IA para los estudiantes de Formación Profesional. Esto, unido al hecho de que algunas empresas de la región ya están implantando algunas experiencias en esta dirección, parece indicar la conveniencia de que los alumnos empiecen a familiarizarse con estos ámbitos de conocimiento.\nPython posee los elementos necesarios para construir los gráficos que se necesitan para una explicación sencilla de un conjunto de datos de producción alimentaria, y su sintaxis resulta fácil de comprender, aunque otras herramientas, como el lenguaje R, sean más utilizadas en el análisis estadístico y de datos.\nTener un contacto práctico con el lenguaje facilitará la adquisición de conocimientos suplementarios, ya sea en el ámbito del análisis estadístico de datos, de los métodos cuantitativos de mejora de procesos (Six Sigma), en la programación informática, o en la introducción a la IA.\nPython es una herramienta gratuita, por lo que tanto el lenguaje como las herramientas para trabajar con él están a disposición libre en la web. En nuestro caso, utilizaremos una herramienta de Google, llamada Google Colaboratory , que permite utilizar scripts Python mediante la interfase jupyter. Esta herramienta se usa dentro del entorno de Google Chrome, y no es necesario hacer ninguna instalación de software. Para el entorno de la enseñanza, Google Colab resulta conveniente y suficiente (ver uso de jupyter en educación).\n\nEl entorno de la industria alimentaria produce un constante flujo de datos como resultado tanto de la implantación de sistemas de captura automáticos como del aumento de la tecnificación de los puestos de trabajo; se requiere por parte de los profesionales industriales que sean capaces de analizar esta enorme cantidad de datos para transformarlos en información para la decisión. En la empresa industrial actual, son los ingenieros y técnicos de planta, y no estadísticos o ingenieros informáticos, quienes participan diariamente en la presentacion y discusion de los datos y en la toma de decisiones operativas, tanto en los equipos de trabajo como ante la Dirección. Por esta razón, es necesario proporcionar a los estudiantes de la Formación Profesional un conocimiento básico de los conceptos, herramientas y métodos del análisis de datos, así como de algunas técnicas de presentación y comunicación de la información.\nLa experiencia industrial muestra que en las situaciones reales, sobre el terreno, la comprensión práctica de los conceptos es más importante que su rigurosa formulación matemática. Por esta razón, en el desarrollo del contenido del libro insistiré más en la forma de aplicar las herramientas y entender los análisis que en el conocimiento formal de las fórmulas estadísticas y su deducción matemática, o de la programación informática en sí. He hecho especial hincapié en la utilización de herramientas sencillas, casi siempre gráficas, que son una ayuda valiosa para comprender la información contenida en un conjunto de datos. El objetivo es proporcionar al estudiante de industrias alimentarias las bases de la metodología para análisis de sus datos, y cómo puede aplicarse esta metoología a la resolución de problemas técnicos concretos, más que el conocimiento de la teoría matemática de la estadística o la programación.\nSe evitan las explicaciones formales sobre temas estadísticos. Así, por ejemplo, al explicar la media de un conjunto de datos considero más importante entender el concepto físico de “centro de gravedad” que los conceptos estadísticos de esperanza matemática, que no se tocan en este texto. En este sentido, se ha intentado que el alumno aprenda bien la diferencia entre “en qué consiste” un estadístico, y “cómo se calcula”. Comprender la diferencia entre el concepto y su fórmula de cálculo es fundamental para entender cómo aplicarlo.\nPor ejemplo, veremos cómo el modo de cálculo de la media aritmética hace que su valor sea equivalente al centro de gravedad de un conjunto de datos; si aproximamos la idea a la de un sistema de palanca en equilibrio sobre un fulcro, será muy fácil entender y hacer entender por qué los valores anómalos extremos desvían notablemente la media y deben analizarse con cuidado: una pequeña desviación en un item muy lejos de gravedad tiene un gran efecto sobre el punto de equilibrio del sistema, debido a la longitud del brazo de palanca; de la misma manera, los valores extremos pueden introducir desviaciones en el valor de la media aritmética que siempre hay que considerar.\nTambién será fácil comprender que otros estadísticos, como la mediana, que se calculan de forma diferente (la mediana es, simplemente, el valor central del conjunto de datos) resultan menos o nada afectados por los valores extremos,; veremos en la práctica las ventajas e inconvenientes de cada uno.\n\n\n\n\n\n\n\n\n\n\n\n(a) Diagrama de puntos (dotplot) mostrando la posición de la media aritmética\n\n\n\n\n\n\n\n\n\n\n\n(b) Palanca de primer grado con el fulcro como punto de equilibrio\n\n\n\n\n\n\n\nFigura 1.1: Comparación gráfica del significado de la media en un diagrama de puntos (dotplot) con una palanca de primer grado.\n\n\n\nUn curso de introducción al análisis de datos industriales debe ser, ante todo, práctico y orientado a su aplicación en el entorno industrial real. Los principales temas de trabajo de análisis de los datos en la industria tienen que ver con su captura, su almacenamiento y su depuración, su descripción utilizando gráficos, la inferencia (intervalos de confianza y tests), la construcción de modelos explicativos, el diseño de los experimentos industriales, el control estadístico de la calidad y la exposición y presentación de resultados. Dado el alcance limitado de este documento, no se entrará en los métodos estadísticos en sí, y necesitarán de un estudio posterior si el alumno tiene interés en profundizar en ellos.\nA pesar de que los temas más especializados puedan ser importantes en algunas aplicaciones específicas, no preparan al estudiante para lo que se va a encontrar en el terreno en la mayor parte de las ocasiones. En cambio, la resolución de problemas en equipo en un entorno de aprendizaje dinámico enfrentándose a problemas exigentes, y el desarrollo de las habilidades de análisis, de síntesis y de comunicación, tendrán un impacto mucho más positivo.\nHe intentado mostrar la necesidad de que los estudiantes comprendan y apliquen el método científico en el entorno industrial, y no sólo apliquen un recetario de procedimientos de manera automática. Son mucho mas importantes la comprensión y la utilización adecuada del método científico y de las herramientas y gráficos básicos, antes que la aplicación rutinaria y mecánica de determinadas fórmulas matemáticas o métodos sofisticados y complejos (como la IA) que el alumno puede no comprender en toda su profundidad.\nLas industrias líderes destacan por la aplicación intensiva de métodos tales como Six Sigma, Lean Manufacturing, diseño robusto de productos, y otros que hacen un uso intensivo de los datos, tanto de los obtenidos en producción como de los obtenidos en la realización de experimentos bien diseñados. Pero la mejora de la competitividad en estas empresas no se debe tanto a la aplicación de unos u otros métodos, como al desarrollo del juicio analítico de sus equipos humanos y a la aplicación de lo aprendido a la mejora continua de los procesos industriales. Veremos que la experiencia y el conocimiento tecnológico de estos procesos son fundamentales para el desarrollo del buen juicio analítico, y, en consecuencia, para la buena interpretación de los resultados que se obtienen con las herramientas estadísticas y de análisis.\nTratándose de un libro para el uso en la Formación Profesional, considero prioritario que su estudio se oriente al desarrollo de habilidades que sean de aplicación práctica directa en el puesto de trabajo y además faciliten la empleabilidad del estudiante, y no a la obtención de conocimiento abstracto. En la última edición del Informe sobre el futuro del empleo disponible cuando se edita este libro, publicada por el Foro Económico Mundial (WEF) en junio de 2025 (World Economic Forum 2025), el WEF considera que:\n\n…al igual que en las dos ediciones anteriores de este informe, el pensamiento analítico sigue siendo la principal habilidad básica para los empleadores, ya que siete de cada 10 empresas lo consideran esencial. A esto le siguen la resiliencia, la flexibilidad y la agilidad, junto con el liderazgo y la influencia social, lo que subraya el papel fundamental de la adaptabilidad y la colaboración junto con las habilidades cognitivas. El pensamiento creativo y la motivación y la autoconciencia ocupan el cuarto y quinto lugar, respectivamente. Esta combinación de habilidades cognitivas, de autoeficacia e interpersonales dentro de los cinco primeros enfatiza la importancia atribuida por los encuestados a tener una fuerza laboral ágil, innovadora y colaborativa, donde tanto la capacidad de resolución de problemas como la resiliencia personal son críticas para el éxito.\n\nA estas habilidades se suman las dos que más crecen en estos dos últimos años: la IA y el análisis de datos. El objetivo de este documento es proporcionar conocimientos que ayuden al estudiante a desarrollarse en esta dirección, y servir de apoyo a los docentes para conseguir ese objetivo.\n\n\n\n\nWorld Economic Forum. 2025. «Future of Jobs Report, 2025». 91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World Economic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2025/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "001-intro.html",
    "href": "001-intro.html",
    "title": "2  La estadística en el entorno industrial.",
    "section": "",
    "text": "2.1 Introducción\n¿Alguna vez te has preguntado cómo podemos resolver problemas en el trabajo de forma sistemática y ordenada? La estadística y el método científico nos ayudan a hacerlo, y son más sencillos de lo que parece.\nEl método científico es como una receta que seguimos para resolver problemas: primero observamos qué está pasando, luego pensamos qué puede estar causando el problema, después hacemos pruebas para comprobarlo, y finalmente sacamos conclusiones y tomamos acciones orientadas a eliminar el problema o conseguir una mejora, aunque sea parcial, y repetimos el ciclo. La estadística nos ayuda a entender si nuestras pruebas funcionaron o no, usando números y datos reales.\nPensar de forma estadística significa entender que todo lo que hacemos en el trabajo está conectado, y que es normal que las cosas cambien o varíen un poco. Lo importante es saber cuándo estos cambios son normales y cuándo indican un problema real. Es como cuando cocinamos: sabemos que cada plato puede salir un poco diferente, pero reconocemos cuando algo ha salido realmente mal.\nEn las fábricas y talleres, estas herramientas son muy útiles. Por ejemplo, si una máquina empieza a producir piezas defectuosas, podemos:\nLa experimentación es muy importante en la industria. En vez de cambiar las cosas al azar, hacemos pruebas organizadas para ver qué funciona mejor. Es como cuando ajustas la temperatura del horno y el tiempo de cocción para que un pastel salga perfecto: pruebas diferentes combinaciones y anotas los resultados.\nLo mejor de todo es que cada vez que hacemos estos experimentos, aprendemos algo nuevo. Incluso si algo no funciona como esperábamos, esa información nos ayuda a mejorar la próxima vez. Es un proceso continuo de aprendizaje y mejora.\nEsta forma de trabajar nos ayuda a:\nRecuerda: no necesitas ser un genio de las matemáticas para usar estas herramientas. Lo importante es ser organizado, observar bien lo que pasa y anotar los resultados de lo que hacemos. Así, poco a poco, podemos mejorar nuestro trabajo y resolver problemas de forma más eficiente.\nLos métodos estadísticos nos ayudan a describir y comprender la variabilidad. Cuando hablamos de variabilidad queremos decir que sucesivas observaciones de un mismo proceso o sistema no dan exactamente los mismos resultados. Por ejemplo, el consumo de gasolina de un coche no es siempre igual, sino que varía de manera considerable. Esta variación depende de muchos factores, como la forma de conducir, el tipo de carretera, la situación del propio vehículo (presión de neumáticos, compresión del motor, …), la marca de la gasolina, el octanaje, o incluso las condiciones meteorológicas. Todos estos factores son causas de variabilidad en el consumo de gasolina. La estadística nos permite analizar estos factores y determinar cuáles son los más importantes o tienen mayor impacto en el consumo; una vez conocidos, podemos actuar sobre ellos.\nEn este libro aprenderemos a utilizar herramientas diversas, tanto estadísticas como de la ciencia de datos, para realizar nuestro análisis. Para aprender de los datos necesitamos más que los simples números; para interpretarlos necesitaremos siempre el conocimiento del proceso industrial que estamos analizando.En un análisis de la producción de un producto lácteo, por ejemplo, los números significan poco sin un conocimiento del proceso; los valores de pH, temperatura o concentración de lactosa influyen en el resultado del proceso de forma diferente. Los datos son números dentro de un contexto, y necesitamos conocer este contexto para dar sentido a los números.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "001-intro.html#introducción",
    "href": "001-intro.html#introducción",
    "title": "2  La estadística en el entorno industrial.",
    "section": "",
    "text": "Observar qué está pasando\nPensar en posibles causas\nHacer pruebas controladas para encontrar el problema\nUsar números y datos para confirmar si lo hemos solucionado\n\n\n\n\n\nResolver problemas de forma ordenada\nTomar mejores decisiones basadas en datos reales\nMejorar la calidad de nuestro trabajo\nAhorrar tiempo y dinero evitando errores\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEl objetivo principal de la mejora industrial es la reducción de la variabilidad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "001-intro.html#el-pensamiento-estadístico-y-el-método-científico.",
    "href": "001-intro.html#el-pensamiento-estadístico-y-el-método-científico.",
    "title": "2  La estadística en el entorno industrial.",
    "section": "2.2 El pensamiento estadístico y el método científico.",
    "text": "2.2 El pensamiento estadístico y el método científico.\nLa estadística y el método científico mantienen una relación fundamental que impulsa el avance del conocimiento y la innovación. El método científico, con sus pasos de observación, hipótesis, experimentación y conclusiones, encuentra en la estadística las herramientas necesarias para validar o refutar hipótesis de manera objetiva y cuantificable.\nEl pensamiento estadístico es una filosofía de aprendizaje y acción basada en tres principios fundamentales:\n\nque todo proceso industrial está compuesto a su vez por procesos interconectados,\nque la variabilidad existe y es inherente a estos procesos, y\nque entender y reducir la variación es clave para la mejora y el éxito\n\nEn el entorno industrial, esta integración entre estadística y método científico ha revolucionado los procesos de mejora continua. Por ejemplo, cuando una línea de producción enfrenta problemas de calidad, el método científico guía la investigación sistemática: se observa el proceso, se formulan hipótesis sobre las causas del problema, se diseñan experimentos controlados, y se analizan los resultados estadísticamente para determinar la validez de las soluciones propuestas.\nLa experimentación industrial, particularmente a través del diseño de experimentos (DOE), se ha convertido en una herramienta fundamental para la mejora continua. En lugar de modificar procesos basándose en intuiciones o experiencias pasadas, las empresas pueden realizar experimentos controlados que:\n\nOptimizan múltiples variables simultáneamente\nIdentifican interacciones entre factores que afectan el proceso\nReducen el tiempo y costo de las mejoras\nProporcionan conclusiones respaldadas por evidencia estadística\n\nAl realizar experimentos controlados, las empresas pueden probar hipótesis que permitan evaluar el impacto de diferentes variables en los procesos y productos y determinar los valores óptimos de los parámetros de los procesos para maximizar la calidad y la eficiencia. Esto genera conocimiento valioso sobre los procesos y productos, permite tomar decisiones más informadas, y ayuda a crear y mantener una ventaja competitiva.\nLa experimentación, combinada con el análisis estadístico, permite a las empresas implementar ciclos de mejora continua, donde los resultados de cada experimento se utilizan para refinar los procesos y productos.\nAl integrar el pensamiento estadístico con el método científico en el entorno industrial, las organizaciones pueden desarrollar una cultura de mejora continua basada en la evidencia de los datos y no en suposiciones o intuiciones. En una cultura industrial basada en el método científico, cada problema es una oportunidad para experimentar, aprender y optimizar. Esta aproximación sistemática no solo mejora la calidad y eficiencia de los procesos, sino que también fomenta la innovación y el aprendizaje organizacional continuo.\nLa clave del éxito radica en entender que la experimentación no es un evento aislado, sino un proceso continuo de aprendizaje y mejora. Cada experimento, exitoso o no, aporta información valiosa que, analizada correctamente mediante métodos estadísticos, contribuye al conocimiento colectivo de la organización y sienta las bases para futuras mejoras. La estadística y el método científico no solo son pilares fundamentales para la investigación académica, sino que también desempeñan un papel crucial en el entorno industrial. La aplicación sistemática del método científico, respaldada por herramientas estadísticas, permite a las empresas optimizar procesos, mejorar la calidad de sus productos y servicios, y fomentar la innovación.\n\n\n\n\n\n\nEl pensamiento estadístico en la toma de decisiones industriales\n\n\n\nEn el entorno industrial, el pensamiento estadístico es esencial para:\n\nTomar decisiones basadas en datos, evitando decisiones basadas en intuiciones o suposiciones.\nEvaluar riesgos, cuantificando la incertidumbre y permitiendo tomar decisiones que minimicen los riesgos.\nMejorar el conocimiento de los procesos para una toma de decisiones más eficaz.\n\nComunicar resultados, presentando los resultados de los análisis de manera clara y concisa.\n\nLa integración del método científico y el pensamiento estadístico en la cultura empresarial impulsa la innovación, la eficiencia y la competitividad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "001-intro.html#los-datos-industriales",
    "href": "001-intro.html#los-datos-industriales",
    "title": "2  La estadística en el entorno industrial.",
    "section": "2.3 Los datos industriales",
    "text": "2.3 Los datos industriales\nEn el entorno industrial, podemos organizar los datos en varias categorías, considerando tanto su naturaleza como la forma en que se obtienen:\nSegún su estructura:\n\nDatos estructurados:\n\nSon datos organizados en un formato definido, como tablas de bases de datos, hojas de cálculo o archivos CSV. Su estructura facilita el análisis y la consulta.\nEjemplos: lecturas de sensores, registros de producción, datos de control de calidad, información de inventario.\n\nDatos no estructurados:\n\nSon datos que no tienen un formato predefinido, como texto, imágenes, audio o video.Su análisis requiere técnicas más avanzadas, como el procesamiento de lenguaje natural o la visión artificial.\nEjemplos: registros de mantenimiento, informes de incidentes, imágenes de inspección visual, grabaciones de audio de maquinaria.\n\nDatos semiestructurados:\n\nSon datos que tienen cierta estructura, pero no tan rígida como los datos estructurados, y permiten una mayor flexibilidad en el almacenamiento y el intercambio de información.\nEjemplos: archivos XML o JSON, registros de eventos.\n\n\nSegún su origen y método de obtención:\n\nDatos históricos (Estudios retrospectivos):\n\nSon datos recopilados en el pasado, que se utilizan para analizar tendencias, identificar patrones y predecir el comportamiento futuro. Permiten comprender la evolución de los procesos.\nEjemplos: registros de producción de años anteriores, datos de fallos de maquinaria, históricos de ventas.\n\nDatos observacionales (Estudios observacionales):\n\nSon datos recopilados mediante la observación de procesos o sistemas, sin intervenir en ellos. Permiten identificar relaciones entre variables y comprender el comportamiento de los procesos en condiciones reales.\nEjemplos: mediciones de temperatura, vibración o presión de maquinaria, registros de tiempo de ciclo de producción.\n\nDatos experimentales (Experimentos diseñados):\n\nSon datos recopilados mediante la realización de experimentos controlados, en los que se manipulan variables para evaluar su efecto. Permiten establecer relaciones de causa y efecto, estudiar las interacciones entre las variables y optimizar el rendimiento de los procesos.\nEjemplos: datos de pruebas de rendimiento de nuevos materiales, resultados de experimentos de optimización de procesos.\n\nDatos de monitorización o control de procesos en tiempo real:\n\nson datos que se recaban de manera instantanea, y permiten actuar de manera casi inmediata en los procesos; permiten implementar el mantenimiento predictivo, y evitar perdidas de produccion.\nEjemplos: datos de sensores que detectan fallos en maquinaria, alarmas de procesos fuera de control.\n\n\nEn el contexto de la Industria 4.0, la cantidad y variedad de datos industriales está aumentando exponencialmente.\n\nEstudios retrospectivos o históricos\nUn estudio retrospectivo o histórico es el que utiliza una muestra o todos los datos históricos de un proceso, recogidos en el pasado durante un período determinado de tiempo. El objetivo de un estudio de este tipo puede ser la investigación sobre la relación entre algunas variables, o explorar la calidad de la información disponible, o construir un modelo que permita explicar el proceso tal como es actualmente, o saber si se ha desviado. Estos modelos del proceso se denominan modelos empíricos, porque están basados en los propios datos del proceso y no en una formulación teórica sobre el mismo.\nUn estudio retrospectivo tiene la ventaja de tener a su disposición un gran número de datos que ya han sido recogidos, minimizando el esfuerzo de obtenerlos. Sin embargo, tiene varios problemas potenciales:\n\nSi no disponemos de detalles suficientes, es posible que no podamos determinar si las condiciones de variación de los valores obtenidos responden a las mismas causas que en la situación actual.\nEs posible que nos falte algún valor clave que no haya sido recogido o que lo haya sido de manera defectuosa\nAlgunas veces, la fiabilidad y validez de los datos de proceso históricos son dudosas, o al menos, cuestionables.\nLos datos históricos no siempre se han recogido con la perspectiva actual del proceso, y es posible que no nos proporciones explicaciones adecuadas del proceso en su situación actual.\nA veces queremos utilizar los datos históricos de proceso para fines que no estaban previstos cuando se recogieron\nLas notas sobre los valores del proceso, incluyendo los valores anormales, pueden ser insuficientes o inexistentes, y no tenemos ninguna explicación sobre los posibles valores anómalos que detectamos en el análisis.\n\nUsar datos históricos siempre tiene el riesgo de que, por la razón que sea, no se hayan recogido datos importantes, o que estos datos se hayan perdido, o se hayan transcrito de forma inadecuada o incorrecta. Es decir, los datos históricos pueden tener problemas de calidad de datos.\nEl hecho de que algunos datos se hayan recogido históricamente no siempre quiere decir que estos datos sean relevantes o útiles. Cuando el grado de conocimiento del proceso no es suficiente, o no se basa en un análisis metódico y riguroso de los datos, es posible que no se hayan recogido algunos datos que pueden ser importantes para el proceso, a veces simplemente porque son complejos o difíciles de analizar. Los datos históricos no pueden proporcionar la información que buscamos si la información de las variables clave nunca se ha recogido o se ha hecho sin una buena base experimental.\nEl propósito del análisis de los datos industriales es aislar las causas que están detrás de los sucesos que afectan e influyen en los procesos. En los datos históricos, estos sucesos pueden haber ocurrido semanas, meses o incluso años antes, sin que haya registros ni notas que hayan intentado explicar estas causas, y los recuerdos de las personas que han participado en ellos se pierden con el tiempo, o se alteran involuntariamente, proporcionando explicaciones supuestamente válidas pero que en realidad son incorrectas. Por eso, con frecuencia, el análisis de los datos históricos puede poner de manifiesto hechos interesantes, pero sus causas quedan sin explicar.\nLos estudios históricos pueden requerir una fase previa de preparación y depuración de datos que puede llegar a ser muy larga y tediosa. Se estima que en muchos estudios de ciencia de datos, el tiempo de preparación de los datos puede llegar al \\(60\\%\\) del tiempo total empleado en el estudio. Las herramientas de análisis de datos son de gran ayuda en esta fase del proceso, aunque en muchas ocasiones será necesario un trabajo manual de recolección de datos en papel, hojas de cálculo diversas y otras fuentes. Esta fase es muy útil no sólo para la preparación de datos para el estudio, sino para mejorar el conocimiento de los datos, cómo se originan y cómo se almacenan. Este conocimiento siempre es de gran utilidad para mejorar los procedimientos actuales de captura de datos, facilitando la fiabilidad de los análisis futuros.\n\n\nEstudios observacionales\nComo su nombre indica, un estudio observacional simplemente observa un proceso durante un tiempo de operación en rutina. Normalmente, el ingeniero o técnico interfiere lo mínimo posible en el proceso; sólo lo suficiente para recoger la información que necesita, si piensa que esa información puede ser relevante. En muchas ocasiones, el estudio no forma parte de los controles de rutina, y representa un trabajo adicional.\nSi se planifican adecuadamente, los estudios observacionales proporcionan datos fiables, precisos y completos para documentar un proceso. Por otra parte, estos estudios proporcionan una información limitada sobre las relaciones entre las variables del proceso, porque es posible que durante el tiempo limitado de observación, el rango de variación de las variables no recoja todas las situaciones posibles; por ejemplo, las situaciones extraordinarias.\n\n\nExperimentos diseñados\nLa tercera forma de recoger información de un proceso son los experimentos diseñados. En un experimento de este tipo, el ingeniero o técnico hace un cambio deliberado en las variables que controla (llamadas factores), observa el resultado, y toma una decisión respecto a qué variable o variables son responsables de los cambios que observa en el proceso.\nUna diferencia importante respecto a los estudios históricos y los observacionales es que las diferentes combinaciones de factores se aplican al azar sobre un conjunto de unidades experimentales. Esto permite establecer con precisión las relaciones causa-efecto, cosa que no suele ser posible ni en los estudios históricos ni en los observacionales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html",
    "href": "005-herramientas.html",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "",
    "text": "3.1 Introducción\nEn este capítulo veremos las herramientas que utiizaremos en el análisis de datos industriales. Nos enfocaremos en dos de ellas: la hoja de cálculo Microsoft Excel, y el lenguaje de programación Python. Ambas herramientas están ampliamente extendidas en las empresas y en las instituciones docentes de todo el mundo. Python es, además del lenguaje de programación más utilizado en el mundo, un software libre, y por lo tanto, con un coste de adquisición cero, lo que facilita su utilización. Microsoft Excel tiene versiones web que se pueden utilizar para un uso básico también sin coste. No son las únicas opciones: hay otros programas estadísticos y de análisis muy utilizados y potentes, tales como Matlab o Minitab, que son de gran interés en ingeniería, aunque tienen un coste bastante elevado, y hojas de cálculo como Google Docs o LibreOffice, casi totalmente compatibles con Microsoft Excel.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#la-hoja-de-cálculo",
    "href": "005-herramientas.html#la-hoja-de-cálculo",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.2 La hoja de cálculo",
    "text": "3.2 La hoja de cálculo\nLa hoja de cálculo es una herramienta presente hoy día en todos los ámbitos de trabajo y educativos. Desde la aparición de Visicalc, en 1978, ha contribuido a la gestión de miles de empresas, se ha utilizado de manera general en análisis de datos y sus gráficos se han utilizado y se utilizan en publicaciones e informes de todas clases. En la década de los años 80 del pasado siglo, la hoja de cálculo Lotus 1-2-3 fue la aplicación más utilizada en los ordenadores IBM-PC y compatibles, y consiguió facturaciones millonarias para la empresa matriz. Lotus 1-2-3 dominó el mercado hasta la aparición de Microsoft Windows a finales de los años 80; este nuevo sistema operativo favoreció la implantación de Microsoft Excel, que desde entonces se convirtió en la hoja de cálculo dominante.\n\n\n\n\n\n\nVisicalc, primera hoja de cálculo para el ordenador Apple II (1979)\n\n\n\n\n\n\n\nHoja de cálculo Lotus 1-2-3 para MS-DOS (1983)\n\n\n\n\n\n\n\n\n\nMicrosoft Excel (2023)\n\n\n\n\n\n\nAplicaciones de la hoja de cálculo\nLas hojas de cálculo son muy útiles para recoger la información de un conjunto de observaciones. Entre sus principales usos, están:\n\nLa introducción, edición y almacenamiento datos.\nEl filtrado y corrección de errores.\nLa manipulación básica, por ejemplo, mediante tablas dinámicas\nLa preparación y edición de gráficos, incluyendo gráficos dinámicos\nLa presentación de la información, con el apoyo opcional de herramientas adicionales como Microsoft PowerPoint.\n\nLos datos se pueden recoger y guardar de múltiples formas. Cuando la recogida de datos se hace de forma manual en papel, es necesario registrar en el ordenador los datos recogidos. Lo más frecuente es que este registro se haga en hojas de cálculo, como Microsoft Excel o Google Sheets. En algunos casos, el almacenamiento se hace sobre bases de datos, genéricas o desarrolladas a medida.\nActualmente, la tendencia es recoger los datos o bien de forma automática, o bien de forma manual sobre sistemas informatizados (pantallas), lo que permite eliminar el papel y disponer directamente de los datos en un formato digitalizado.\nEn la actualidad, la mayoría de los equipos y líneas de producción se interconectan con los sistemas de información (ver IoT) y almacenan en tiempo real todos los datos necesarios, lo que libera al operario de la pesada tarea de reintroducirlos manualmente, a la vez que reduce los errores debidos a la imputación incorrecta.\nEn todos los casos, es imprescindible asegurar que los sistemas de información pueden exportar sus datos a ficheros de texto tipo fichero plano o tipo CSV, de forma que podamos importarlos tanto a Excel como a R, como veremos más adelante. Estos sistemas de exportación de datos deben diseñarse de forma flexible y abierta, para que tanto la captura como la exportación puedan modificarse y adaptar la recogida de la información a las necesidades de cada momento.\nEn este libro trataremos exclusivamente de lo que llamaremos datos rectangulares: grupos de valores que están asociados a una o más variables, y a varias observaciones. Hay muchos más datos que no se ajustan a esta organización tabular, es el caso de imágenes, sonidos o archivos documentales de texto. Pero la forma más común de almacenar datos industriales es la de las tablas rectangulares, organizadas según el principio de los datos arreglados, que detallaremos en el siguiente capítulo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#los-lenguajes-de-programación",
    "href": "005-herramientas.html#los-lenguajes-de-programación",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.3 Los lenguajes de programación",
    "text": "3.3 Los lenguajes de programación\n\nPython en el análisis de datos industriales: una herramienta versátil\nPython se ha convertido en el lenguaje de referencia para el análisis de datos en entornos industriales por su combinación de simplicidad, potencia y ecosistema científico. En industrias alimentarias, donde se manejan grandes volúmenes de datos de producción, calidad, trazabilidad y consumo energético, Python ofrece ventajas clave:\n\nVentajas técnicas\n\nSintaxis clara y accesible: ideal para estudiantes sin experiencia previa en programación.\nBibliotecas especializadas:\n\npandas para manipulación de datos tabulares (ej. registros de producción, análisis de lotes).\nmatplotlib y seaborn para gráficos de control, tendencias y correlaciones.\nnumpy para cálculos numéricos precisos (temperaturas, tiempos, concentraciones).\n\nAutomatización de tareas repetitivas: generación de informes, limpieza de datos, detección de anomalías.\n\n\n\nAplicaciones concretas en industrias alimentarias\n\nControl de calidad: análisis de desviaciones en parámetros críticos (pH, humedad, temperatura).\nOptimización de procesos: visualización de curvas de cocción, enfriamiento, fermentación.\nTrazabilidad: seguimiento de lotes desde materia prima hasta producto final.\nSeguridad alimentaria: detección de patrones en alertas sanitarias o fallos de producción.\n\n\n\n¿Por qué incluir Python en la Formación Profesional?\nUn módulo básico de Python y gráficos en programas de formación profesional en industrias alimentarias permite:\n\n\nDesarrollo de competencias clave\n\nAlfabetización digital aplicada: no solo usar software, sino entender cómo se procesan los datos.\nPensamiento lógico y estructurado: útil para resolver problemas técnicos y documentar procesos.\nAutonomía en el análisis: no depender exclusivamente de hojas de cálculo o software cerrado.\n\n\n\nEnfoque pedagógico sugerido\n\nAprendizaje basado en proyectos: por ejemplo, analizar datos reales de producción o simular un sistema de trazabilidad.\nVisualización como herramienta de comprensión: enseñar a interpretar gráficos de dispersión, histogramas, series temporales.\nModularidad y reproducibilidad: fomentar el uso de scripts anotados y reutilizables, alineado con tu enfoque docente.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#otras-herramientas-r-julia",
    "href": "005-herramientas.html#otras-herramientas-r-julia",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.4 Otras herramientas: R, Julia",
    "text": "3.4 Otras herramientas: R, Julia\n\nEl software estadístico R\nR es un lenguaje de programación y un entorno de software utilizado para el análisis estadístico, la visualización de datos y la modelización. Algunas de las características clave de R son:\n\nAmplio espectro de funcionalidades: R abarca una amplia gama de herramientas y paquetes diseñados para realizar diversos análisis estadísticos, exploración de datos y modelización.\nHerramientas gráficas: R dispone de algunas de las bibliotecas gráficas más potentes para la exploración y descripción de datos.\nEstadística descriptiva: R ofrece funciones para calcular estadísticas descriptivas básicas, como la media, mediana, desviación estándar, varianza, rango, cuartiles y percentiles. Estas funciones son esenciales para explorar y resumir datos.\nContrastes de hipótesis: R proporciona funciones para realizar pruebas estadísticas, como t-tests, test chi-cuadrado, ANOVA y pruebas no paramétricas. Estas pruebas permiten evaluar hipótesis y comparar grupos de datos.\nDistribuciones de probabilidad: R incluye una amplia variedad de funciones para trabajar con distribuciones de probabilidad (por ejemplo, normal, uniforme, binomial, Poisson). Esto es útil para generar números aleatorios, calcular probabilidades y cuantiles.\nBibliotecas de funciones (librerías): Además de las amplias funciones básicas de las que dispone, R es capaz de utilizar bibliotecas de funciones (llamadas librerías) que han sido desarrolladas por los propios usuarios, y que amplían sus funcionalidades a todos los campos imaginables, desde el análisis genético al análisis de riesgos bancarios o el control estadístico de procesos.\n\n\nUsos y aplicaciones de R en la estadística industrial\nEn el contexto de la estadística industrial, R se utiliza para:\n\nControl de calidad: R permite analizar datos de procesos industriales, identificar desviaciones y controlar la calidad de los productos.\nOptimización de procesos: Mediante técnicas estadísticas avanzadas, R ayuda a optimizar procesos industriales, reducir costos y mejorar la eficiencia.\nAnálisis de fiabilidad: R se utiliza para evaluar la confiabilidad de sistemas y componentes en la industria.\n\n\n\nUtilización práctica de R en el entorno industrial\n[..a desarrollar..]\n\nImportación de datos y exportación de datos\nManipulación de datos: depuración, corrección, filtrado de datos.\nExploración gráfica de los datos\nAnálisis estadísticos específicos.\nGráficos de control\nInformes automatizados\n\n\n\n\nEl lenguaje Julia\nJulia es un lenguaje de programación más reciente, diseñado específicamente para el cálculo numérico y la ciencia de datos. Ofrece un rendimiento cercano al de lenguajes de bajo nivel como C, pero con la sintaxis y facilidad de uso de lenguajes de alto nivel como Python y R. Julia es especialmente útil en aplicaciones donde el rendimiento es crítico, como en simulaciones industriales y análisis de grandes volúmenes de datos.\n\nVentajas\n\nRendimiento: Diseñado específicamente para computación numérica de alto rendimiento, Julia ofrece velocidades comparables a lenguajes compilados como C o Fortran, pero con una sintaxis más cercana a los lenguajes de scripting. Es excelente para cálculos numéricos intensivos y proporciona soporte nativo para paralelismo y computación distribuida.\nSintaxis expresiva: Su sintaxis es intuitiva y similar a la de lenguajes matemáticos, lo que facilita la escritura de código conciso y legible.\nInteroperabilidad: Permite llamar a código escrito en otros lenguajes como C, Python o R, lo que facilita la integración con herramientas existentes.\nEn crecimiento: Aunque más joven que Python y R, Julia está ganando rápidamente popularidad en la comunidad científica y de datos.\n\n\nEn resumen, mientras que R sigue siendo una opción poderosa y preferida para la estadística industrial, Python y Julia se presentan como alternativas viables y, en algunos casos, superiores, dependiendo de los requisitos específicos del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#el-concepto-de-reproducibilidad",
    "href": "005-herramientas.html#el-concepto-de-reproducibilidad",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.5 El concepto de reproducibilidad",
    "text": "3.5 El concepto de reproducibilidad\nLa reproducibilidad de un ensayo o experimento es la capacidad de ser reproducido o replicado por otros, en particular, por la comunidad científica. La reproducibilidad se refiere a la capacidad de obtener resultados consistentes al replicar un estudio o experimento utilizando los mismos datos, metodología original y, en su caso, el mismo código informático empleado para los análisis.En otras palabras, cuando se replica un análisis de datos o un experimento, los resultados deben ser alcanzados nuevamente con un alto grado de confiabilidad.\nLa repetibilidad o replicabilidad se refiere a la posibilidad de obtener resultados consistentes al replicar un estudio con un conjunto distinto de datos, pero obtenidos siguiendo el mismo diseño experimental. Implica obtener resultados consistentes utilizando nuevos datos o nuevos resultados computacionales para responder a la misma pregunta científica.\nEl químico irlandés Robert Boyle, en el siglo XVII, subrayó la importancia de la reproducibilidad en la ciencia. Boyle sostenía que los fundamentos del conocimiento debían basarse en hechos producidos experimentalmente, que pudieran volverse creíbles para la comunidad científica por su reproducibilidad. La bomba de aire de Boyle, un aparato científico complicado y costoso en ese momento, condujo a una de las primeras disputas documentadas sobre la reproducibilidad de un fenómeno científico.\n\nImportancia en la Ciencia\nLa reproducibilidad es esencial para la investigación científica, ya que permite validar y verificar los resultados obtenidos. En las últimas décadas, ha habido una creciente preocupación por la falta de reproducibilidad en muchos resultados científicos publicados, lo que ha llevado a una crisis de reproducibilidad o replicación. La reproducibilidad garantiza que los resultados científicos sean confiables y puedan ser validados por otros investigadores. Es un pilar fundamental para el avance del conocimiento en todas las disciplinas.\n\n\nReproducibilidad en metrología\nEn metrología, la reproducibilidad es la capacidad de un instrumento de dar el mismo resultado en mediciones diferentes, realizadas en las mismas condiciones a lo largo de periodos dilatados de tiempo. Esta cualidad debe evaluarse a largo plazo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#ventajas-de-los-lenguajes-de-programación-en-scripts-frente-a-la-hoja-de-cálculo-en-la-reproducibilidad-de-un-análisis",
    "href": "005-herramientas.html#ventajas-de-los-lenguajes-de-programación-en-scripts-frente-a-la-hoja-de-cálculo-en-la-reproducibilidad-de-un-análisis",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.6 Ventajas de los lenguajes de programación en scripts frente a la hoja de cálculo en la reproducibilidad de un análisis",
    "text": "3.6 Ventajas de los lenguajes de programación en scripts frente a la hoja de cálculo en la reproducibilidad de un análisis\nLas ventajas de utilizar un lenguaje de scripts en lugar de hojas de cálculo tradicionales (como Microsoft Excel) incluyen:\n\nCódigo abierto (scripts): los lenguajes permiten crear flujos de trabajo basados en código, lo que mejora la reproducibilidad de los análisis y facilita la colaboración entre investigadores. Puedes escribir scripts en Python para automatizar tareas y asegurar la reproducibilidad. Los scripts son transparentes y pueden ser compartidos y verificados por otros investigadores.\nFlexibilidad estadística: Python es especialmente útil para técnicas avanzadas de análisis, lo que lo convierte en una excelente opción para investigadores que buscan análisis de vanguardia, como es el caso de los modelos de inteligencia artificial (IA)\nGráficos muy potentes y completos: Los gráficos disponibles en estos lenguajes son una de sus fortalezas, no sólo por su variedad sino también por su flexibilidad.\nCapacidad para tratar grandes cantidades de datos: pueden manejar grandes conjuntos de datos sin problemas, lo que es fundamental en la estadística industrial.\nPrecisión: Aunque no está diseñado específicamente para el análisis estadístico, como sí es el caso de R, Python puede realizar todos los análisis que se necesitan en el entorno industrial, y es más preciso que Excel en ciertos casos, como en análisis de regresión lineal.\nCapacidad avanzada: Python ofrece una amplia gama de librerías y funciones para realizar análisis estadísticos avanzados, como modelos lineales, series temporales, análisis multivariante y más.\n\nPor su parte, la hoja de cálculo tiene una curva de aprendizaje más sencilla y es en general más fácil de usar, pero tiene algunos inconvenientes:\n\nInexactitudes: Estudios han demostrado que Excel puede mostrar ciertas inexactitudes en análisis de regresión lineal y otros métodos estadísticos.\nLimitaciones gráficas: Los gráficos de Excel son bastante limitados cuando se trata de presentar información sobre un análisis de datos.\nLimitaciones estadísticas: Excel no está diseñado específicamente para análisis estadístico avanzado, por lo que puede carecer de algunas capacidades necesarias para investigaciones más complejas.\nFalta de transparencia para la auditoría: Las fórmulas y cálculos en Excel pueden ser difíciles de rastrear y verificar, lo que afecta la reproducibilidad.\n\nPor estas razones usaremos Excel para el almacenamiento de datos y el análisis básico, y usaremos Python para el análisis gráfico más detallado y el análisis numérico y estadístico.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html",
    "href": "010-organizacion.html",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "",
    "text": "4.1 Definiciones y términos útiles",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#definiciones-y-términos-útiles",
    "href": "010-organizacion.html#definiciones-y-términos-útiles",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "",
    "text": "Población y muestra\nUna población es un conjunto de de personas, cosas o, en general, objetos en estudio. A veces, una población es demasiado grande para que podamos abarcarla completa; para poder estudiarla, obtenemos una muestra, que consiste en un subconjunto de la población que hemos seleccionado para su estudio. El proceso de obtener una muestra se llama muestreo, y se realiza de acuerdo con normas y procedimientos específicos.\nEn muchas ocasiones, cuando se recogen los datos como resultado de una experimentación, definimos la población como todos los resultados que podríamos haber obtenido. Llamamos a este conjunto de posibles resultados una población conceptual. Por ejemplo, cuando medimos el \\(pH\\) de varias muestras de leche, la población es el conjunto de todos los resultados posibles que podríamos haber tenido. Muchos problemas de ingeniería y tecnología se refieren a poblaciones conceptuales.\n\n\n\n\n\n\nRecuerda\n\n\n\nEn la mayoría de las ocasiones, nuestros datos provienen de una muestra obtenida de una población,\n\n\nCuando tomamos una muestra, debemos estar seguros de que contiene las propiedades que queremos estudiar en la población. En ese caso, decimos que la muestra es representativa: los individuos de la muestra son representativos de la población. Para que la muestra sea representativa, debe ser obtenida mediante un muestreo aleatorio. Una muestra aleatoria simple de tamaño \\(n\\) consiste en \\(n\\) individuos de una población, elegidos de forma que cada conjunto posible de \\(n\\) individuos tiene la misma probabilidad de ser elegido.\n\n\n\n\n\n\nEjemplo 1: Muestreando una cámara de maduración de queso\n\n\n\nImagina que debes analizar el contenido de extracto seco de una producción de quesos en fase de maduración en una cámara frigorífica. Dado que la cámara está muy llena y es complicado acceder a su interior, decides tomar tu muestra de los quesos que están más a tu alcance, situados cerca de la puerta y a la altura de la vista.\n¿Consideras que esta es una forma correcta de obtener una muestra representativa?\n¿Cómo definirías la población en este caso?\n\n\n\n\n\n\n\n\nRespuesta al ejemplo 1: Muestreando una cámara de maduración de queso\n\n\n\n\n\nLa forma de tomar la muestra no es correcta, ya que al seleccionar únicamente los quesos más accesibles introduces un sesgo en la muestra. Las condiciones de humedad, temperatura y circulación de aire podrían variar dentro de la cámara, afectando el contenido de extracto seco según la ubicación de los quesos.\nPara obtener resultados fiables, es fundamental que la muestra sea representativa de toda la producción. Para ello, se debe aplicar un método de muestreo aleatorio, donde todos los quesos tengan la misma probabilidad de ser seleccionados.\nEn este caso, la población está formada por el conjunto total de quesos en proceso de maduración dentro de la cámara.\n\n\n\n\n\n\n\n\n\n¿Qué es la probabilidad?\n\n\n\nEl concepto de probabilidad se explica de forma detallada en el capítulo 9\n\n\n\n\nParámetro y estadístico\nUn parámetro es una característica de una población. Podemos estimar su valor mediante la extracción de una muestra, que utilizaremos para calcular un estadístico muestral. Llamamos estadístico a un número que representa una propiedad o característica de la muestra, y constituye una estimación del valor de un parámetro de la población que estamos estudiando.\n\n\nVariables y casos\nA los objetos descritos en un conjunto de datos los llamamos casos, de forma genérica. A veces, estos casos pueden corresponder a personas; en ese caso podemos llamarlos individuos. Cuando los objetos que estudiamos no son personas, como es lo habitual en el entorno industrial, utilizamos la nomenclatura genérica.\nUn atributo es una característica que define una propiedad de un objeto, persona o cosa. Por ejemplo, edad, peso, altura, sexo, color de ojos, son atributos de una persona. Llamamos variable a una característica cualquiera de un individuo o caso que puede ser medida u observada. Una variable puede tomar diferentes valores en diferentes individuos o casos.\nSegún estas definiciones que acabamos de ver, una muestra está formada por un conjunto de casos, y cada caso contiene un determinado número de variables, que contienen los valores que hemos analizado o medido.\n\n\nTipos de variables\nAlgunas variables, como el color, sirven para clasificar los individuos en categorías. Otras, como la altura o el peso de un individuo, pueden tomar valores numéricos con los que podemos hacer cálculos. Por ejemplo, podemos sumar la altura de varias personas, pero no tiene sentido sumar los colores del arco-iris (aunque sí podemos contarlos, y hacer cálculos con estos recuentos). También podemos categorizar variables continuas: podemos clasificar nuestro grupo de personas en altas o bajas, y podemos contar cuántas personas entran en cada categoría.\n\n\n\n\n\n\n\n\n\nVariables cualitativas  o categóricas\n\nVariables cuantitativas  o métricas\n\n\n\n\n\nNominales\nOrdinales\nDiscretas\nContinuas\n\n\nValores en categorías arbitrarias\nValores en categorías ordenadas\nValores enteros en escala numérica\nValores continuos en escala numérica\n\n\n(sin unidades)\n(sin unidades)\nUnidades contadas\nUnidades medidas\n\n\n\nUna variable categórica coloca a un individuo en uno o más grupos o categorías\nUna variable métrica toma valores numéricos con los que tiene sentido realizar cálculos aritméticos como sumar, restar, etc.\nLas variables categóricas se conocen también como variables cualitativas porque indican cualidades.\nLas variables métricas se conocen también como variables cuantitativas porque indican cantidades.\n\n\n\n\n\n\nComentario: ¿Cualitativo quiere decir “que tiene calidad”?\n\n\n\nAlgunas veces se usa el término cualitativo de forma incorrecta para referirse a la calidad de algo, por ejemplo, cuando alguien dice: “Este envase es muy cualitativo”. La forma correcta sería: “Este envase es de gran calidad”.\nEn realidad, cualitativo no proviene calidad sino de cualidad, y hace referencia a las características o propiedades de un objeto o fenómeno, sin implicar juicio de valor. Como hemos visto, este término se usa en estadística para describir variables que representan categorías o atributos, como el color, el tipo de material o la marca, sin medir cantidades.\n\n\n\n\n\n\n\n\nPara resolver\n\n\n\nEjemplo 1. Tiramos un dado al aire. Describe a qué corresponde la variable y el caso.\nEjemplo 2. Durante un proceso de envasado de un producto que dura una hora, controlamos el peso de cada envase cada minuto. Describe la variable y el caso. ¿Puede haber más de una variable?\n\n\n\n\n\n\n\n\nRespuestas: Para resolver\n\n\n\n\n\nEjemplo 1: La variable es el resultado que obtenemos cada vez; podríamos denominarla, por ejemplo, \\(resultado\\_obtenido\\). Colocaríamos este nombre en el encabezado de una columna en una hoja de cálculo. Cada tirada que hacemos es un caso; iríamos colocando el resultado que obtenemos cada vez en una nueva fila de nuestra hoja de cálculo.\nEjemplo 2. En este caso, la variable es el \\(peso\\_obtenido\\), y cada pesada constituye un caso. Si registrásemos, además, la hora y el minuto en el que que hemos hecho cada control de peso, podríamos definir una nueva variable, que podríamos llamar \\(hora\\), y que colocaríamos en una columna al lado del \\(peso\\_obtenido\\). Incluso podríamos definir otra variable adicional, el \\(numero\\_de\\_pesada\\), que sería un número secuencial empezando en \\(1\\) y que se incrementaría en cada pesada, de forma que al final esta variable nos daría el número de pesadas realizadas, y nos indicaría además el orden en el que las hemos realizado. Puesto que hemos realizado una pesada cada minuto, tendríamos tres variables y 61 líneas (un encabezado y 60 líneas correspondientes una a cada minuto)\n\n\n\n\n\nReglas básicas para establecer los nombres de las variables.\nSegún hemos visto, existen diferentes tipos de variables, cualitativas (categóricas) y cuantitativas (métricas). Normalmente, los valores de las variables categóricas se describen mediante textos del tipo “color blanco”, “hombre”, “mujer”, “alto”, “bajo”, etc.  Suelen corresponder con características descriptivas, y por lo tanto, no puede hacerse cálculos directamente con ellos, a menos que se hayan resumido, por ejemplo, mediante un conteo. Las variables métricas consisten en valores numéricos, que pueden ser enteros o continuos, y pueden utilizarse directamente para hacer cálculos tales como sumas, etc.A partir de aquí utilizaremos una nomenclatura compatible con las hojas de cálculo en formato europeo para escribir los números; usaremos la coma para la separación decimal y el punto y coma para la separación de los números cuando los escribamos de forma seriada.\nUna variable está descrita siempre por un nombre, que designa la variable, y un valor o conjunto de valores, que corresponden a los casos. Este conjunto de valores, como acabamos de ver, pueden ser textos o números.\nExisten también otros tipos de variables que veremos más tarde, como variables lógicas o fechas, según el tipo de dato que almacenemos en esa variable.\nEjemplos de valores de texto: “Carlos”, “fruta”, “Lluvia fuerte”, “muy ácido”, “sabor a fresa”\nEjemplos de valores numéricos: \\(1\\); \\(7\\); \\(10,65\\)\nSiempre que sea posible, utilizaremos el nombre del atributo o característica que estamos midiendo o analizando, o su abreviatura, para designar una variable; por ejemplo, si estamos recogiendo la altura de una serie de personas, llamaremos altura a la variable; si estamos recogiendo el peso, usaremos el nombre peso, etc.\nA veces, asignar un nombre a una variable no es todo lo fácil que podría parecer a simple vista. Por ejemplo, ¿qué nombre daríamos a una variable que va a recoger los valores de \\(pH\\) de la leche en una cuba de queso en el momento de añadir el cuajo? Está claro que \\(pH\\) no es suficiente, porque en el proceso hay varias medidas de \\(pH\\) y sería bueno que pudiésemos diferenciarlas con facilidad. En un caso como éste, es probable que necesitemos utilizar varias palabras o abreviaturas que describan mejor el nombre de la variable.\nPara la construcción correcta de estos nombres, se han establecido un conjunto de reglas, con el objetivo de evitar errores y facilitar el intercambio de los datos entre diferentes programas de análisis. Son éstas:\n\nUn nombre válido consiste en una combinación de letras, números y signo de subrayado (\\(\\_\\))\nUn nombre de variable no puede empezar por un número, un punto o un signo de subrayado (\\(\\_\\)); debe empezar siempre por una letra.\nLos nombres de variables irán siempre en minúsculas. Según esta regla, \\(Peso\\) no es un nombre válido, pero \\(peso\\) si lo es.\nNo utilizaremos espacios en blanco, acentos ni caracteres especiales como \\(\\tilde{n}\\), \\(\\%\\), guiones o paréntesis.\nHay veces en que nos interesa unir varias palabras para construir un nombre de variable. Se utilizan diferentes formas de unir palabras, por ejemplo:\n\nun punto, como en \\(peso.en.cm\\),\nlo que se ha llamado escritura de camello (camelCase), que se llama así por el uso de mayúsculas y minúsculas mezcladas (\\(PesoEnCm\\))\nel signo de subrayado \\(\\_\\), como en \\(peso\\_en\\_cm\\)\n\nAlgunas de estas opciones son utilizadas en distintas comunidades de usuarios, por ejemplo la opción 1 es utilizada en la guía de estilo de Google, y la opción 2 es muy utilizada por los programadores del entorno de los lenguajes de Microsoft. Nosotros utilizaremos el signo de subrayado (\\(\\_\\)), que es la forma más usada en el entorno de programación de R.\nSiempre se separarán las palabras mediante el signo de subrayado (_) para facilitar la lectura. Así, aunque \\(temperatura1\\) es un nombre válido, preferiremos \\(temp\\_1\\); es más corto y de lectura más clara. Igualmente, preferiremos \\(peso\\_empaquetado\\) a \\(pesoempaquetado\\)\nMantendremos los nombres razonablemente cortos para facilitar la lectura. Aunque podemos hacer los nombres todo lo largos que queramos, es más cómodo utilizar nombres cortos. Por ejemplo, podríamos utilizar \\(temperatura\\_de\\_la\\_leche\\_al\\_cuajar\\), pero preferiremos abreviarlo como \\(temp\\_cuajo\\).\n\nNombres no válidos:\n\n\\(peso\\ en\\ gramos\\) (contiene espacios)\n\\(pH\\_de\\_la\\_leche\\_en\\_Recepci\\acute{o}n\\) (demasiado largo, tiene un acento, tiene mayúsculas)\n\\(extracto\\_seco\\_total\\_a\\_la\\_salida\\_de\\_la\\_salmuera\\) (demasiado largo)\n\nAlternativas válidas:\n\n\\(peso\\_g\\)\n\\(pH\\_leche\\_rec\\) (en este caso, de manera excepcional, podemos mantener el uso de la mayúscula por corrección formal)\n\\(est\\_salida\\_sal\\)\n\nUn caso particular es el uso de la \\(\\tilde{n}\\), ya que no hay una alternativa fácil para el uso en las fechas (\\(a\\tilde{n}o\\)). R admite el uso de la \\(\\tilde{n}\\) en los nombres de variables, por lo que podremos usarlo con cuidado, poniendo atención a los posibles errores que se pudiesen producir en algunas librerías.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#el-flujo-de-trabajo",
    "href": "010-organizacion.html#el-flujo-de-trabajo",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.2 El flujo de trabajo",
    "text": "4.2 El flujo de trabajo\nUn flujo de trabajo en análisis de datos es un proceso sistemático y estructurado que guía la manipulación, exploración y análisis de datos desde su recolección hasta la obtención de resultados finales y su comunicación. Es una hoja de ruta que asegura que cada paso se realice de manera ordenada, eficiente y reproducible, facilitando la comprensión y utilización de los datos.\nHadley Wickham (Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel 2023) ha propuesto un método de flujo de trabajo que se ha convertido en estándar en la ciencia de datos (hay versión en español: (Garret Grolemund Hadley Wickham 2023))\n\nEste flujo de trabajo abarca diversas actividades como la importación de datos, su limpieza y transformación, el análisis exploratorio, y el modelado, culminando en la interpretación y presentación de los resultados. Todo esto se hace siguiendo metodologías específicas para asegurar la calidad y precisión del análisis.\nLa importancia de seguir un flujo de trabajo bien definido radica en la capacidad de replicar estudios, minimizar errores y fomentar la transparencia, permitiendo que cualquier persona pueda entender y validar las decisiones tomadas durante el análisis. Además, mejora la eficiencia al estandarizar procedimientos y facilita la colaboración entre diferentes analistas o equipos de trabajo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#etapas-en-un-flujo-de-trabajo-estructurado.",
    "href": "010-organizacion.html#etapas-en-un-flujo-de-trabajo-estructurado.",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.3 Etapas en un flujo de trabajo estructurado.",
    "text": "4.3 Etapas en un flujo de trabajo estructurado.\n\nRecolección de datos\nLa primera etapa es la recolección de datos. Esto implica obtener datos desde diversas fuentes como archivos CSV, bases de datos, APIs, etc. La recolección de datos es fundamental porque la calidad del análisis depende de la calidad de los datos recolectados.\n\n\nInspección de los datos\nUna vez recolectados, se procede a inspeccionar los datos para entender su estructura y contenido. Esto incluye examinar los tipos de datos, la presencia de valores faltantes, duplicados y la distribución de las variables.\n\n\nLimpieza de los datos\nLa limpieza de datos es crucial para asegurar que la información sea precisa y esté en el formato adecuado. Esta etapa incluye:\n\nManejo de valores faltantes.\nEliminación de duplicados.\nCorrección de inconsistencias.\nTransformación de datos a un formato adecuado para el análisis.\n\n\n\nTransformación de los datos\nTransformar los datos a un formato ordenado o arreglado (tidy) es esencial. Según Wickham, los datos arreglados tienen una estructura clara: cada variable es una columna, cada observación es una fila, y cada valor tiene su propia celda. Este formato facilita el análisis y la visualización de datos.\n\n\nAnálisis exploratorio de datos (EDA)\nEl Análisis Exploratorio de Datos (EDA) busca entender los patrones y relaciones en los datos mediante estadísticas descriptivas y visualizaciones. Durante esta etapa se realizan:\n\nEstadísticas básicas (media, mediana, desviación estándar).\nGráficos y diagramas para visualizar la distribución de los datos y las relaciones entre variables.\n\n\n\nModelado de datos\nDependiendo del objetivo del análisis, se pueden aplicar diversos modelos estadísticos para extraer información y hacer predicciones. Esto puede incluir:\n\nModelos de regresión.\nAnálisis de clasificación.\nModelos de series temporales, entre otros.\n\n\n\nComunicación de resultados\nFinalmente, es fundamental comunicar los resultados de manera clara y efectiva. Esto se hace a través de:\n\nTablas y resúmenes interpretativos.\nGráficos y visualizaciones.\nInformes y presentaciones que expliquen los hallazgos y sus implicaciones.\n\nSiguiendo estos pasos, puedes manejar y analizar datos de manera organizada y reproducible, facilitando la colaboración y la toma de decisiones informadas. Este flujo de trabajo asegura que los datos se traten de manera sistemática, desde su recolección hasta la comunicación de los resultados.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#razones-para-seguir-un-flujo-de-trabajo",
    "href": "010-organizacion.html#razones-para-seguir-un-flujo-de-trabajo",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.4 Razones para seguir un flujo de trabajo",
    "text": "4.4 Razones para seguir un flujo de trabajo\n\nReproducibilidad: Un flujo de trabajo organizado permite que los análisis sean reproducibles. Otros pueden seguir los mismos pasos para obtener resultados similares, lo que es crucial en la investigación y en la toma de decisiones basadas en datos.\nConsistencia: Ayuda a asegurar que los pasos se realizan de manera consistente cada vez que se ejecuta el análisis, reduciendo la posibilidad de errores humanos.\nTransparencia: Proporciona un registro claro de los pasos tomados durante el análisis, facilitando la revisión y validación de los resultados.\nEficiencia: Mejora la eficiencia al estandarizar el proceso, permitiendo a los analistas concentrarse en el análisis y la interpretación de los datos en lugar de tareas repetitivas.\nColaboración: Facilita la colaboración entre equipos, ya que los flujos de trabajo bien documentados permiten que otros comprendan fácilmente los métodos y pasos utilizados.\nAdaptabilidad: Permite adaptar y ajustar el análisis de manera más fácil cuando se presentan nuevos datos o cuando cambian los objetivos del análisis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#problemas-de-no-seguir-un-flujo-de-trabajo-estructurado",
    "href": "010-organizacion.html#problemas-de-no-seguir-un-flujo-de-trabajo-estructurado",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.5 Problemas de no seguir un flujo de trabajo estructurado",
    "text": "4.5 Problemas de no seguir un flujo de trabajo estructurado\n\nErrores y Sesgos: La falta de un enfoque estructurado puede resultar en errores y sesgos inadvertidos en el análisis, lo que puede llevar a conclusiones incorrectas.\nDificultad para Replicar Resultados: Sin un flujo de trabajo claro, replicar resultados se vuelve complicado, lo que puede afectar la credibilidad del análisis y la capacidad de validación por otros.\nFalta de Documentación: La ausencia de una documentación adecuada dificulta entender los pasos y las decisiones tomadas durante el análisis, lo que puede ser un obstáculo en auditorías y revisiones.\nIneficiencia: Sin una estructura clara, los analistas pueden gastar tiempo valioso realizando tareas repetitivas y resolviendo problemas que podrían haberse evitado con un enfoque más organizado.\nProblemas de Colaboración: La colaboración se vuelve más difícil si los miembros del equipo no pueden seguir o entender los pasos tomados por otros, lo que puede llevar a malentendidos y duplicación de esfuerzos.\nDificultad para Adaptarse a Cambios: Sin un flujo de trabajo definido, adaptar el análisis a nuevos datos o cambios en los objetivos puede ser más complejo y propenso a errores.\n\nEn resumen, seguir un flujo de trabajo estructurado es esencial para garantizar la precisión, eficiencia, y reproducibilidad del análisis de datos, evitando problemas que puedan comprometer la integridad y utilidad de los resultados. .",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#un-ejemplo-revisando-los-datos-existentes.",
    "href": "010-organizacion.html#un-ejemplo-revisando-los-datos-existentes.",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.6 Un ejemplo: revisando los datos existentes.",
    "text": "4.6 Un ejemplo: revisando los datos existentes.\nCuando nos incorporamos a un equipo de trabajo existente, lo más seguro es que ya se disponga de un sistema de archivo de los datos, de acuerdo con los métodos habituales del equipo. En muchos casos, el diseño de la captura de datos sigue aproximadamente el modelo manual en papel; se introducen los datos en la hoja de cálculo y una vez completados, se imprime el documento para su archivo.\nEl error más común que se suele cometer es, precisamente, tratar la hoja de cálculo como un bloc de notas, es decir, hacer anotaciones de forma libre, colocar los datos y el resultado de los análisis al lado y en cualquier parte de la hoja, y apoyarnos en el contexto para interpretar lo que hemos guardado. Pero para que el ordenador sea capaz de analizar nuestros datos de manera eficiente, debemos estructurarlos de tal forma que el programa use la información tal como nosotros queremos.\nEs común utilizar una hoja para guardar múltiples tablas de datos, tal como vemos en la Figura 4.1. Esta estructura, sin embargo, resulta enormemente confusa para su análisis, o lo imposibilita completamente.\n\n\n\n\n\n\nFigura 4.1: Hoja Excel desordenada: ¡No hagas esto!\n\n\n\nEn otros casos, los datos se guardan en hojas de cálculo que se componen de diferentes pestañas para cada semana, cada mes o cada año, como vemos en la Figura 4.2. Sin embargo, esta forma de almacenar los datos tampoco es la óptima para su análisis.\n\n\n\n\n\n\nFigura 4.2: Hoja Excel con una estructura no ordenada\n\n\n\nSi las diferentes tablas presentan situaciones diferentes, o datos que no están relacionados, podemos utilizar diferentes pestañas. Pero si los datos están vinculados, por ejemplo, se corresponden con las mismas medidas, hechas en fechas diferentes (meses, años), la respuesta es que las pestañas no son la forma correcta de almacenarlos datos; lo mejor es añadir una variable que nos permita diferenciar los datos por fecha; nuestro programa de análisis nos permitirá filtrar los datos según la fecha que deseemos, y todos estarán en una única tabla, facilitando la coherencia y el análisis posterior.\nHay muchas formas de almacenar la información en una hoja de cálculo, pero sólo la estructura de datos ordenados o arreglados facilita la utilización de los datos tanto por la hoja de cálculo como por otros programas de análisis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#los-datos-ordenados-o-arreglados-tidy-data",
    "href": "010-organizacion.html#los-datos-ordenados-o-arreglados-tidy-data",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.7 Los datos ordenados o arreglados (tidy data)",
    "text": "4.7 Los datos ordenados o arreglados (tidy data)\nDe la misma manera que la gramática permite ordenar y estructurar un escrito de acuerdo a reglas comunes, hay reglas para que el almacenamiento de los datos sea lo más homogéneo posible y se reduzcan los errores al mínimo.\nLas reglas principales al almacenar nuestros datos en una hoja de cálculo son tres:\n\ncolumnas=variables,\nfilas=observaciones,\nceldas=valores.\n\nCada variable debe tener su propia columna, cada observación debe tener su propia fila, y cada valor debe tener su propia celda o casilla .\nEstas tres reglas básicas son las que hacen que nuestro conjunto de datos sea ordenado (o arreglado)(Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel 2023) (hay edición online en español: (Garret Grolemund Hadley Wickham 2023):\nLa Figura 4.3 muestra estas reglas de forma visual.\n\n\n\n\n\n\nFigura 4.3: Sigue estas tres reglas para tener un conjunto de datos ordenado: las variables están en columnas, las observaciones están en filas, y los valores están en celdas. Fuente de la imagen: Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel (2023)\n\n\n\nEstas tres reglas están interrelacionadas porque es imposible satisfacer sólo dos de tres.\nEn una hoja de cálculo, una tabla de datos arreglada tendría este aspecto:\n\n\n\n\n\n\nFigura 4.4: Hoja Excel con estructura rectangular de datos ordenados\n\n\n\nDatos rectangulares: formato tabla",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#qué-es-un-fichero-plano-y-un-fichero-csv",
    "href": "010-organizacion.html#qué-es-un-fichero-plano-y-un-fichero-csv",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.8 Qué es un fichero plano y un fichero CSV",
    "text": "4.8 Qué es un fichero plano y un fichero CSV\nSe suele llamar fichero plano a un fichero de datos de texto sin ningún tipo de formato, donde los datos están separados por espacios o tabulaciones. Muchos equipos automáticos, como balanzas de laboratorio o básculas de proceso, producen ficheros planos de texto, que se pueden importar a Excel o R. Un fichero CSV es un fichero plano en el que los valores están separados por un carácter especial, llamado separador. Este separador puede ser una coma , (cuando los decimales se separan mediante un punto, como en EEUU) o un punto y coma ; (cuando los decimales se separan mediante una coma, como en España)\n\n\n\n\n\n\n\n\nFichero plano separado por espacios\n\n\n\n\n\n\n\nFichero CSV separado por comas (USA, GB)\n\n\n\n\n\n\n\n\n\nFichero CSV separado por puntos y comas (Europa, España)\n\n\n\n\n\n\nFigura 4.5: Tres tipos de ficheros planos de texto.\n\n\n\nEn un fichero plano o en un fichero CSV, la primera fila puede contener los nombres de las columnas. En algunos casos, los elementos de texto pueden estar entre comillas. En estos casos, los programas de importación se ocupan de la conversión de formatos.\nLa importación de un fichero CSV en Excel en español es directa si se ha generado con puntos y comas como separador y comas para los decimales; si no es así, nos aparecerá como un fichero plano de texto sin formato, y tendremos que realizar una conversión.\n\nCómo exportar un fichero CSV desde Excel.\nUna vez que tenemos nuestros datos en Excel, hay dos formas en las que podemos poner los datos a disposición de Python para su análisis: exportarlos a un archivo de texto con formato CSV, o leer directamente los datos de Excel desde Python utilizando las funciónes adecuadas. En ambos casos, el resultado es un DataFrame o cuadro de datos, que es una estructura equivalente a la de nuestra tabla de datos en Excel. En el capçitulo siguiente veremos cómo realizar esta importación de datos Python desde nuestro fichero CSV\nEn el ejemplo, crearemos un fichero CSV desde Excel y haremos un manejo básico de los datos.\n\nPaso 1: Guardar el Fichero CSV desde Excel\n\nAbre tu fichero en Excel.\nSelecciona Archivo &gt; Guardar como.\nElige el formato CSV (delimitado por comas) (*.csv).\nGuarda el archivo.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#practica-con-el-csv",
    "href": "010-organizacion.html#practica-con-el-csv",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.9 Practica con el CSV",
    "text": "4.9 Practica con el CSV\n\nEn Excel, prueba a guardar y recuperar tus datos en formato CSV.\n\nUna vez cargados los datos en Excel,\n\nselecciona el menú Datos &gt; Filtro\n\ny prueba a filtrar y seleccionar los datos según diferentes criterios",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "010-organizacion.html#para-resolver-1",
    "href": "010-organizacion.html#para-resolver-1",
    "title": "4  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "4.10 Para resolver",
    "text": "4.10 Para resolver\nPoner aquí distintos ejemplos de nombres de variables para ver si son válidos o no. Describir medidas y preguntar cómo llamaríamos a esa variable (por ejemplo, temperatura de la leche que acabamos de descargar de una cisterna) :::\n\n\n\n\nHadley Wickham, Garret Grolemund. 2023. «R Para Ciencia de Datos». 2023. https://es.r4ds.hadley.nz/.\n\n\nHadley Wickham, Garret Grolemund, Mine Çetinkaya-Rundel. 2023. R for Data Science, 2nd ed. 1005 Gravenstein Highway North, Sebastopol, CA95472: O’Reilly Media Inc. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html",
    "href": "020-intro-python.html",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "",
    "text": "5.1 Cómo usar este cuaderno\nPython es un lenguaje de programación. Para trabajar con el lenguaje, se han desarrollado diferentes interfaces que mejoran el manejo del programa y facilitan el trabajo.\nNosotros utilizaremos Google Colaborate, conocido como Colaboratory o simplemente, Google Colab, basado en Jupyter, entorno originalmente desarrollado para Python y que se ha convertido en universal para muchos lenguajes de programación. En este entorno, podemos escribir código y ejecutarlo. También podemos escribir en celdas de texto, lo que nos permite intercalar explicaciones a nuestros cálculos o cualquier otro contenido escrito.\nEn este cuaderno vamos a ver algunas características de Python que resultan importantes para nuestro objetivo de análisis de datos industriales. Usaremos para ello fuentes de datos disponibles en la web y veremos diferentes aplicaciones del lenguaje sobre dichas fuentes de datos. Aunque el código resulte complejo a primera vista, el objetivo es simplemente una toma de contacto, en cuadernos sucesivos veremos datos reales y aplicación a la industria láctea.\nCon el fin de ayudar al desarrollo de la sesión de estudio, este cuaderno se puede seguir en el texto a continuación o puede ser abierto en Colab para trabajar directamente sobre él. Una vez abierto, es conveniente guardarlo en el espacio de Colab en el Google Drive de cada uno.\nAbrir cuaderno de introducción a Pythonen Google Colab/\nDesargar los datos de ejemplo del archivo camembert.csv\nEn este cuaderno usaremos varios archivos de datos de demostración, ya existentes en diversas fuentes de Python, que nos servirán para hacer diferentes ejercicios numéricos y gráficos.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#cómo-usar-este-cuaderno",
    "href": "020-intro-python.html#cómo-usar-este-cuaderno",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "",
    "text": "Si has abierto el cuaderno en Colab por primera vez, sigue estos pasos:\n\nGuarda una copia en tu carpeta de Google Drive. Para ello, vete a Archivo/Guardar una copia en Drive y guárdalo.\nUna vez guardada la copia, puedes cambiar el nombre en ’Archivo/Cambiar nombre`, o bien directamente haciendo click sobre el nombre en la parte superior de la hoja.\nSI usas la versión web de Drive (abriendo el Drive en el navegador), posiblemente tendrás activada la opción de abrir los cuadernos que tengas en tu carpeta usando el botón derecho de tu ratón.\n\nSi vas a crear una hoja nueva (en blanco) mediante el enlace a Google Colaborate, sigue estos pasos:\n\nHaz click en el enlace a continuacion https://colab.research.google.com/, o copia y pega el texto en la barra de navegación de tu navegador, o simplemente introduce “Google Colab” en la barra de búsqueda y haz click sobre el resultado correcto de la búsqueda, irás directamente al sitio.\nAbre un cuaderno nuevo.\nGuarda una copia en Archivo/Guardar una copia como\nRenombra la hoja en Archivo/Cambiar nombre usando el nombre que quieras",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#primeros-pasos",
    "href": "020-intro-python.html#primeros-pasos",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.2 Primeros pasos",
    "text": "5.2 Primeros pasos\nColab funciona en un entorno Jupyter En este entorno, el texto y el código Pythonse introducen en celdas.\nHay dos tipos de celdas: texto y código. En la parte superior de la ventana, puedes elegir el tipo que quieres insertar haciendo click en + Código o bien en + Texto\nPara editar una celda de texto existente, haz doble click sobre ella. Ahora puedes escribir usando las convenciones de código markdown. Cuando hayas terminado, ejecuta la celda para que Colab formatee y presente el texto.\nPara ejecutar una celda de código, haz click sobre ella.\n\nShift-Enterejecuta el código y pasa a la siguiente celda.\nCtrl-Enterejecuta el código y permanece en la misma celda\n\nRecuerda también que las celdas de código deben ejecutarse en orden secuencial desde el principio\nSi la ejecución de una celda de código produce un error, no te preocupes, no tiene consecuencias graves. Simplemente, edita el código y corrige el error. En muchos casos, Google Gemini te puede ayudar a detectar los errores en el código y reescribirlo correctamente.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#cálculos-simples-con-python",
    "href": "020-intro-python.html#cálculos-simples-con-python",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.3 Cálculos simples con python",
    "text": "5.3 Cálculos simples con python\nEn su forma más básica, python se puede utilizar como una simple calculadora, utilizando los siguientes operadores aritméticos:\nAdición: \\(+\\) (Ejemplo: \\(2+2\\))\nResta: \\(-\\) (Ejemplo: \\(2-2\\))\nMultiplicación: \\(*\\) (Ejemplo: \\(2*2\\))\nDivisión: \\(/\\) (Ejemplo: \\(2/2\\))\nExponenciación: ** (Ejemplo: \\(2\\)**\\(2\\))\nPrueba a ejecutar las siguientes celdas:\n\n2+2\n\n4\n\n\n\n2-2\n\n0\n\n\n\n2*2\n\n4\n\n\n\n2/2\n\n1.0\n\n\n\n2**2\n\n4\n\n\n\n(3*4)/2\n\n6.0",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#creación-y-asignación-de-variables",
    "href": "020-intro-python.html#creación-y-asignación-de-variables",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.4 Creación y asignación de variables",
    "text": "5.4 Creación y asignación de variables\nUna variable permite almacenar un valor (por ejemplo, 4) o un objeto (por ejemplo, una descripción de función). Más tarde se puede usar el nombre de esta variable para acceder fácilmente al valor o al objeto que está almacenado dentro de ella.\nPodemos imaginar este concepto como una estantería llena de celdas vacías, en las cuales podemos colocar diferentes objetos: números, letras, frases, etc La variable es el espacio que almacena un valor, y que podemos llamar u obtener simplemente escribiendo su nombre.\nEn python, una variable es un objeto. Podemos asignar valores a ese objeto, como si colocásemos libros en nuestra estantería. Esta asignación se hace siempre con el operador de asignación =, en la forma\n\\(nombre\\_de\\_objeto = valor\\)\nPor ejemplo, asignaremos un valor 4 a una variable que se llame mi_var con el operador de asignación de la siguiente forma:\n\\(mi\\_var = 4\\)\nRecuerda que ahora estamos en una celda de texto; prueba a hacer la asignación en la celda de código a continuación, añadiendo el operador de asignación y ejecutando la celda con Mayus-Intro:\n\nmi_var 4\n\nUna vez que asignamos un valor a una variable, python recuerda su valor hasta que lo cambiemos mediante una nueva asignación, borremos la variable, o finalicemos nuestra sesión.\nPara ver el contenido del objeto(lo que hemos almacenado en mi_var), escribimos el nombre y ejecutamos la celda (sólo si hemos editado y ejecutado correctamente la celda anterior, en caso contrario obtendremos un error porque mi_var no está definida):\n\nmi_var",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#cómo-establecer-los-nombres-de-variables",
    "href": "020-intro-python.html#cómo-establecer-los-nombres-de-variables",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.5 Cómo establecer los nombres de variables",
    "text": "5.5 Cómo establecer los nombres de variables\nPara facilitar la legibilidad del código, los nombres de variables en python deben cumplir algunas normas generales. Buscaremos siempre que los nombres de los objetos sean descriptivos, por lo que necesitamos una forma de unir varias palabras.\nLas convenciones más aceptadas para la designación de nombres de variables en Python se basan principalmente en la PEP 8, la guía de estilo oficial para el código Python. Aquí te resumo los puntos clave:\n\nSnake Case (Recomendado para variables y funciones):\n\nPara nombres de variables y funciones que constan de varias palabras, se utiliza snake_case, donde las palabras se escriben en minúscula y se separan por guiones bajos.\nEjemplos: mi_variable, nombre_completo, calcular_total.\n\nCamel Case (Para clases):\n\nPara nombres de clases, se utiliza CamelCase (también conocido como PascalCase), donde cada palabra comienza con mayúscula y no hay separadores.\nEjemplos: MiClase, GestionDeUsuarios, HttpRequest.\n\nConstantes (Mayúsculas con guiones bajos):\n\nPara constantes (variables cuyo valor no debería cambiar durante la ejecución del programa), se utiliza ALL_CAPS con guiones bajos para separar las palabras.\nEjemplos: MAX_VALOR, PI, TASA_INTERES.\n\nReglas generales (obligatorias por el lenguaje):\n\nLongitud: Las variables pueden tener cualquier longitud.\nCaracteres permitidos: Pueden contener letras (mayúsculas y minúsculas), números y guiones bajos (_).\nNo pueden empezar con un número: Un nombre de variable no puede comenzar con un dígito.\nSensibilidad a mayúsculas y minúsculas: Python distingue entre mayúsculas y minúsculas. mi_variable y Mi_Variable son nombres de variables diferentes.\nEvitar palabras reservadas: No se pueden usar palabras reservadas de Python (como if, else, for, while, etc.) como nombres de variables.\nNo usar caracteres especiales: No se permiten espacios, signos de puntuación u otros símbolos especiales.\n\nRecomendaciones de estilo:\n\nNombres descriptivos: Elige nombres de variables que sean claros y descriptivos, que reflejen su propósito en el programa. Esto mejora la legibilidad del código.\nEvitar nombres de una sola letra: A menos que sean contadores simples (i, j, k en bucles) o iteradores, es mejor evitar nombres de variables de una sola letra.\nEvitar nombres que se confundan: Ten cuidado con letras que pueden confundirse fácilmente, como la l minúscula y el número 1, o la O mayúscula y el número 0.\nGuiones bajos al principio (_nombre): Un solo guion bajo al principio (_) suele indicar que una variable o función es “interna” o “no pública” dentro de un módulo o clase. Es una convención, no una restricción estricta.\nDobles guiones bajos al principio (__nombre): Los dobles guiones bajos al principio de un nombre de atributo de clase (__nombre) activan el “name mangling” (mutilación de nombre) de Python, que hace que el atributo sea más difícil de acceder directamente desde fuera de la clase, actuando como una forma de “privacidad”.\n\n\nSeguir estas convenciones, especialmente las de la PEP 8, ayuda a que tu código sea más consistente, legible y fácil de entender por otros desarrolladores (y por ti mismo en el futuro).\nUsemos un nombre realmente largo para designar a una variable:\n\neste_es_un_nombre_realmente_largo = 5.3\n\nAhora recuperemos el valor que hemos almacenado\n\neste_es_un_nombre_realmente_largo\n\n5.3\n\n\nRecuerda que python utiliza la escritura anglosajona para la separación de decimales, usando un punto, y no una coma como en España. ¿Qué pasaría si utilizamos una coma para separar los decimales?\n\nmi_var = 2,5\n\nPython acepta la asignación, pero cuando investigamos la variable, el resultado es diferente de lo que esperábamos, no obtenemos un valor numérico:\n\nmi_var\n\n(2, 5)\n\n\nPython ha entendido que estábamos creando una tupla de dos valores (2 y 5) (en python, una tupla es una colección ordenada e inmutable de elementos)\nPara python las mayúsculas y minúsculas son diferentes: mi_var y mi_Var son variables diferentes.\nEn una variable python podemos almacenar también texto:\n\nmi_var = \"Esto es una frase 12345\"\n\nPython almacena esta cadena alfanumérica exactamente igual que antes hizo con los valores numéricos:\n\nmi_var\n\n'Esto es una frase 12345'\n\n\n\nmi_var = 123.45\n\n\nmi_var\n\n123.45\n\n\n\nmi_var*5\n\n617.25\n\n\nPara almacenar una cadena alfanumérica necesitamos encerrarla entre comillas, ya sean sencillas o dobles. De hecho, si almacenamos un número entre comillas, python no va aidentificarlo como un número, sino como un texto:\n\nmi_var = \"123.45\"\n\n\nmi_var\n\n'123.45'\n\n\n\nmi_var*5\n\n'123.45123.45123.45123.45123.45'\n\n\nComo ahora mi_var es una cadena de letras, python interpreta mi_var*5no como una multiplicación, sino como una instrucción para repetir esa cadena de caracteres cinco veces.\nEn el ejemplo que acabamos de hacer, seguro que te has dado cuenta de que python permite reutilizar los objetos simplemente reasignándoles el valor correspondiente. Al hacerlo, perdemos el valor original y lo sustituimos por el nuevo valor.\nHay que tener atención con las reglas de escritura: para asignar un texto a un objeto debemos tener cuidado de cerrar las comillas, si no lo hacemos, python nos advertirá del error:\n\nmi_var = \"Esto es una cadena alfanumerica que no hemos cerrado\n\n\nAlgunos ejercicios\nIntenta detectar los errores en las siguientes celdas de código:\n\nmi_variable = 10\nmi_varıable\n\n\nmi_var = 5\nMi_Var\n\nPodemos asignar un objeto a otro objeto con el operador de asignación:\n\nx = 5\ny = 3\n\n\nx + y\n\n8\n\n\n\ny = x\n\n\nx + y\n\n10\n\n\nSupongamos que tenemos 5 peras y 4 manzanas. Crea una variable que se llame fruta que sume el total de unidades de fruta que tenemos, insertando debajo de esta celda todas las celdas de código que necesites (Pista: ¡En Python sí podemos sumar peras con manzanas!)\n\n# Celda para el ejercicio de las frutas\nperas = 5\nmanzanas = 4\nfruta = peras + manzanas\nprint(fruta)\n\n9",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#los-tipos-de-datos",
    "href": "020-intro-python.html#los-tipos-de-datos",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.6 Los tipos de datos",
    "text": "5.6 Los tipos de datos\nHemos visto que python puede trabajar con diferentes tipos de datos, tales como números y textos.\nLos valores decimales como 4.5 se llaman flotantes(float). Los números enteros como 4 se llaman enteros(ìnt). Los valores booleanos (VERDADERO o FALSO) se denominan booleanos(bool o logical). Los valores de texto (o cadena alfanumérica) se denominan cadenas de caracteres. También se les llama simplemente cadenas. Las comillas en el editor indican que “un texto entre comillas” es una cadena de caracteres. En python, una cadena de caracteres puede escribirse entre comillas dobles, como \"pera\" o simples, como 'pera'\nAlgunos ejemplos:\n\nmi_var_numero = 4.5\nmi_var_texto = \"esto es un texto\"\nmi_var_texto_2 = 'esto también es un texto'\nmi_var_logica = False\n\nLas variables lógicas pueden tomar los valores True o False (recuerda que las mayúsculas son significativas en python: False no es lo mismo que false o FALSE). Python responde también con un valor lógico cuando hacemos una evaluación lógica. Por ejemplo,\n\nmi_var_numero == 4.5\n\nTrue\n\n\n\nmi_var_texto == 4.5\n\nFalse\n\n\nComo en python se usa = como operador de asignación, el operador lógico que prueba una igualdad es ==, un doble igual, y no un =, un igual sencillo. La desigualdad se verifica con el operador != que significa “no es igual a”.\nVeamos otros dos ejemplos que proporcionan una respuesta de valor lógico:\n\nmi_var_numero == 10\n\nFalse\n\n\n\nmi_var_numero &gt; 5\n\nFalse\n\n\n\nmi_var_numero != 5 # el operador != significa \"no es igual a\"\n\nTrue",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#las-funciones-en-python-qué-son-y-por-qué-las-usamos",
    "href": "020-intro-python.html#las-funciones-en-python-qué-son-y-por-qué-las-usamos",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.7 Las Funciones en Python: ¿Qué son y por qué las usamos?",
    "text": "5.7 Las Funciones en Python: ¿Qué son y por qué las usamos?\nHasta ahora hemos usado operadores aritméticos para realizar cálculos básicos y hemos asignado valores a variables. Pero, ¿qué pasa si queremos realizar tareas más complejas o repetir una misma operación muchas veces? Aquí es donde entran las funciones.\nUna función es un bloque de código organizado y reutilizable que se utiliza para realizar una única acción relacionada. Piensa en una función como una “receta” o un “mini-programa” que encapsula una serie de pasos. En lugar de escribir esos pasos una y otra vez, simplemente “llamamos” a la función por su nombre.\n¿Por qué son útiles las funciones?\n\nReutilización de Código: Evitan tener que escribir el mismo código varias veces.\nModularidad: Dividen el programa en partes más pequeñas y manejables.\nClaridad: Hacen el código más fácil de leer y entender.\n\nPython ya trae consigo muchas funciones integradas (built-in functions) que podemos usar directamente. Veamos algunos ejemplos muy comunes:\n\nprint(): Mostrar información\nEsta función ya la hemos usado. Nos permite mostrar valores, texto o el contenido de variables en la consola\n\nprint(\"¡Hola, esto es una función!\")\nmi_nombre = \"Juan\"\nprint(\"Mi nombre es:\", mi_nombre)\n\n¡Hola, esto es una función!\nMi nombre es: Juan\n\n\n\n\nlen(): Obtener la longitud\nEsta función nos devuelve el número de elementos de un objeto, como el número de caracteres en una cadena de texto o el número de elementos en una lista.\n\nfrase = \"Programar en Python es divertido\"\nlongitud_frase = len(frase)\nprint(\"La frase tiene\", longitud_frase, \"caracteres.\")\n\nmi_lista_numeros = [10, 20, 30, 40]\nprint(\"Mi lista tiene\", len(mi_lista_numeros), \"elementos.\")\n\nLa frase tiene 32 caracteres.\nMi lista tiene 4 elementos.\n\n\n\n\ntype(): Conocer el tipo de dato\nÚtil para verificar qué tipo de dato tiene una variable.\n\nmi_numero = 100\nmi_texto = \"Cien\"\nmi_booleano = True\n\nprint(type(mi_numero))\nprint(type(mi_texto))\nprint(type(mi_booleano))\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n&lt;class 'bool'&gt;\n\n\n\n\n¿Cómo funcionan las funciones? Argumentos y Retorno\n\nArgumentos: Muchas funciones necesitan información para poder trabajar. Esta información se les pasa entre paréntesis, y se llaman argumentos. Por ejemplo, a print() le pasamos lo que queremos mostrar.\nRetorno de Valores: Algunas funciones realizan un cálculo o una operación y nos devuelven un resultado. Por ejemplo, len() nos devuelve un número.\n\nEntender las funciones es clave, porque las librerías que usaremos a continuación son, en esencia, grandes colecciones de funciones especializadas.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#importando-herramientas-bibliotecas-y-módulos",
    "href": "020-intro-python.html#importando-herramientas-bibliotecas-y-módulos",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.8 Importando Herramientas: Bibliotecas y Módulos",
    "text": "5.8 Importando Herramientas: Bibliotecas y Módulos\nPython es un lenguaje muy potente, pero su verdadera fuerza reside en su vasto ecosistema de bibliotecas (también llamadas módulos o paquetes).\nUna biblioteca es una colección de código (funciones, clases, etc.) escrito por otros programadores y disponible para que tú lo uses en tus propios proyectos. Imagina que son “cajas de herramientas” especializadas:\n\n¿Necesitas hacer cálculos matemáticos complejos? Hay una librería para eso.\n¿Necesitas manipular datos en tablas? Hay una librería para eso.\n¿Necesitas crear gráficos? ¡También hay librerías para eso!\n\nUsar librerías nos permite evitar “reinventar la rueda”, ahorrando mucho tiempo y esfuerzo, y aprovechando código que ya ha sido probado y optimizado.\nPara poder usar las funciones y herramientas que están dentro de una librería, primero debemos importarla a nuestro entorno de trabajo. La forma más común de hacerlo es con la instrucción import.\n\nSintaxis de import\nLa sintaxis básica es:\nimport nombre_de_la_libreria\nSin embargo, para las librerías de análisis de datos, es muy común y recomendado usar un alias (un nombre corto) para hacer el código más conciso y fácil de leer. Esto se hace con la palabra clave as:\nimport nombre_de_la_libreria as alias\nAhora vamos a importar las librerías que serán esenciales para trabajar con datos, realizar cálculos estadísticos y crear gráficos en Python.\n\n# Importando las librerías fundamentales para el análisis de datos\n\n# Pandas: Es la librería principal para trabajar con estructuras de datos tabulares\n#         como Series (vectores) y DataFrames (tablas).\nimport pandas as pd\n\n# NumPy: Proporciona soporte para arreglos (arrays) y matrices multidimensionales,\n#        además de funciones matemáticas de alto nivel. Pandas se construye sobre NumPy.\nimport numpy as np\n\n# Matplotlib.pyplot: Es la librería base para crear gráficos en Python.\n#                    Seaborn la usa \"por debajo\" para dibujar.\nimport matplotlib.pyplot as plt\n\n# Seaborn: Es una librería de visualización de datos de alto nivel\n#          basada en Matplotlib. Facilita la creación de gráficos estadísticos atractivos.\nimport seaborn as sns\n\nprint(\"¡Librerías principales para el análisis de datos importadas correctamente!\")\nprint(\"Ahora podemos acceder a sus funciones usando sus alias (pd, np, plt, sns).\")\n\n¡Librerías principales para el análisis de datos importadas correctamente!\nAhora podemos acceder a sus funciones usando sus alias (pd, np, plt, sns).\n\n\nUna vez que hemos importado una librería con un alias (por ejemplo, pandas as pd), para usar una función de esa librería, escribimos el alias seguido de un punto y el nombre de la función: pd.nombre_de_la_funcion().",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#las-principales-estructuras-de-datos-en-python-para-el-análisis",
    "href": "020-intro-python.html#las-principales-estructuras-de-datos-en-python-para-el-análisis",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.9 Las principales estructuras de datos en Python para el análisis",
    "text": "5.9 Las principales estructuras de datos en Python para el análisis\nAhora que entendemos las funciones y cómo importar librerías, podemos introducir las estructuras de datos que nos permitirán organizar y manipular nuestros datos de manera eficiente. Las dos más importantes en el contexto del análisis de datos con Python (y con la librería Pandas) son las Series y los DataFrames.\n\nSeries de pandas (El “vector” en Python para datos)\nSi en otros lenguajes de programación o herramientas estadísticas has trabajado con el concepto de “vector” (una secuencia de valores de un mismo tipo), el equivalente más directo en Python, usando pandas, es una serie.\n\nConcepto: Una serie de pandas es un arreglo unidimensional (como una columna de una hoja de cálculo o un vector). Puede contener cualquier tipo de dato (números enteros, flotantes, texto, booleanos, etc.) y tiene un índice asociado para acceder a sus elementos.\n\nCreación de una serie: Podemos crear una Serie a partir de una lista de Python.\n\n# Crear una lista de edades\nlista_edades = [25, 30, 22, 35, 28, 40, 33, 29]\n\n# Convertir la lista a una Serie de Pandas\n# Usamos 'pd.' porque la función Series() viene de la librería Pandas. Atención: la función se escribe con mayúsculas.\nedades_serie = pd.Series(lista_edades)\n\nprint(\"Mi Serie de Edades:\")\nprint(edades_serie)\n\nMi Serie de Edades:\n0    25\n1    30\n2    22\n3    35\n4    28\n5    40\n6    33\n7    29\ndtype: int64\n\n\nOperaciones básicas con series: Las series tienen muchas funciones y métodos útiles. Por ejemplo, podemos calcular la media:\n\nprint(\"\\nEdad promedio:\", edades_serie.mean()) # Usando el método .mean() de la Serie\nprint(\"Edad máxima:\", edades_serie.max())\nprint(\"Edad mínima:\", edades_serie.min())\n\n\nEdad promedio: 30.25\nEdad máxima: 40\nEdad mínima: 22\n\n\n\n\nDataFrames de Pandas (La “Tabla” o “Hoja de Cálculo”)\nEl DataFrame es la estructura de datos más importante y utilizada en pandas para el análisis de datos. Es el equivalente a una tabla en una base de datos, una hoja de cálculo de Excel, o un “data frame” en R.\n\nConcepto: Un DataFrame es una estructura bidimensional (filas y columnas). Piensa en ella como una colección de series (columnas) que comparten el mismo índice (las filas). Cada columna en un DataFrame es una serie de pandas.\n\nCreación de un DataFrame (a partir de un diccionario): Una forma sencilla de crear un DataFrame pequeño es a partir de un diccionario de Python, donde las claves del diccionario se convierten en los nombres de las columnas y los valores son listas que se convierten en las columnas.\n\n# Crear un diccionario con datos\ndatos_alumnos = {\n    'Nombre': ['Ana', 'Luis', 'Marta', 'Pedro', 'Sofía'],\n    'Edad': [24, 30, 28, 35, 26],\n    'Ciudad': ['Madrid', 'Barcelona', 'Valencia', 'Sevilla', 'Málaga'],\n    'Calificación': [8.5, 7.2, 9.1, 6.8, 8.9]\n}\n\n# Crear un DataFrame a partir del diccionario\ndf_alumnos = pd.DataFrame(datos_alumnos)\n\nprint(\"\\nDataFrame de Alumnos:\")\nprint(df_alumnos)\n\n\nDataFrame de Alumnos:\n  Nombre  Edad     Ciudad  Calificación\n0    Ana    24     Madrid           8.5\n1   Luis    30  Barcelona           7.2\n2  Marta    28   Valencia           9.1\n3  Pedro    35    Sevilla           6.8\n4  Sofía    26     Málaga           8.9\n\n\nVisualización y exploración básica de DataFrames:\nEs fundamental poder echar un vistazo rápido a los datos una vez que están cargados en un DataFrame.\n\ndf.head(): Muestra las primeras 5 filas (útil para ver la estructura).\ndf.tail(): Muestra las últimas 5 filas.\ndf.info(): Proporciona un resumen conciso del DataFrame, incluyendo el número de entradas, columnas, tipos de datos no nulos y uso de memoria.\ndf.describe(): Genera estadísticas descriptivas (conteo, media, desviación estándar, etc.) de las columnas numéricas.\n\n\nprint(\"\\nPrimeras 3 filas del DataFrame de alumnos:\")\nprint(df_alumnos.head(3)) # Podemos especificar el número de filas\n\nprint(\"\\nInformación del DataFrame de alumnos:\")\ndf_alumnos.info()\n\nprint(\"\\nEstadísticas descriptivas del DataFrame de alumnos:\")\nprint(df_alumnos.describe())\n\n\nPrimeras 3 filas del DataFrame de alumnos:\n  Nombre  Edad     Ciudad  Calificación\n0    Ana    24     Madrid           8.5\n1   Luis    30  Barcelona           7.2\n2  Marta    28   Valencia           9.1\n\nInformación del DataFrame de alumnos:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Nombre        5 non-null      object \n 1   Edad          5 non-null      int64  \n 2   Ciudad        5 non-null      object \n 3   Calificación  5 non-null      float64\ndtypes: float64(1), int64(1), object(2)\nmemory usage: 292.0+ bytes\n\nEstadísticas descriptivas del DataFrame de alumnos:\n            Edad  Calificación\ncount   5.000000      5.000000\nmean   28.600000      8.100000\nstd     4.219005      1.036822\nmin    24.000000      6.800000\n25%    26.000000      7.200000\n50%    28.000000      8.500000\n75%    30.000000      8.900000\nmax    35.000000      9.100000\n\n\nSelección de Columnas en un DataFrame:\nPara trabajar con una columna específica de un DataFrame, la seleccionamos usando corchetes [] y el nombre de la columna entre comillas. El resultado será una Serie de Pandas.\n\n# Seleccionar la columna 'Edad'\nedades_alumnos = df_alumnos['Edad']\nprint(\"\\nColumna 'Edad' (como Serie):\")\nprint(edades_alumnos)\n\n# Calcular la media de la columna 'Calificación'\nmedia_calificacion = df_alumnos['Calificación'].mean()\nprint(f\"\\nLa calificación promedio de los alumnos es: {media_calificacion:.2f}\")\n\n\nColumna 'Edad' (como Serie):\n0    24\n1    30\n2    28\n3    35\n4    26\nName: Edad, dtype: int64\n\nLa calificación promedio de los alumnos es: 8.10",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#el-data-frame-continuación-con-ejemplo-real",
    "href": "020-intro-python.html#el-data-frame-continuación-con-ejemplo-real",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.10 El data frame (continuación con ejemplo real)",
    "text": "5.10 El data frame (continuación con ejemplo real)\nAhora que conocemos los DataFrames, podemos entender mejor cómo leer datos reales. El marco de datos o DataFrame es el objeto más útil y más usado en el análisis de datos. Consiste en una estructura de dos dimensiones, formada por una serie de vectores de igual longitud, igual que una tabla de una hoja de cálculo, en la que cada columna es una variable y cada fila, un caso, observación o individuo. Algunas bibliotecas Python incluyen data frames de muestra, que son útiles para entender cómo están formados.\nVeamos uno de ellos, el famoso conjunto de datos iris, creada por el investigador y estadístico Ronald Fisher, que contiene un conjunto de medidas realizadas sobre flores del género Iris realizadas por este investigador en los años 30 del siglo XX.\nPara cargar este conjunto de datos de ejemplo, vamos a utilizar una función de la librería seaborn que ya importamos:\n\n# Cargamos el dataset 'iris' usando una función de Seaborn\ndf_iris_sns = sns.load_dataset('iris')\n\nprint(\"Primeras 5 filas del dataset Iris:\")\nprint(df_iris_sns.head())\n\nPrimeras 5 filas del dataset Iris:\n   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n\n\nLa función head() es útil para presentarnos sólo el encabezado del data frame. Veamos la estructura de este data frame más a fondo:\n\nprint(\"\\nInformación general del DataFrame Iris:\")\ndf_iris_sns.info()\n\n\nInformación general del DataFrame Iris:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\n\nprint(\"\\nEstadísticas descriptivas de las columnas numéricas del DataFrame Iris:\")\nprint(df_iris_sns.describe())\n\n\nEstadísticas descriptivas de las columnas numéricas del DataFrame Iris:\n       sepal_length  sepal_width  petal_length  petal_width\ncount    150.000000   150.000000    150.000000   150.000000\nmean       5.843333     3.057333      3.758000     1.199333\nstd        0.828066     0.435866      1.765298     0.762238\nmin        4.300000     2.000000      1.000000     0.100000\n25%        5.100000     2.800000      1.600000     0.300000\n50%        5.800000     3.000000      4.350000     1.300000\n75%        6.400000     3.300000      5.100000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\nPara utilizar una columna de un dataframe, la seleccionamos por su nombre:\n\nprint(\"\\nContenido de la columna 'sepal_length':\")\nprint(df_iris_sns[\"sepal_length\"]) # podemos usar también la doble comilla para designar las columnas\n\n\nContenido de la columna 'sepal_length':\n0      5.1\n1      4.9\n2      4.7\n3      4.6\n4      5.0\n      ... \n145    6.7\n146    6.3\n147    6.5\n148    6.2\n149    5.9\nName: sepal_length, Length: 150, dtype: float64\n\n\nAhora que tenemos una columna seleccionada (que es una serie de pandas), podemos aplicar directamente funciones estadísticas sobre ella:\n\n# Calculamos la media de la columna 'sepal_length'\nmedia_sepal_length = df_iris_sns[\"sepal_length\"].mean()\nprint(f\"\\nLa longitud media del sépalo es: {media_sepal_length:.2f}\") # esta es una forma de dar formato al número para imprimirlo con dos decimales fijos\n\n\nLa longitud media del sépalo es: 5.84\n\n\n\n# También podemos calcular la desviación estándar\ndesviacion_sepal_length = df_iris_sns[\"sepal_length\"].std()\nprint(f\"La desviación estándar de la longitud del sépalo es: {desviacion_sepal_length:.2f}\")\n\nLa desviación estándar de la longitud del sépalo es: 0.83\n\n\nCon nuestros DataFrames cargados y la comprensión de las Series, podemos realizar rápidamente cálculos estadísticos básicos sobre nuestras columnas numéricas. Pandas hace que esto sea muy sencillo.\n\nEstadísticas Descriptivas Comunes:\n\n.mean(): Media\n.median(): Mediana\n.std(): Desviación estándar\n.min(): Valor mínimo\n.max(): Valor máximo\n.sum(): Suma de todos los valores\n.count(): Número de valores no nulos\n\n\nVamos a aplicar algunas de estas operaciones al DataFrame df_iris_sns:\n\nprint(\"Estadísticas de 'petal_width' en el DataFrame Iris:\")\nprint(f\"Media de petal_width: {df_iris_sns['petal_width'].mean():.2f}\")\nprint(f\"Mediana de petal_width: {df_iris_sns['petal_width'].median():.2f}\")\nprint(f\"Desviación estándar de petal_width: {df_iris_sns['petal_width'].std():.2f}\")\nprint(f\"Valor mínimo de petal_width: {df_iris_sns['petal_width'].min():.2f}\")\nprint(f\"Valor máximo de petal_width: {df_iris_sns['petal_width'].max():.2f}\")\nprint(f\"Número de observaciones de petal_width: {df_iris_sns['petal_width'].count()}\")\n\nEstadísticas de 'petal_width' en el DataFrame Iris:\nMedia de petal_width: 1.20\nMediana de petal_width: 1.30\nDesviación estándar de petal_width: 0.76\nValor mínimo de petal_width: 0.10\nValor máximo de petal_width: 2.50\nNúmero de observaciones de petal_width: 150",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#visualización-de-datos-con-seaborn-y-matplotlib",
    "href": "020-intro-python.html#visualización-de-datos-con-seaborn-y-matplotlib",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.11 Visualización de Datos con Seaborn y Matplotlib",
    "text": "5.11 Visualización de Datos con Seaborn y Matplotlib\nUna imagen vale más que mil palabras, y en el análisis de datos, la visualización es fundamental para entender patrones, distribuciones y relaciones en nuestros datos. En Python, las librerías Matplotlib y Seaborn son las herramientas estándar para crear gráficos.\n\nMatplotlib.pyplot (alias plt): Es la base, una librería de bajo nivel que da un control muy granular sobre cada aspecto del gráfico.\nSeaborn (alias sns): Construye sobre Matplotlib, ofreciendo una interfaz de alto nivel para crear gráficos estadísticos complejos y estéticamente agradables con menos código. Es ideal para explorar distribuciones, relaciones entre variables y comparaciones entre grupos.\n\nSiempre importamos ambas, ya que Seaborn a menudo usa funciones de Matplotlib para mostrar y personalizar los gráficos (como plt.title() para el título o plt.show() para mostrar el gráfico).\nEl texto siguiente incluye el código para crear el gráfico, no te preocupes si no lo entiendes a la primera, iremos explicándolo línea a línea.\n\nEjemplo: Histograma de la longitud del sépalo (Distribución de una variable numérica)\nUn histograma nos permite ver cómo se distribuyen los valores de una variable numérica.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Cargar el dataset iris\niris = sns.load_dataset(\"iris\")\n\n# Crear el histograma\nsns.histplot(data=iris, x=\"sepal_length\", bins=20, kde=True, color=\"skyblue\", edgecolor=\"black\")\n\n# Añadir título y etiquetas\nplt.title(\"Distribución de Sepal Length en el dataset Iris\")\nplt.xlabel(\"Sepal Length (cm)\")\nplt.ylabel(\"Frecuencia\")\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n\n\n\nExplicación línea por línea\n1. import seaborn as sns - Importa la biblioteca Seaborn con el alias sns. - Seaborn es una capa de alto nivel sobre Matplotlib, ideal para visualizaciones estadísticas.\n2. import matplotlib.pyplot as plt - Importa Matplotlib para controlar aspectos del gráfico como título, ejes y visualización.\n3. iris = sns.load_dataset(\"iris\") - Carga el conjunto de datos iris directamente desde Seaborn. - Este dataset contiene medidas de sépalos y pétalos de tres especies de iris.\n4. sns.histplot(...) Este es el núcleo del histograma. Vamos a desglosar cada argumento:\n\n\n\n\n\n\n\nParámetro\nDescripción\n\n\n\n\ndata=iris\nIndica que usaremos el DataFrame iris como fuente de datos.\n\n\nx=\"sepal_length\"\nEspecifica la variable que queremos graficar en el eje X.\n\n\nbins=20\nDivide el rango de valores en 20 intervalos (barras). Puedes ajustar este número para más o menos detalle.\n\n\nkde=True\nAñade una curva de densidad suavizada sobre el histograma. Útil para visualizar la forma de la distribución.\n\n\ncolor=\"skyblue\"\nCambia el color de las barras del histograma. Puedes usar nombres de colores o códigos hexadecimales.\n\n\nedgecolor=\"black\"\nAñade bordes negros a cada barra para mejorar la legibilidad.\n\n\n\n5. plt.title(...) - Añade un título descriptivo al gráfico.\n6. plt.xlabel(...) y plt.ylabel(...) - Etiquetan los ejes X e Y respectivamente, lo que es esencial para la interpretación del gráfico.\n7. plt.show() - Muestra el gráfico en pantalla. Sin esta línea, el gráfico no se renderiza en algunos entornos.\nEn el capítulo siguiente veremos con detalle distintos tipos de gráficos y su construcción en Excel y en Python.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#cómo-leer-nuestros-datos-en-python-desde-google-colaborate",
    "href": "020-intro-python.html#cómo-leer-nuestros-datos-en-python-desde-google-colaborate",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.12 Cómo leer nuestros datos en Python desde Google Colaborate",
    "text": "5.12 Cómo leer nuestros datos en Python desde Google Colaborate\nHasta ahora hemos utilizado datos que hemos creado sobre la marcha, o hemos usado un conjunto de datos preexistente, como es el caso de los datos Iris.\nAhora vamos a ver cómo podemos leer o utilizar nuestros propios datos, a partir de ficheros CSV. También podemos leer directamente nuestras hojas Excel.\n\nOpción 1: Subir los datos al espacio de trabajo de Google Colaborate\nLos pasos son los mismos tanto si queremos trabajar con un CSV o directamente con una hoja Excel: 1. Dentro del entorno Colab, seleccionamos el icono de carpeta a la izquierda, que nos abre la barra lateral de Archivos 2. Hacemos click sobre el icono de la hoja con la flecha vertical, lo que nos abre una ventana de selección de archivos. 3. Seleccionamos la hoja de cálculo o CSVcon la que vamos a trabajar y la subimos al espacio de trabajo de Google Colaborate\nUna vez la hoja de cálculo o el CSV en nuestro espacio de trabajo, procedemos a leer los datos.\nEl mayor inconveniente de esta forma de trabajo es que cada vez que salimos de la sesión, Google Colab borra todos nuestros archivos del espacio de trabajo; cada vez que iniciemos una sesión, tendremos que repetir el proceso de subir nuestros datos al espacio de trabajo. Por eso, la mejor opción es tener nuestros datos en una carpeta de Google Drive, que puede utilizarse desde Colab tal como vamos a ver a continuación.\n\n\nOpción 2: Leer directamente los datos de nuestra carpeta de Google Drive\nEsta es una opción mucho más cómoda que nos evita los pasos intermedios, ya que no necesitamos subir la hoja de cálculo al espacio de trabajo.\nEmpezamos, como siempre, cargando las bibliotecas que nos proporcionan las funciones que vamos a utilizar durante el ejercicio. Aunque algunas no se usarán en la lectura de datos, en buenas prácticas es conveniente agrupar al principio del cuaderno todas las instrucciones de carga de bibliotecas en una misma celda.\n\nimport os\nimport pandas as pd             # manejo de dataframes y series\nimport matplotlib.pyplot as plt # funciones específicas de `matplotlib.pyplot`\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\") # estilo de `seaborn` con fondo blanco\n\nA continuación, la instrucciones para acceder a nuestro Google Drive y leer el archivo de datos.\n\n# 1. Ejecuta esta celda para montar tu Google Drive.\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# 2. Verifica y ajusta esta ruta de archivo si es necesario:\ngoogle_drive_path_folder = '/content/drive/MyDrive/'\nnombre_archivo_base = 'camembert.csv'\nnombre_archivo_completo = os.path.join(google_drive_path_folder, nombre_archivo_base)\n\ntry:\n    df = pd.read_csv(nombre_archivo_completo, decimal = \",\", sep=';', encoding='ISO-8859-1')\n    print(\"¡Archivo \", nombre_archivo_base,\" cargado correctamente!\")\n    print(f\"Dimensiones de los datos: {df.shape}\")\nexcept Exception as e:\n    print(f\"Error al cargar el archivo: {e}\")\n\n\n\nExplicación de los pasos:\n\n1. Montar Google Drive\n\n\n\n\n\n\n\nLínea\nExplicación\n\n\n\n\nfrom google.colab import drive\nImporta el módulo drive de Google Colab para acceder a Google Drive.\n\n\ndrive.mount('/content/drive')\nMonta tu Google Drive en la ruta /content/drive. Te pedirá autorización la primera vez.\n\n\n\n\n\n\n2. Definir ruta del archivo CSV\n\n\n\n\n\n\n\nLínea\nExplicación\n\n\n\n\ngoogle_drive_path_folder = '/content/drive/MyDrive/'\nRuta donde está el archivo dentro de Google Drive.\n\n\nnombre_archivo_base = 'camembert.csv'\nNombre del archivo CSV que se quiere cargar.\n\n\nos.path.join(...)\nUne la ruta y el nombre del archivo para formar la ruta completa. Evita errores en distintos sistemas operativos.\n\n\n\n\n\n\n3. Cargar el archivo CSV con pandas\n\n\n\n\n\n\n\nLínea\nExplicación\n\n\n\n\ntry:\nInicia un bloque que intenta ejecutar el código. Si hay error, pasa al bloque except.\n\n\npd.read_csv(...)\nCarga el archivo CSV como un DataFrame de pandas. Parámetros:\n\n\n    decimal=\",\"\nIndica que los decimales usan coma (formato europeo).\n\n\n    sep=\";\"\nIndica que los campos están separados por punto y coma.\n\n\n    encoding=\"ISO-8859-1\"\nCodificación para leer caracteres especiales como tildes.\n\n\nprint(...)\nMuestra mensaje de éxito si el archivo se carga correctamente.\n\n\ndf.shape\nDevuelve número de filas y columnas del DataFrame.\n\n\nexcept Exception as e:\nCaptura cualquier error que ocurra al cargar el archivo.\n\n\nprint(f\"Error...\")\nMuestra el mensaje de error en pantalla.\n\n\n\nEn Python siempre se prefiere encapsular la instrucción en una estructura try...except, que maneja los posibles errores o excepciones que pueden producirse en la lectura, por ejemplo, que nuestro path sea incorrecto, o que hayamos escrito mal el nombre del fichero, o cualquier otro error que haga que la instruccion de lectura falle. Añadimos también algunas instrucciones de print() que nos ayuden a saber que todo ha ido bien (o mal).\nUna vez cargados los datos, podemos revisarlos y ver la estructura del DataFrame usando las funciones adecuadas: df.dtypes, df.head(), df.info(), o df.describe()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "020-intro-python.html#resumen",
    "href": "020-intro-python.html#resumen",
    "title": "5  Introducción a Python en el entorno Google Colab",
    "section": "5.13 Resumen",
    "text": "5.13 Resumen\nEn este capítulo hemos tomado contacto con las funciones básicas de Python y su utilización a través de un interface Jupyter, en este caso Google Colaborate, y hemos visto algunas funciones gráficas básicas.\nHemos visto los principales tipos de datos, particularmente el data frame, y hemos aprendido a leer datos que previamente se habían introducido en una hoja de cálculo.\nHemos aprendido a leer nuestros datos partiendo de ficheros CSV exportados desde Excel o desde otros programas.\nEn el capítulo siguiente veremos con más detalle algunos gráficos que nos permitirán describir y entender mejor nuestros conjuntos de datos.\nA modo de ejemplo, el siguiente enlace muestra varios ejemplos de cuadernos Jupyter de todos los nivles de complejidad, desde introducción a su uso hasta aplicaciones complejas. Explorarlos ayudará a comprender mejor la interfaz.\nhttps://github.com/jupyter/jupyter/wiki/A-gallery-of-interesting-Jupyter-Notebooks",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introducción a Python en el entorno Google Colab</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html",
    "href": "030-exploracion.html",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "6.1 Explorando los datos\nUna imagen vale más que mil palabras, y en el análisis de datos, la visualización es fundamental para entender patrones, distribuciones y relaciones en nuestros datos.\nEn este capítulo estudiaremos cómo describir conjuntos de datos de forma visual, utilizando varios tipos de gráficos distintos:\nVeremos la relación visual entre un histograma y un diagrama de caja, y aprenderemos también a construir tablas de frecuencias en Excel y en Python. Finalmente, veremos algunos otros tipos de gráficos que son útiles para aplicaciones concretas, como los gráficos de densidad.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#explorando-los-datos",
    "href": "030-exploracion.html#explorando-los-datos",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "El diagrama de tallo y hojas (stem and leaf plot o stemplot)\nEl diagrama de tallo y hojas, también conocido como stemplot, es una herramienta gráfica utilizada en estadística para representar la distribución de un conjunto de datos. Es especialmente útil para conjuntos de datos pequeños y proporciona una forma rápida y efectiva de visualizar la forma de los datos y su dispersión. El stemplot recibe este nombre porque el dibujo que resulta se asemeja a un tallo el que le salen las hojas que son los datos individuales.\nLos componentes de un stemplot son:\n\nTallo: Representa el grupo principal de los valores de los datos. Generalmente, se usa la parte más significativa del número. Por ejemplo, en el número 43, el tallo podría ser 4.\nHojas: Representan los dígitos finales o menos significativos de los valores de los datos. Siguiendo el ejemplo anterior, la hoja sería 3.\n\n\nConstrucción del diagrama en la hoja de cálculo\nSupongamos que queremos medir la altura de un grupo de alumnos de nuestra clase. Éste es nuestro grupo:\n\nRealizamos la medida de altura de cada persona y registramos los valores en una hoja de cálculo, siguiendo las buenas prácticas que hemos visto al estudiar los datos ordenados.\n\nVamos a utilizar los datos de medidas de altura de nuestro grupo de alumnos. Quitamos el último dígito a la derecha de nuestros valores y colocamos verticalmente los valores resultantes ordenándolos de menor a mayor, y evitando las repeticiones. Para evitar errores en la escala, debemos incluir los valores intermedios aunque no haya ninguno en nuestros datos (en el ejemplo, el valor 16 que correspondería a los 160). Esto forma el “tallo” de nuestro diagrama:\n\nA continuación añadimos las “hojas” en la celda a la derecha, que consisten en los valores que hemos “cortado” de nuestro árbol, uno al lado de otro, incluyendo esta vez los valores repetidos, en orden de menor a mayor. Por ejemplo, para el valor 135, descartamos 13 y utlizamos 5; para el valor 138, descartamos 13 y utilizamos 8, y así sucesivamente para todos los valores.\n\n\n\n\nInterpretación del diagrama de tallo y hojas\n\nTallo: Los números a la izquierda del símbolo | representan los valores base (o tallos), en este caso, las decenas de las alturas.\nHojas: Los números a la derecha del símbolo | representan los dígitos adicionales (o hojas). Por ejemplo, en la línea 13 | 58, el tallo es 13 (130), y las hojas son 5 y 8, que corresponden a los datos 135 y 138.\n\nEl diagrama nos dice que los valores en torno a 150 cm son los más frecuentes, y que hay un valor alto (175) que se separa un poco del resto.\n\n\nResumen\nEl stemplot es muy sencillo de hacer y nos da una visión rápida y compacta de la distribución de nuestros valores, así como de la posible existencia de valores que se separan del conjunto. Estos valores alejados, que se conocen en inglés como outliers, tienen mucha importancia en el analisis e interpretación de los datos, como veremos más adelante.\nLa ventaja principal del stemplot es que mantiene los valores originales de las observaciones, y puede hacerse fácilmente con bolígrafo y papel, sin necesidad de más herramientas.\nSu principal inconveniente es la elaboración manual (aunque lenguajes como R tienen funciones que lo contruyen de forma automática), y por lo tanto, la dificultad de aplicarlo a volumenes de datos medios o grandes. El uso generalizado de los ordenadores ha hecho que actualmente esta herramienta tenga muy poco uso, y se utilicen en su lugar otras más gráficas y de construcción automática, como el histograma.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#distribuciones-de-frecuencias",
    "href": "030-exploracion.html#distribuciones-de-frecuencias",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.2 Distribuciones de frecuencias",
    "text": "6.2 Distribuciones de frecuencias\nUna distribución de frecuencias es una tabla que muestra la frecuencia con la que ocurren los valores diferentes en un conjunto de datos. Esta herramienta es fundamental en la estadística descriptiva y permite resumir y visualizar cómo se distribuyen los datos de manera clara y comprensible. A partir de una tabla de frecuencias se pueden construir diagramas de barra o histogramas para visualizar la tabla de forma gráfica.\nPara construir una distribución de frecuencias, agrupamos nuestros valores por intervalos, y contamos el número de observaciones que aparecen en cada intervalo. Los componentes de una distribución de frecuencias son:\n\nlas categorías o clases son los intervalos o valores específicos de los datos que se están analizando. Cada categoría representa un rango de valores en caso de datos continuos, o valores específicos en caso de datos discretos.\nla frecuencia absoluta es un recuento simple de cuántas veces aparece cada valor en un conjunto de datos.\nla frecuencia relativa nos muestra la proporción de cada valor frente al total. Puede expresarse como fracción (entre 0 y 1) o como porcentaje (respecto a 100), y se calcula como: \\[\n\\text{Frecuencia Relativa} = \\frac{\\text{Frecuencia Absoluta}}{\\text{Número Total de Observaciones}}\n\\]\nla frecuencia acumulada nos dice cuántas observaciones están por debajo de un cierto valor.\nla frecuencia relativa acumulada es la proporción de valores que están por debajo de un cierto valor\n\n\nConstrucción en Excel\nLa tabla a continuación muestra una distribución de frecuencias de las alturas de nuestro grupo de alumnos, calculada mediante una tabla dinámica de Excel.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#instrucciones-paso-a-paso-en-excel",
    "href": "030-exploracion.html#instrucciones-paso-a-paso-en-excel",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.3 Instrucciones paso a paso en Excel",
    "text": "6.3 Instrucciones paso a paso en Excel\nPara crear una tabla de frecuencias de la variable altura_cm mediante tablas dinámicas en Excel, sigue estos pasos:\n1. Selecciona los Datos\n\nAbre tu archivo de Excel y selecciona toda la tabla que incluye los encabezados (nombre y altura_cm).\n\n2. Inserta una tabla dinámica\n\nVe a la pestaña Insertar en la barra de herramientas de Excel.\nHaz clic en Tabla Dinámica.\nEn el cuadro de diálogo que aparece, asegúrate de que el rango de datos seleccionado es correcto y elige dónde deseas colocar la tabla dinámica (en una nueva hoja de cálculo o en la hoja actual).\n\n3. Añade la frecuencia absoluta\n\nEn el panel de campos de la tabla dinámica, arrastra el campo altura_cm a la sección Filas.\nArrastra nuevamente el campo altura_cm a la sección Valores.\n\n4. Ajusta la configuración de valores\n\nHaz clic en el campo altura_cm en la sección Valores.\nSelecciona Configuración de campo de valor.\nEn el cuadro de diálogo que aparece, asegúrate de que esté seleccionada la opción Recuento\nAcepta todo hasta volver a Excel.\n\n5. Añade la frecuencia relativa\n\nEn el panel de campos de la tabla dinámica, arrastra de nuevo el campo altura_cm a la sección Valores. Ahora la variable aparecerá como altura_cm2.\n\n6. Ajusta de nuevo la configuración de valores\n\nHaz clic en el campo altura_cm2 en la sección Valores.\nSelecciona Configuración de campo de valor.\nEn el cuadro de diálogo que aparece, asegúrate de que esté seleccionada la opción Recuento.\nEn ese mismo cuadro, haz click en el botón Formato de número, selecciona Número y 2 decimales, y acepta.\nEn ese mismo cuadro, selecciona la pestaña que dice Mostrar valores como\nEn el menú desplegable, escoge la opción % del total de columnas.\nAcepta todo hasta volver a Excel.\n\n7. Ordena y formatea\n\nPuedes ordenar las alturas en orden ascendente o descendente haciendo clic en la flecha junto a altura_cm en la tabla dinámica.\nTambién puedes cambiar el formato de la tabla dinámica para que sea más fácil de leer.\nPuedes renombrar los encabezados de la tabla para que sea más fácil de leer, rotulando las columnas, por ejemplo, como frec_absy frec_rel, o cualquier otro encabezado que te resulte claro y útil.\n\n\n\nConstrucción de una tabla de frecuencias en Python\n\nimport pandas as pd\n\naltura_cm = [153,135,140,140,175,138,145,154,152,159,154]\nfrecuencias = pd.Series(altura_cm).value_counts()\nprint(frecuencias)\n\n140    2\n154    2\n153    1\n135    1\n175    1\n138    1\n145    1\n152    1\n159    1\nName: count, dtype: int64\n\n\nCon un poco más de código podemos hacer la tabla agrupando los valores en clases de amplitud 10, con las frecuencias absolutas y relativas. Es un poco más complicado, tómate tu tiempo para entender cada paso de las instrucciones.\n\nimport pandas as pd\nimport numpy as np\n\naltura_cm = [153,135,140,140,175,138,145,154,152,159,154]\nserie = pd.Series(altura_cm)\n\n# Crear intervalos automáticamente con amplitud 10\nbins = np.arange(min(altura_cm)//10*10, max(altura_cm)//10*10 + 20, 10)\n\n# Agrupar y contar\ntabla = pd.cut(serie, bins=bins).value_counts(sort=False)\n\n# Convertir a DataFrame con acumulados y relativos\ndf = tabla.to_frame(name='Frecuencia')\ndf['Frecuencia acumulada'] = df['Frecuencia'].cumsum()\ndf['Frecuencia relativa (%)'] = (df['Frecuencia'] / df['Frecuencia'].sum() * 100).round(2)\ndf['Acumulada (%)'] = df['Frecuencia relativa (%)'].cumsum()\n\nprint(df)\n\n            Frecuencia  Frecuencia acumulada  Frecuencia relativa (%)  \\\n(130, 140]           4                     4                    36.36   \n(140, 150]           1                     5                     9.09   \n(150, 160]           5                    10                    45.45   \n(160, 170]           0                    10                     0.00   \n(170, 180]           1                    11                     9.09   \n\n            Acumulada (%)  \n(130, 140]          36.36  \n(140, 150]          45.45  \n(150, 160]          90.90  \n(160, 170]          90.90  \n(170, 180]          99.99  \n\n\nExplicación del código línea a línea.\nEste script en Python construye una tabla de frecuencias agrupadas a partir de un conjunto de datos numéricos (altura_cm), usando pandas y numpy.\nimport pandas as pd\nimport numpy as np\n🔹 Importa las bibliotecas necesarias: - pandas: para manipulación tabular. - numpy: para cálculos numéricos y generación de rangos.\naltura_cm = [153,135,140,140,175,138,145,154,152,159,154]\n🔹 Define el conjunto de datos como una lista de alturas en centímetros.\nserie = pd.Series(altura_cm)\n🔹 Convierte la lista en una Series de pandas, lo que permite aplicar funciones estadísticas fácilmente.\nbins = np.arange(min(altura_cm)//10*10, max(altura_cm)//10*10 + 20, 10)\n🔹 Genera los límites de clase con amplitud 10: - min(altura_cm)//10*10: redondea hacia abajo el mínimo a la decena más cercana. - max(altura_cm)//10*10 + 20: redondea hacia arriba el máximo y añade 10 extra para incluir el último dato. - np.arange(...): crea un array de límites desde el mínimo hasta el máximo, en pasos de 10.\n📌 Ejemplo de salida: [130, 140, 150, 160, 170, 180]\ntabla = pd.cut(serie, bins=bins).value_counts(sort=False)\n🔹 Agrupa los datos en los intervalos definidos: - pd.cut(...): asigna cada dato a un intervalo. - .value_counts(sort=False): cuenta cuántos datos hay en cada intervalo, sin reordenarlos.\ndf = tabla.to_frame(name='Frecuencia')\n🔹 Convierte la serie de frecuencias en un DataFrame con una columna llamada 'Frecuencia'.\ndf['Frecuencia acumulada'] = df['Frecuencia'].cumsum()\n🔹 Calcula la frecuencia acumulada sumando progresivamente los valores de la columna 'Frecuencia'.\ndf['Frecuencia relativa (%)'] = (df['Frecuencia'] / df['Frecuencia'].sum() * 100).round(2)\n🔹 Calcula la frecuencia relativa en porcentaje: - Divide cada frecuencia por el total. - Multiplica por 100. - Redondea a 2 decimales.\ndf['Acumulada (%)'] = df['Frecuencia relativa (%)'].cumsum()\n🔹 Calcula el porcentaje acumulado, útil para análisis tipo Pareto.\nprint(df)\n🔹 Muestra la tabla final con: - Intervalos - Frecuencia absoluta - Frecuencia acumulada - Frecuencia relativa (%) - Porcentaje acumulado\nComo ves en el resultado, a veces se utilizan los símbolos ( y [ para definir los intervalos, tal como se hace en matemáticas.\n\nIntervalo abierto: El símbolo ( se utiliza para denotar un intervalo abierto. El límite correspondiente no está incluuido en el intervalo.\nIntervalo cerrado o semiabierto:El símbolo [ se utiliza para denotar un intervalo cerrado o semiabierto. EL límite correspondiente sí está incluido en el intervalo.\n\nEjemplos:\n\n\\((a, b)\\) representa todos los números reales mayores que \\(a\\) y menores que \\(b\\) (excluye los valores \\(a\\) y \\(b\\)).\n\\([a, b]\\) representa todos los números reales mayores o iguales que \\(a\\) y menores o iguales que \\(b\\) (incluye \\(a\\) y \\(b\\)).\n\\([a, b)\\) representa todos los números reales mayores o iguales que \\(a\\) y menores que \\(b\\) (incluye \\(a\\), pero excluye \\(b\\))\n\\((a, b]\\) representa todos los números reales mayores que \\(a\\) y menores o iguales que \\(b\\) (excluye \\(a\\), pero incluye \\(b\\)). :::\n\nSi comparamos los dos métodos que hemos utilizado para construir la tabla de frecuencias, vemos que:\n\nen Excel los pasos que hemos dado no están registrados y a la vista, y, por lo tanto, no son fácilmente revisables\nen Python, todos los pasos y opciones que hemos utilizado están a la vista en el código del script\n\nSi otra persona quisiera modificar la tabla, le sería fácil editar el código y relanzar el script, mientras que en Excel no sería fácil asegurarse de todos y cada uno de los pasos y clicks de ratón que hemos dado para construir y formatear la tabla.\nEs el caso, por ejemplo, de que enviásemos la tabla a otra persona y ésta tuviese que editarla en nuestra ausencia. En Excel, tendríamos que enviar a esa persona una explicación con las instrucciones oportunas; en cambio, en Python, una vez que se comprende el código, no hacen falta más explicaciones adicionales. Incluso sin una comprensión total del código, se podría duplicar exactamente la tabla copiando y ejecutando el código.\nEsta es una de las principales razones de la conveniencia del aprendizaje de Python incluso para las actividades más sencillas.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#ejercicio-propuesto",
    "href": "030-exploracion.html#ejercicio-propuesto",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.4 Ejercicio propuesto",
    "text": "6.4 Ejercicio propuesto\nEn la tabla de frecuencias anterior, calcular frecuencia absoluta acumulada y frecuencia relativa acumulada en Excel, e incluirlas en la tabla como dos columnas adicionales. La frecuencia relativa acumulada nos permitirá crear diagramas de Pareto, muy útiles en los procesos de mejora de calidad.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#diagrama-de-barras",
    "href": "030-exploracion.html#diagrama-de-barras",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.5 Diagrama de barras",
    "text": "6.5 Diagrama de barras\nCuando nuestra variable es discreta, podemos representar las frecuencias de cada valor de forma gráfica utilizando un diagrama de barras. Este diagrama utiliza barras rectangulares para representar la frecuencia de cada categoría.\nEste gráfico es muy utilizado para representar, por ejemplo, resultados de encuestas, como el número de votos que han obtenido los diferentes partidos políticos en unas elecciones, o tablas discretas, como los kilos fabricados por meses en una fábrica.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#histograma",
    "href": "030-exploracion.html#histograma",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.6 Histograma",
    "text": "6.6 Histograma\nPara visualizar las variables continuas se utiliza el histograma, que es un diagrama que utiliza las barras rectangulares para hacer un gráfico de la distribución de valores continuos, previamente agrupados en intervalos (bins en inglés), tal como se ha hecho en la tabla de frecuencias.\n\nComponentes de un histograma\n\nEje horizontal (X): Representa los intervalos de valores de la variable. Cada intervalo abarca un rango específico de valores.\nEje vertical (Y): Muestra la frecuencia o el número de veces que los valores caen dentro de cada intervalo.\nBarras: Cada barra en el histograma representa un intervalo. La altura de la barra indica la frecuencia de los valores dentro de ese intervalo.\n\n\n\nCómo interpretar un histograma\n\nSi las barras son altas en un intervalo específico, significa que muchos valores del conjunto de datos caen dentro de ese rango.\nUn histograma puede ayudar a identificar patrones, como si los datos están distribuidos de manera simétrica, sesgada, o si hay picos y valles significativos.\n\n\n\nConstrucción de un histograma en Excel\nUsaremos el ejemplo de la altura en cm. de un grupo de alumnos, cuyos datos hemos guardado en el fichero csv aula1.csv.\nPodemos utilizar dos métodos para hacer un histograma en Excel\n\nMétodo 1: histograma directo\n\nSeleccionamos el rango de datos. Podemos hacerlo mediante un click en el encabezado de la columna de la variable altura_cm, en este caso, la columna B.\nEn la opción de menú Insertar, seleccionamos el icono Seleccionar gráficos de estadística\nSeleccionamos Histograma\n\n\n\n\n\n\nExcel calcula automáticamente la amplitud de los intervalos y el número de columnas; estas opciones pueden modificarse seleccionando con el botón derecho del ratón el elemento a modificar. En este caso, utilizamos estas opciones:\n\nEliminamos título del gráfico\nModificamos anchura de las barras seleccionando Dar formato a serie de datos, Opciones de serie, Ancho del rango= 50%\nModificamos los intervalos seleccionando el eje X: Dar formato al eje, Opciones de eje, Ancho del rango = 10\n\nLa descripción de los intervalos utiliza los mismos símbolos que hemos visto en las tablas de frecuencias de Python.\nEn el momento de escribir este manual, Excel no permite hacer histogramas múltiples ni agrupados por otra variable, por lo que para diseños más complejos, no hay más remedio que recurrir a otras opciones como las tablas dinámicas de Excel o, mejor aún, Python o R.\n\n\nMétodo 2: utilizar una tabla dinámica\nLa tabla dinámica que hemos construido en Excel ha convertido nuestra variable continua, altura_cm, en una tabla de valores discretos, al agrupar los valores en intervalos. En Excel podemos representar las frecuencias absolutas de nuestra tabla gráficamente, insertando un gráfico de barras a partir de la tabla:\n\nCon el cursor dentro de la tabla, seleccionamos la opción de menú Insertar\nInsertamos un gráfico de barras\n\nOpcionalmente, aplicamos las siguientes opciones de formato:\n\nHacemos click sobre uno de los botones del gráfico dinámico con el boton derecho del ratón, y seleccionamos la opción ocultar todos los botones del gráfico.\nEliminamos el título del gráfico\nAbrimos el formato de la serie de datos, e introducimos en la opción Ancho del rango el valor \\(50\\%\\) para ensanchar las barras.\n\n\n\n\n\n\nAl utiizar la tabla dinámica para construir el gráfico, Excel utiliza las categorías de la tabla dinámica. Dado que estas categorías (los intervalos que ha formado la tabla dinámica) son discretas, Excel utiliza el resultado de la tabla dinámica para hacer el gráfico con un diagrama de barras. No es posible insertar un histograma a partir de una tabla dinámica.\n\n\n\nAnálisis de un caso real\nEl histograma muestra su utilidad cuando representamos la distribución de un conjunto de valores más grande que nuestros once alumnos. Veamos su aplicación a los datos diarios de una fabricación de queso camembert a lo largo de un año.Los datos de esta fabricación están en el fichero camembert.csv.\nLa tabla de datos tiene esta estructura:\n\n\n\n\n\nLa tabla está formada por 211 casos.\n\nConstrucción del histograma con Excel\n\nMétodo directo\nSeleccionamos el rango del extracto seco (columna est) e insertamos el histograma, ajustando el ancho de intervalo a \\(1\\) en las opciones de gráfico:\n\n\n\n\n\n\n\nMétodo 2: mediante una tabla dinámica\nLos pasos a seguir son:\n\nConstruir la tabla dinámica\nAgrupar los datos\nInsertar el gráfico a partir de la tabla\n\nCon una agrupación de datos en intervalos de 1, esta es nuestra tabla dinámica:\n\n\n\n\n\nEn la figura siguiente vemos el histograma correspondiente a la tabla anterior, con un intervalo de clase de 1 punto de extracto seco total, y otras dos alternativas si el intervalo de clase fuese de 2 puntos o de de 0,5 puntos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa decisión de cambiar la amplitud del intervalo en un histograma influye en el aspecto del gráfico y es una opción personal; lo mejor es utilizar la que en nuestra opinión refleje mejor el aspecto de la distribución de datos, ni demasiado grande ni demasiado pequeña. En todo caso, debemos ser capaces de interpretar que la distribución de los valores es la misma en los tres casos: hay una mayoría de casos con valores entre 46 y 48, y muy pocos casos con valores muy bajos o muy altos. En este caso, la distribución de los valores es aproximadamente simétrica, y se reparten alrededor de una mayoría de valores centrales.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#visualización-de-datos-en-python-con-seaborn-y-matplotlib",
    "href": "030-exploracion.html#visualización-de-datos-en-python-con-seaborn-y-matplotlib",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.7 Visualización de Datos en Python con Seaborn y Matplotlib",
    "text": "6.7 Visualización de Datos en Python con Seaborn y Matplotlib\nEn Python, las librerías Matplotlib y Seaborn son las herramientas estándar para crear gráficos.\n\nMatplotlib.pyplot (alias plt): Es la base, una librería de bajo nivel que da un control muy granular sobre cada aspecto del gráfico.\nSeaborn (alias sns): Construye sobre Matplotlib, ofreciendo una interfaz de alto nivel para crear gráficos estadísticos complejos y estéticamente agradables con menos código. Es ideal para explorar distribuciones, relaciones entre variables y comparaciones entre grupos.\n\nSiempre importamos ambas, ya que Seaborn a menudo usa funciones de Matplotlib para mostrar y personalizar los gráficos (como plt.title() para el título o plt.show() para mostrar el gráfico).\nEl texto siguiente incluye el código para crear el gráfico, no te preocupes si no lo entiendes a la primera, iremos explicándolo línea a línea.\n\nCreación del histograma en Python\nLas funciones de histograma de Python permiten construir el histograma directamente sin necesidad de hacer previamente una tabla de frecuencias (en realidad, la tabla de frecuencias se calcula internamente). Es mucho más sencillo utilizar estas funciones, ya que el código se simplifica mucho.\nVemos que los dos gráficos no son idénticos a pesar de provenir de los mismos datos, porque la construcción de los intervalos subyacente es ligeramente diferente. Esto no debe preocuparnos, porque el aspecto general de la distribución de los datos es el mismo.\nEn la fase de exploración nos importa más entender estas propiedades de los datos (aspecto, forma de la distribución) que la precisión en la construcción del gráfico. La función básica .hist(), por su parte, es más sencilla de usar.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nurl_datos = 'https://raw.githubusercontent.com/juanriera/master-queseria/master/datos/camembert.csv'\n\ntry:\n    df = pd.read_csv(url_datos, decimal = \",\", sep=';', encoding='ISO-8859-1')\nexcept Exception as e:\n    print(f\"Error al cargar el archivo: {e}\")\n\ndf[\"est\"].hist() # histograma básico de matplotlib\n\n# Añadir título y etiquetas\nplt.title(\"Distribución del extracto seco total en una fabricación anual de camembert\")\nplt.xlabel(\"Extracto seco total (est)\")\nplt.ylabel(\"Frecuencia\")\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n\n\n\nPodemos utilizar también las funciones de seaborn para el histograma:\n\nsns.displot(df[\"est\"], kde = True)\n\n# Añadir título y etiquetas\nplt.title(\"Distribución del extracto seco total en una fabricación anual de camembert\")\nplt.xlabel(\"Extracto seco total (est)\")\nplt.ylabel(\"Frecuencia\")\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n\n\n\n\n\nOtros ejemplos\nEn ocasiones, nos encontramos con datos que son asimétricos: hay una mayoría de valores bajos o bien de valores altos. Veamos un caso: los recuentos bacterianos de bacterias coliformes, que tenemos en la última columna a la derecha de nuestra tabla, en la variable ´coliformes´.\n\nEn este caso, vemos que la mayoría de los casos tienen valor cero. Es el caso de los recuentos de bacterias contaminantes, en el que la mayoría de los análisis tienen recuentos cero o muy bajos, y sólo en pocos casos tienen valores altos. Veremos con más detalle cómo tratar estas distribuciones má adelante.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#diagrama-de-caja-o-boxplot",
    "href": "030-exploracion.html#diagrama-de-caja-o-boxplot",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.8 Diagrama de caja o boxplot",
    "text": "6.8 Diagrama de caja o boxplot\nEste gráfico fue creado por el estadístico John Tukey en 1977, y es una herramienta fundamental en la exploración de datos. Se basa en un grupo de medidas que se utiliza ampliamente en la descripción de conjuntos de datos, el conjunto de cuartiles. Si dividimos un grupo de datos ordenados en cuatro partes iguales, mediante tres puntos de corte, llamamos primer cuartil o \\(Q1\\) al valor que se situa en el 25%; segundo cuartil, o \\(Q2\\), al valor que se sitúa en el centro (50%), y tercer cuartil, o \\(Q3\\), al punto que se situa en el 75% de los datos. A estos tres valores añadimos el mínimo y el máximo, y tenemos un conjunto de cinco números que nos permiten describir la forma de la distribución de datos con cierta precisión. El segundo cuartil (\\(Q2\\)), que corresponde al 50% de los datos, se conoce habitualmente como mediana. El valor resultante de restar \\(Q3-Q1\\) es lo que se conoce como rango intercuartil o \\(IQR\\), y es una medida de la dispersión de la distribución de datos (mide la amplitud de la distribución).\nEl diagrama de caja, también conocido como boxplot, es un gráfico que permite resumir las características principales de un conjunto de datos utilizando estos cinco números, tal como se explica a continuación. Sus ventajas son:\n\nMuestra la mediana y los cuartiles (Q1 y Q3) de los datos.\nPermite identificar la simetría de la distribución: si la mediana no está en el centro, la distribución no es simétrica.\nPermite detectar posibles valores atípicos, representando los valores atípicos (outliers) que están lejos del resto de los datos (un valor es atípico si está más allá de (Q3 + 1.5 IQR) o (Q1 - 1.5 IQR).\n\nLa construcción de un diagrama de caja es como sigue:\n\nMicrosoft Excel no dispone de un diseño de gráficos de caja que sea práctico, por lo que recurriremos siempre a Python para realizarlos.\n\nRelación entre el boxplot y el histograma\nResulta muy útil comprender visualmente la relación entre el boxplot y el histograma para entender la distribución de los datos. En la gráfica siguiente analizamos el extracto seco total de nuesta fabricacion usando ambos gráficos simultáneamente. La caja del boxplotse corresponde con el 50% central de los datos, y la líneamedia nos representa la mediana, como hemos visto. Vemos también que hemos utilizado una opción de formato para destacar las observaciones que se separan de forma anormal del conjunto de datos (valores atípicos o outliers)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#gráficos-de-densidad",
    "href": "030-exploracion.html#gráficos-de-densidad",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.9 Gráficos de densidad",
    "text": "6.9 Gráficos de densidad\nUn gráfico de densidad es una representación visual suavizada de la distribución de un conjunto de datos. A diferencia de los histogramas, que dividen los datos en intervalos y cuentan las frecuencias, los gráficos de densidad utilizan técnicas estadísticas no paramétricas para estimar la función de densidad de probabilidad.\nExcel no permite la representación de los gráficos de densidad; en Pythonlo hemos hecho arriba hacerlo con seaborn, añadiendo la funcion de densidad a un histograma añadiendo la opción kde = True.\nLos gráficos de densidad son especialmente útiles cuando estamos comparando diferentes grupos de datos entre sí, por ejemplo en este caso comparamos el extracto seco total de nuestra fabricación de camembert trimestre a trimestre:\n\n\n\n\n\n\n\n\n\nLa curva de densidad nos permite visualizar la desviación que se produce por el verano (T3); son los valores que el boxplot nos ha mostrado como outliers.\n\nUna alternativa al boxplot: el violin plot\nEl violin plot es una alternativa reciente al boxplot, tiene la ventaja de que muestra la densidad de la distribución superpuesta al boxplot, lo que permite visualizar con más claridad la distribución de los datos:\n\nsns.violinplot(y='est', data=df)\n\nplt.show()\n\n\n\n\n\n\n\n\nEl violin plot, como el boxplot, se usan muy a menudo para comparar distribuciones de datos. Puedes comparar el gráfico de densidades por trimestres que hemos hecho antes con este violin plot múltiple por trimestres.\n\nsns.violinplot(x= 'trimestre', y='est', data=df, alpha=0.4)\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#gráficos-de-dispersión",
    "href": "030-exploracion.html#gráficos-de-dispersión",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.10 Gráficos de dispersión",
    "text": "6.10 Gráficos de dispersión\nUn gráfico de dispersión, también conocido como diagrama de dispersión o scatter plot, es una representación gráfica que utiliza puntos para mostrar la relación entre dos variables numéricas. Cada punto en el gráfico representa una observación del conjunto de datos y se coloca en el plano cartesiano de acuerdo con sus valores en las dos variables que se están comparando.\nUn gráfico de dispersión se compone mediante puntos:\n\nCada punto en el gráfico representa una observación.\nLa posición del punto en el gráfico está determinada por los valores de las dos variables para esa observación.\n\nLos gráficos de dispersión son útiles para identificar varios aspectos de la relación entre las dos variables:\n\nSi los puntos tienden a agruparse a lo largo de una línea recta ascendente, esto indica una correlación positiva (a medida que una variable aumenta, la otra también lo hace).\nSi los puntos se agrupan a lo largo de una línea descendente, esto indica una correlación negativa (a medida que una variable aumenta, la otra disminuye).\nSi los puntos forman una curva en lugar de una línea recta, esto sugiere una relación no lineal entre las variables.\nLa dispersión de los puntos puede indicar la variabilidad de los datos. Puntos que están muy lejos del patrón general pueden ser valores atípicos.\n\nRepresentamos las variables esty mg de nuestros datos de la fabricación de camembert.\n\n\n\n\n\n\n\n\n\n\n\n(a) Tabla de origen mostrando una parte de los datos seleccionados\n\n\n\n\n\n\n\n\nGráfico de dispersión\n\n\n\n\n\n\nFigure 6.1: Gráfico de dispersión en Excel\n\n\n\nLo esperable en nuestros datos es que el valor de est y el de mg estén asociados, y a valores altos de la primera variable correspondan valores altos de la segunda (para el mismo producto y sin cambios de tecnología). Sin embargo, algunos valores parecen no encajar en este modelo; es el caso de un valor de estpor encima de 52% con un valor de mginferior a 23%, o el caso de un valor de mg en torno al 28% con un valor de est por debajo de 47%. Debemos revisar estos valores aparentemente anormales para verificar si ha habido un error en la toma de muestras o un error analítico. En fábricas con productos diferentes cuyos resultados analíticos se recogen en una tabla de datos común, a veces puede haber errores en la introducción de los códigos de producto, de forma que la toma de muestras y los análisis pueden ser correctos, pero estar mal asignados al guardar los datos.\nPython nos permite separar el diagrama de dispersión en función de una tercera variable, en este caso, usremos el trimestre para ver lo que ha sucedido por el verano.\n\n# Crear un diagrama de dispersión de sepal_length vs. sepal_width\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data=df, x='est', y='mg', hue='trimestre', s=80, alpha=0.7)\nplt.title('Relación entre el EST y la MG en una fabricación anual de camembert')\nplt.xlabel('EST (%)')\nplt.ylabel('MG (%)')\nplt.legend(title='Trimestre') # Mostrar leyenda para la especie\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\nComo vemos, los gráficos de dispersión son una herramienta esencial en el análisis exploratorio de datos, ya que permiten visualizar relaciones y patrones en los datos, identificar correlaciones y detectar posibles anomalías. Esta información es crucial para realizar análisis estadísticos más profundos y tomar decisiones basadas en datos.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#gráficos-de-series-temporales",
    "href": "030-exploracion.html#gráficos-de-series-temporales",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.11 Gráficos de series temporales",
    "text": "6.11 Gráficos de series temporales\nHasta ahora hemos utilizado gráficos y tablas que describen la estructura y forma de una variable, o las relaciones entre dos variables. Hay otros gráficos que tienen en cuenta la forma en la que esos datos cambian con el tiempo. En este caso, será necesario que hayamos recogido en una variable de nuestra tabla los intervalos de tiempo en los que se han producido nuestros valores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "030-exploracion.html#algunos-ejemplos",
    "href": "030-exploracion.html#algunos-ejemplos",
    "title": "6  La exploración de los datos mediante gráficos.",
    "section": "6.12 Algunos ejemplos",
    "text": "6.12 Algunos ejemplos\n\nproceso de llenado de envases de queso crema: se llena una tarrina cada 15 segundos. Nuestros datos deben recoger el tiempo y el peso.\nnuestro fichero de fabricación de queso camembert recoge los valores analíticos medios diarios del producto fabricado.\nUna fábrica recoge leche diariamente y analiza cada día la composición de la leche que entra en la fábrica.\n\nEn un gráfico de series temporales,\n\nel eje horizontal (X) representa el tiempo. Los puntos de tiempo pueden ser minutos, horas, días, meses, años, etc.\nel eje vertical (Y) representa los valores de la variable que se está estudiando. Estos valores pueden ser medidas como temperatura, ventas, precios, etc.\ncada valor individual corresponde a un punto\nlos valores se conectan mediante una línea que conecta los puntos de datos, mostrando cómo cambian los valores de la variable a lo largo del tiempo.\nnormalmente, en un gráfico de series temporales no suelen representarse los puntos individuales para facilitar la legibilidad del gráfico.\n\nEn nuestro conjunto de datos de fabricación de queso camembert, la primera columna de la tabla recoge la variable fecha, lo que nos permite ordenar nuestros valores en el tiempo.\nCuando representamos valores en el tiempo, nunca usaremos el diagrama de barras, sino el gráfico de líneas.\n\nCómo hacer los gráficos de series temporales en Excel\nPara hacer el gráfico en Excel, seleccionamos la columna este insertamos un gráfico de líneas. A continuación, con el cursor sobre el gráfico, pulsamos el boton derecho y seleccionamos la opción Seleccionar datos. Una vez abierto el cuadro de opciones, editamos las etiquetas del eje X y seleccionamos el rango de la variable fecha desde la fila 2 hasta la última.Aceptamos, y a continuación editamos el formato del eje Y para sustituir el valor mínimo de \\(0\\) por \\(42\\), que es el valor que queremos como mínimo para nuestro gráfico.\n\n\n\nTabla y gráfico de series\n\n\n\n\nGráficos de series temporales en Python utilizando pandas\n\n\nResumen\nLos gráficos de series temporales son útiles para:\n\nIdentificar Tendencias:\n\nUna tendencia es una dirección general en la que los datos se mueven a lo largo del tiempo. Puede ser creciente, decreciente o constante.\n\nDetección de Estacionalidad:\n\nLa estacionalidad se refiere a patrones que se repiten en intervalos regulares de tiempo, como las ventas de productos estacionales.\n\nIdentificar Ciclos:\n\nLos ciclos son fluctuaciones que ocurren en intervalos no regulares y pueden deberse a factores económicos o de otra índole.\n\nDetección de Anomalías:\n\nLos picos y caídas repentinas pueden indicar eventos inusuales o errores en los datos.\n\n\nLos gráficos de series temporales son cruciales en diversas áreas:\n\nEconomía y Finanzas:\n\nSeguimiento de precios de acciones, tasas de interés y otros indicadores económicos.\n\nCiencia y Tecnología:\n\nMonitoreo de variables ambientales, datos meteorológicos y medidas científicas.\n\nNegocios:\n\nAnálisis de ventas, demanda de productos y desempeño empresarial a lo largo del tiempo.\n\n\nLos gráficos de series temporales proporcionan una visión clara y concisa de cómo cambian los datos a lo largo del tiempo. Esta visualización es fundamental para el análisis predictivo, la toma de decisiones y la identificación de patrones y anomalías en los datos.\nhttps://tidyplots.org/\n \n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# 1. Cargar y Preprocesar los Datos\n# Suponiendo que el archivo se llama 'datos_fabricacion.csv'\n\n# Parámetros clave:\n# sep=';'          -&gt; El delimitador es el punto y coma.\n# decimal=','      -&gt; La coma es el separador decimal (formato europeo).\n# parse_dates=['fecha'] -&gt; Indica a Pandas que convierta la columna 'fecha'.\n# dayfirst=True    -&gt; Especifica el formato dd/mm/aaaa para la fecha.\n\ntry:\n    df = pd.read_csv(\n        'datos_fabricacion.csv', \n        sep=';', \n        decimal=',',\n        parse_dates=['fecha'], \n        dayfirst=True \n    )\nexcept FileNotFoundError:\n    print(\"Error: Asegúrate de que tu archivo CSV esté en el directorio correcto o usa la ruta completa.\")\n    # Crear un DataFrame de ejemplo si el archivo no se encuentra para que el código sea reproducible\n    data = {\n        'fecha': pd.to_datetime(['11/01/2020', '15/04/2020', '20/07/2020', '25/10/2020', '12/01/2020', '15/05/2020', '20/08/2020', '25/11/2020'], format='%d/%m/%Y'),\n        'fabricacion': [1, 1, 1, 1, 1, 1, 1, 1],\n        'est': [46.68, 48.09, 45.97, 46.24, 47.94, 45.69, 46.60, 47.10],\n        'mg': [24, 25, 24, 23.5, 25.5, 26, 28, 24],\n        'coliformes': [80, 0, 9200, 20, 0, 0, 0, 175]\n    }\n    df = pd.DataFrame(data)\n    print(\"Usando datos de ejemplo para continuar la ejecución.\")\n\nError: Asegúrate de que tu archivo CSV esté en el directorio correcto o usa la ruta completa.\nUsando datos de ejemplo para continuar la ejecución.\n\n\n\n# 2. Asignar Trimestre\n# Extraer el trimestre de la columna 'fecha'\ndf['trimestre'] = df['fecha'].dt.quarter.astype(str)\ndf['trimestre_etiqueta'] = 'T' + df['trimestre']\n\n# 3. Visualización con Seaborn (Curva de Densidad por Trimestre)\n\nplt.figure(figsize=(10, 6))\n\n# Usamos sns.kdeplot, que es la función ideal para comparar densidades.\nsns.kdeplot(\n    data=df, \n    x='est', \n    hue='trimestre_etiqueta', # Columna para diferenciar las curvas\n    fill=True,               # Rellenar el área bajo la curva\n    alpha=0.2,               # Transparencia del relleno\n    linewidth=2              # Grosor de la línea\n)\n\n# Configuración del Título y Etiquetas\nplt.title('Comparación de la Curva de Densidad del Extracto Seco Total (EST) por Trimestre', fontsize=16)\nplt.xlabel('Extracto Seco Total (EST)', fontsize=12)\nplt.ylabel('Densidad', fontsize=12)\n\n# Mostrar leyenda y gráfico\nplt.gca().get_legend().set_title(\"Trimestre\")\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-analisis-queso.html",
    "href": "040-analisis-queso.html",
    "title": "7  Caso 1. Análisis de una fabricación de queso camembert usando Google Colab.",
    "section": "",
    "text": "7.1 Introducción a los gráficos básicos\nEn este ejercicio usaremos un fichero o archivo de datos en formato CSV que habremos exportado previamente desde nuestra hoja de cálculo. Más adelante aprenderemos cómo leer directamente los datos de nuestras hojas Excel.\nAbrir cuaderno en Google Colab/\nDescargar los datos de ejemplo camembert.csv usados en este cuaderno\nEn este cuaderno vamos a cargar los datos desde GitHub; estos datos pueden ser descargados en el ordenador o Google Drive de cada uno, junto con el cuaderno, en los enlaces facilitados al principio. Una vez en Drive, puede leerse la tabla mediante el método que vimos al final del capítulo anterior.\nPodemos mostrar el dataframe que hemos leído, mediante la funcion .head(), que nos muestra las cinco primeras lineas.\nTambién podemos usar la función .info(), que nos dice la estructura interna de nuestro dataframe y el tipo de los datos (entero, numérico, carácter…). Dado que la fecha, como hemos visto, está formateada como fecha y asignada como ìndex, ya no aparece en el listado de columnas de datos, sino que aparece en la primera línea como DateTimeIndex, y la informacion nos dice los límites de esas fechas.\nUna vez leído correctamente el DataFrame, podemos hacer algunos gráficos de sus columnas numéricas. También usaremos las funciones de seaborn que producen salidas muy atractivas y son funcionjes fáciles de manejar.\nExploremos los resultados de análisis de extracto seco total, recogidos en la variable est de nuestro DataFrame.\nPara ello empezamos haciendo el histograma con seaborn, que nos permite incluir una curva de densidad fácilmente (pregunta: ¿qué es una curva de densidad?)\nsns.displot(df[\"est\"], kde = True)\n\nplt.show()\nLos histogramas nos muestran con claridad la distribución de los datos: los valores más frecuentes están entre 45% y 47%, y hay algún valor un poco más alto, por encima de 52%, que no parece que haya ocurrido muchas veces.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Caso 1. Análisis de una fabricación de queso camembert usando Google Colab.</span>"
    ]
  },
  {
    "objectID": "040-analisis-queso.html#introducción-a-los-gráficos-básicos",
    "href": "040-analisis-queso.html#introducción-a-los-gráficos-básicos",
    "title": "7  Caso 1. Análisis de una fabricación de queso camembert usando Google Colab.",
    "section": "",
    "text": "El boxplot\n\nsns.boxplot(df['est'])\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLos gráficos de series temporales\nEn la celda en la que hemos obtenido la información del DataFrame con df.info(), veíamos que la columna fecha se lee como un object. En Python, un object es, por ejemplo, una cadena de caracteres. Python no tiene forma de saber que esto es una fecha, a menos que convirtamos explcícitamente el tipo de dato en fecha. Para ello, Python dispone de funciones muy sofisticadas que es muy conveniente conocer. Vamos, en primer lugar a dar el formato fecha a la columna, mediantela función ’Datetime\n\n\n# Reemplazar la columna 'fecha' in-place\ndf['fecha'] = pd.to_datetime(\n    df['fecha'], \n    format='%d/%m/%Y',\n    errors='coerce' # Mantiene la robustez ante datos incorrectos\n)\n\n# Opcional: Verificar el cambio\nprint(df['fecha'].head())\nprint(df['fecha'].dtype)\n\n0   2020-01-11\n1   2020-01-12\n2   2020-01-13\n3   2020-01-14\n4   2020-01-15\nName: fecha, dtype: datetime64[ns]\ndatetime64[ns]\n\n\nPara representar los datos por mes, sólo tenemos que crear una nueva columna mes indicando a pandas que extraiga del índice la parte de fecha que corresponde al mes. ¿Fácil, no?\n\ndf['fecha_index']= pd.DatetimeIndex(df.fecha).normalize()\ndf.set_index('fecha_index',inplace=True)\ndf.sort_index(inplace=True)\n\n\ndf['mes'] = df.index.month\nsns.boxplot(x='mes', y='est', data=df)\n\nplt.show()\n\n\n\n\n\n\n\n\nEn vez de usar el número para el mes, podemos usar el código de letras abreviado (en este caso, seabornutiliza la abreviatura en inglés, pero hemos cambiado la codificación para que lo represente en español, en la instruccion locale que hemos usado en la primera casilla). Aprovechamos para personalizar un poco el gráfico.\n\ndf['mes_abreviado'] = df.index.strftime('%b')\nsns.boxplot(x='mes_abreviado', y='est', data=df)\n# Opcional: Personalizar el gráfico\nplt.title('Boxplot de la variable \"est\" por Mes')\nplt.xlabel('Mes')\nplt.ylabel('Valor de \"est\"')\nplt.grid(axis='y', linestyle='--', alpha=0.7) # Añadir una cuadrícula para mejor lectura\nplt.xticks(rotation=45) # Rotar las etiquetas del eje X si son muchas\nplt.tight_layout() # Ajusta automáticamente los parámetros de la subtrama para un diseño ajustado\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIntroducción a los gráficos de series temporales\nPara utilizar las funciones de series de pandas resulta conveniente convertir en una columna de fecha la primera columna, que pandas ha leido como texto, y asignarla como index del dataframe; el formateo de series temporales en esta biblioteca es uno de sus puntos más fuertes.\n\ndf['fecha_index']= pd.DatetimeIndex(df.fecha).normalize()\ndf.set_index('fecha_index',inplace=True)\ndf.sort_index(inplace=True)\n\nLa función .plot() nos representa los valores de la columna en orden secuencial:\n\ndf[\"est\"].plot()\nplt.show()\n\n\n\n\n\n\n\n\nSi utilizamos las funciones de pandas, podemos reformatear las fechas como serie temporal. Creamos una serie temporal y la remuestreamos para hacer las medias semanales del extracto seco total, eliminando las semans en las que no hay valores con .dropna(). La función resample('W-MON') formatea las fechas para que las semanas empiecen en lunes, como es el caso en Europa (en USA la semana empieza el domingo). Aprovechamos para mostrar una de las potencias de Python: podemos hacer que varios cálculos se hagan a continuacion de los otros, simplemente enlazando las funciones, hasta el .plot()\n\nplt.rcParams['figure.figsize'] = (10.0,4.0)\nts = pd.Series(df[\"est\"].dropna())\nts.resample('W-MON').mean().plot(title=\"Media del extracto seco total semanal\")\nplt.show()\n\n\n\n\n\n\n\n\nVeamos a continuación otras formas de formatear la serie sobre la marcha, pero esta vez representando la desviación típicaen vez de la media.\n\nts.resample('W-MON').std().plot(title=\"Desviación típica del extracto seco total semanal\");\n\n\n\n\n\n\n\n\nO podemos representar un período específico de la serie indicando a pandas los límites inferior y superior de las fechas que queremos.\n\nts[\"2020-05\":\"2020-08\"].resample('W-MON').mean().plot(title=\"Media del extracto seco total semanal\");\n\n\n\n\n\n\n\n\nA modo ilustrativo, aunque sin un interés prioritario, muestro un grafico jointplot() de seaborn que muestra la facilidad con la que esta biblioteca puede hacer un gráfico complejo de dispersión e histograma simultáneamente.\n\nsns.jointplot(x=\"est\", y=\"mg\", data = df[~df.index.duplicated(keep='first')])\n\n\n\n\n\n\n\n\nFinalmente, una serie de cálculos más complejos para obtener los gráficos de capacidad de un proceso, como muestra de cómo se pueden usar las funciones y gráficos de Python para prácticamente cualquier necesidad.\nA continuación, una serie de celdas que realizan gráficos diversos, puedes dedicar un rato a estudiarlas e intentar comprender bien su programación.\n\nlimite_rechazo = 231    ##\nlimite_deficientes = 242    ##\n\nLSL = df.est.mean() - 3 * df.est.std()    ## lower specification limit\nUSL = df.est.mean() + 3 * df.est.std()    ## upper specification limit\n\ndf.insert(6,'LSL', LSL)\ndf.insert(7,'USL', USL)\n\n\ndf['fecha'] = df.index\n\n\ndf.head()\n\n\n\n\n\n\n\n\nfecha\nfabricacion\nest\nmg\nph\ncloruros\nLSL\nUSL\ncoliformes\nmes\nmes_abreviado\n\n\nfecha_index\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-11\n2020-01-11\n1\n46.68\n24.0\n4.85\n1.50\n42.279767\n50.994641\n80.0\n1\nJan\n\n\n2020-01-12\n2020-01-12\n1\n48.09\n25.0\n4.67\n1.61\n42.279767\n50.994641\n0.0\n1\nJan\n\n\n2020-01-13\n2020-01-13\n1\n45.97\n24.0\n4.71\n1.48\n42.279767\n50.994641\n9200.0\n1\nJan\n\n\n2020-01-14\n2020-01-14\n1\n46.24\n23.5\n4.78\n1.69\n42.279767\n50.994641\n20.0\n1\nJan\n\n\n2020-01-15\n2020-01-15\n1\n45.81\n23.0\n4.77\n1.56\n42.279767\n50.994641\n150.0\n1\nJan\n\n\n\n\n\n\n\n\ndf2 = pd.melt(df, id_vars= ['fecha'], value_vars=[\"est\",\"LSL\", \"USL\"],  value_name=\"valores\")\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nfecha\nvariable\nvalores\n\n\n\n\n0\n2020-01-11\nest\n46.68\n\n\n1\n2020-01-12\nest\n48.09\n\n\n2\n2020-01-13\nest\n45.97\n\n\n3\n2020-01-14\nest\n46.24\n\n\n4\n2020-01-15\nest\n45.81\n\n\n\n\n\n\n\n\nplt.rcParams['figure.figsize'] = (10.0,4.0)\ng = sns.lineplot(data=df2, x=\"fecha\", y=\"valores\", hue=\"variable\")\n\n\n\n\n\n\n\n\n\ndf3 = df2.groupby([df2['fecha'].dt.isocalendar().week, \"variable\"]).mean()\n\n\ndf3.reset_index(inplace=True)\n\n\ndf3.head()\n\n\n\n\n\n\n\n\nweek\nvariable\nfecha\nvalores\n\n\n\n\n0\n2\nLSL\n2020-01-11 12:00:00\n42.279767\n\n\n1\n2\nUSL\n2020-01-11 12:00:00\n50.994641\n\n\n2\n2\nest\n2020-01-11 12:00:00\n47.385000\n\n\n3\n3\nLSL\n2020-01-15 19:12:00\n42.279767\n\n\n4\n3\nUSL\n2020-01-15 19:12:00\n50.994641\n\n\n\n\n\n\n\n\nplt.rcParams['figure.figsize'] = (10.0,4.0)\nsns.set_style(\"whitegrid\")\ng = sns.lineplot(data=df3, x=\"week\", y=\"valores\", hue=\"variable\")\n\n\n\n\n\n\n\n\n\ndf2 = df.copy()\ndf.drop(['fabricacion','mg', 'cloruros','coliformes'], axis=1, inplace = True)\n\n\ndf2 = df2['est'].groupby(df['fecha'].dt.isocalendar().week).agg(['mean','std'])\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nweek\n\n\n\n\n\n\n2\n47.385000\n0.997021\n\n\n3\n45.770000\n0.547494\n\n\n4\n46.540000\n0.893476\n\n\n5\n46.037778\n0.493958\n\n\n6\n45.090000\n1.252837\n\n\n\n\n\n\n\n\ndf2['mean'].plot();\n\n\n\n\n\n\n\n\n\ndf2['std'].plot();\n\n\n\n\n\n\n\n\n\nLSL = df2['mean'] - 3 * df2['std']    ## lower specification limit\nUSL = df2['mean'] + 3 * df2['std']    ## upper specification limit\n\ndf2.insert(2,'LSL', LSL)\ndf2.insert(3,'USL', USL)\n\n# limite_rechazo = 231    ##\n# limite_deficientes = 242    ##\n# df3.insert(5,'rechazo', limite_rechazo)\n# df3.insert(6,'deficientes', limite_deficientes)\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nmean\nstd\nLSL\nUSL\n\n\nweek\n\n\n\n\n\n\n\n\n2\n47.385000\n0.997021\n44.393938\n50.376062\n\n\n3\n45.770000\n0.547494\n44.127517\n47.412483\n\n\n4\n46.540000\n0.893476\n43.859571\n49.220429\n\n\n5\n46.037778\n0.493958\n44.555904\n47.519652\n\n\n6\n45.090000\n1.252837\n41.331490\n48.848510\n\n\n\n\n\n\n\n\ndf2['semana'] = df2.index # necesitamos la semana en una columna de valor\ndf3 = pd.melt(df2, id_vars= ['semana'], value_vars=[\"mean\",\"LSL\", \"USL\"],  value_name=\"valores\")\n\n\ng = sns.lineplot(data=df3, x=\"semana\", y=\"valores\", hue=\"variable\")",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Caso 1. Análisis de una fabricación de queso camembert usando Google Colab.</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html",
    "href": "999-indice_dataframes.html",
    "title": "8  Indice de DataFrames utilizados",
    "section": "",
    "text": "8.1 Datos de fabricación de diferentes recetas de queso ibérico para balance de materia\nfecha\nreceta\noperador\nlitros_past\nkg_cuba\nmg_cuba\nmp_cuba\nlactosa_cuba\nph_cuajado\nph_inicio_moldeo_cuba\n...\nformato\nph_inicio_moldeo_queso\nph_inicio_desmoldeo_queso\nph_fin_desmoldeo_queso\ntiempo_salado\nmg_salida_salmuera\nph_salida_salmuera\nEST salida salmuera\npeso_queso_total\nsal_queso\n\n\n\n\n0\n22/02/2016\nreceta 1\n7.0\n9020\n9404.95\n5.18\n4.06\n4.15\n6.56\n6.47\n...\n1 kg\n6.33\n5.66\n5.30\n7:25\n31.03\n5.66\n56.85\n8760.12\n1.32\n\n\n1\n22/02/2016\nreceta 2\n7.0\n9830\n10241.07\n6.19\n4.88\n3.85\n6.60\n6.50\n...\n3 kg\n6.45\n5.86\n5.48\n16:22\n29.80\n5.42\n56.36\n6052.30\n1.52\n\n\n2\n22/02/2016\nreceta 3\n7.0\n14330\n15059.58\n4.84\n3.78\n4.41\n6.59\n6.38\n...\n3 kg\n6.30\n5.26\n5.11\n16:20\n30.95\n5.45\n57.84\n12532.00\n1.64\n\n\n3\n22/02/2016\nreceta 4\n7.0\n6780\n7506.26\n6.18\n4.89\n3.86\n6.60\n6.50\n...\nmini\n6.39\n5.82\n5.66\n8:25\nNaN\nNaN\nNaN\n4437.50\nNaN\n\n\n4\n22/02/2016\nreceta 5\n8.0\n9520\n10067.49\n4.60\n3.62\n4.28\n6.57\n6.41\n...\nmini\n6.20\n5.64\n5.16\n7:15\n31.54\n5.40\n57.47\n14215.50\n1.47\n\n\n\n\n5 rows × 21 columns",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#análisis-de-producto-terminado-de-una-fabricación-diaria-de-camembert",
    "href": "999-indice_dataframes.html#análisis-de-producto-terminado-de-una-fabricación-diaria-de-camembert",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.2 Análisis de producto terminado de una fabricación diaria de camembert",
    "text": "8.2 Análisis de producto terminado de una fabricación diaria de camembert\nEn formato CSV y Excel\n\n\n\n\n\n\n\n\n\nfecha\nfabricacion\nest\nmg\nph\ncloruros\ncoliformes\n\n\n\n\n0\n11/01/2020\n1\n46.68\n24.0\n4.85\n1.50\n80.0\n\n\n1\n12/01/2020\n1\n48.09\n25.0\n4.67\n1.61\n0.0\n\n\n2\n13/01/2020\n1\n45.97\n24.0\n4.71\n1.48\n9200.0\n\n\n3\n14/01/2020\n1\n46.24\n23.5\n4.78\n1.69\n20.0\n\n\n4\n15/01/2020\n1\n45.81\n23.0\n4.77\n1.56\n150.0",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#lecturas-de-humedad-relativa-y-temperatura-en-una-cueva-para-maduración-de-queso",
    "href": "999-indice_dataframes.html#lecturas-de-humedad-relativa-y-temperatura-en-una-cueva-para-maduración-de-queso",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.3 Lecturas de humedad relativa y temperatura en una cueva para maduración de queso",
    "text": "8.3 Lecturas de humedad relativa y temperatura en una cueva para maduración de queso\n\n\n\n\n\n\n\n\n\nfecha\ntemp\nhrel\n\n\n\n\n0\n2020-01-01 00:00:00\n10.232205\n87.796974\n\n\n1\n2020-01-01 00:05:00\n10.091809\n87.807218\n\n\n2\n2020-01-01 00:10:00\n10.139651\n87.801365\n\n\n3\n2020-01-01 00:15:00\n10.030622\n87.796916\n\n\n4\n2020-01-01 00:20:00\n9.901193\n87.811986",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#datos-de-análisis-de-quesos-tipo-ibérico-a-la-salida-de-la-salmuera",
    "href": "999-indice_dataframes.html#datos-de-análisis-de-quesos-tipo-ibérico-a-la-salida-de-la-salmuera",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.4 Datos de análisis de quesos tipo ibérico a la salida de la salmuera",
    "text": "8.4 Datos de análisis de quesos tipo ibérico a la salida de la salmuera\n\n\n\n\n\n\n\n\n\nfecha\nreceta\nformato\nmaduracion\nextractoseco\nmateriagrasa\nsal\nph\n\n\n\n\n0\n07/10/2012\nmezcla 5\n3 kg\ntierno\n57.330002\n32.139999\n1.04\n5.27\n\n\n1\n07/10/2012\nmezcla 5\n500 g\ntierno\n58.549999\n32.980000\n0.99\n5.45\n\n\n2\n07/10/2012\nmezcla 5\n3 kg\ntierno\n58.509998\n33.160000\n1.16\n5.26\n\n\n3\n07/10/2012\nmezcla 5\n500 g\ntierno\n59.009998\n33.070000\n1.27\n5.46\n\n\n4\n07/10/2012\nmezcla 5\n500 g\ntierno\n57.869999\n31.940001\n0.93\n5.47",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#datos-a-salida-de-saladero-de-quesos-ibéricos",
    "href": "999-indice_dataframes.html#datos-a-salida-de-saladero-de-quesos-ibéricos",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.5 Datos a salida de saladero de quesos ibéricos",
    "text": "8.5 Datos a salida de saladero de quesos ibéricos\n\n\n\n\n\n\n\n\n\nFecha\nNum_Cuba\nLote\nFORMATO\nPH\nES\nMG\nSAL\n\n\n\n\n0\n05/11/2015\n7\n42015\nBarra 3 kg\n5.51\n58.31\n32.36\n1.45\n\n\n1\n13/03/2015\n28\n18315\nBarra 4,2 kg\n5.49\n58.59\n30.90\n1.18\n\n\n2\n04/05/2016\n8\n23616\nBarra 3 kg\n5.43\n57.82\n31.60\n1.57\n\n\n3\n14/03/2016\n9\n18516\nBarra 3 kg\n5.59\n58.17\n31.24\n1.37\n\n\n4\n23/05/2016\n6\n25516\n1 kg\n5.46\n59.71\n31.92\n0.92",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#datos-de-un-proceso-de-envasado-de-queso-en-unidades-de-250-g",
    "href": "999-indice_dataframes.html#datos-de-un-proceso-de-envasado-de-queso-en-unidades-de-250-g",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.6 Datos de un proceso de envasado de queso en unidades de 250 g",
    "text": "8.6 Datos de un proceso de envasado de queso en unidades de 250 g\n\n\n\n\n\n\n\n\n\nfecha\npeso\n\n\n\n\n0\n2020-05-16 09:00:00\n254.55\n\n\n1\n2020-05-16 09:00:10\n246.92\n\n\n2\n2020-05-16 09:00:20\n236.29\n\n\n3\n2020-05-16 09:00:30\n241.60\n\n\n4\n2020-05-16 09:00:40\n247.41",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#datos-de-entrada-de-leche-en-una-fábrica-de-queso-ibérico-de-mezcla.",
    "href": "999-indice_dataframes.html#datos-de-entrada-de-leche-en-una-fábrica-de-queso-ibérico-de-mezcla.",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.7 Datos de entrada de leche en una fábrica de queso ibérico de mezcla.",
    "text": "8.7 Datos de entrada de leche en una fábrica de queso ibérico de mezcla.\n\n\n\n\n\n\n\n\n\ncodruta\npeso\ntipoleche\nfecha\nacidez\ncaseina\ndensidad\nest\nlactosa\nmg\nph\nprot\ntemp\n\n\n\n\n0\n13\n13380\nVACA\n04/11/2015\n15.0\n2.31\n1.030\n12.45\n4.71\n3.71\n6.84\n3.34\n6.0\n\n\n1\n13\n8520\nVACA\n04/11/2015\n15.0\n2.41\n1.030\n12.83\n4.74\n3.94\n6.81\n3.40\n6.0\n\n\n2\n22\n23900\nVACA\n04/11/2015\n15.0\n2.29\n1.030\n12.47\n4.85\n3.66\n6.82\n3.22\n6.0\n\n\n3\n41\n4180\nCABRA\n04/11/2015\n14.0\n2.88\n1.032\n14.88\n4.70\n5.50\n6.86\n3.95\n6.1\n\n\n4\n24\n14640\nVACA\n04/11/2015\n15.0\n2.26\n1.030\n12.35\n4.69\n3.68\n6.86\n3.25\n7.7",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "999-indice_dataframes.html#datos-de-análisis-de-producto-terminado-de-diferentes-productos-en-una-tabla-excel",
    "href": "999-indice_dataframes.html#datos-de-análisis-de-producto-terminado-de-diferentes-productos-en-una-tabla-excel",
    "title": "8  Indice de DataFrames utilizados",
    "section": "8.8 Datos de análisis de producto terminado de diferentes productos, en una tabla Excel",
    "text": "8.8 Datos de análisis de producto terminado de diferentes productos, en una tabla Excel\n\n# Read the Excel file and specify the sheet name\ndf = pd.read_excel('datos/analisis.xlsx', sheet_name='analisis')\ndf.head()\n\n\n\n\n\n\n\n\nFECHA\nCLAVE\nCODIGO\nHUMEDAD\nEXSECO\nESMAGRO\nHQMAGRO\nGRASA\nGRASASECO\nPH\nCLORUROS\nCOLIS\nfecha_2\n\n\n\n\n0\n7\n660\n10078\n48.48\n51.52\n18.52\n72.36\n33.0\n64.05\n5.17\n1.53\n160\n1988-01-07\n\n\n1\n7\n660\n20078\n47.35\n52.65\n17.65\n72.85\n35.0\n66.48\n5.21\n1.25\n10\n1988-01-07\n\n\n2\n7\n660\n30078\n47.02\n52.98\n19.98\n70.18\n33.0\n62.29\n5.28\n1.34\n20\n1988-01-07\n\n\n3\n8\n660\n10088\n47.02\n52.98\n18.98\n71.24\n34.0\n64.18\n5.16\n1.34\n18620\n1988-01-08\n\n\n4\n8\n660\n20088\n47.22\n52.78\n18.78\n71.55\n34.0\n64.42\n5.25\n1.34\n2200\n1988-01-08",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Indice de DataFrames utilizados</span>"
    ]
  },
  {
    "objectID": "160-bibliografia.html",
    "href": "160-bibliografia.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Hadley Wickham, Garret Grolemund. 2023. “R Para Ciencia de\nDatos.” 2023. https://es.r4ds.hadley.nz/.\n\n\nHadley Wickham, Garret Grolemund, Mine Çetinkaya-Rundel. 2023. R for\nData Science, 2nd Ed. 1005 Gravenstein Highway North, Sebastopol,\nCA95472: O’Reilly Media Inc. https://r4ds.hadley.nz/.\n\n\nWorld Economic Forum. 2025. “Future of Jobs Report, 2025.”\n91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World\nEconomic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2025/.",
    "crumbs": [
      "Anexos",
      "Bibliografía"
    ]
  }
]