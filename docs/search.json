[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notas para el master de quesería",
    "section": "",
    "text": "1 Introducción general\nEste documento trata sobre el análisis de datos industriales utilizando tanto la hoja de cálculo como un lenguaje de programación moderno como es Python . El uso de las técnicas estadísticas se ha reducido al mínimo, orientando el análisis al uso de métodos gráficos descriptivos sencillos.\n¿Por qué usar Python como herramienta? Hay varias razones que parecen oportunas:\n\nPython es actualmente el lenguaje de programación más extendido y el más utilizado en los nuevos campos de la inteligencia artificial (IA). Su conocimiento, aunque sea de forma básica, aportará un plus a los alumnos en su curriculum.\nEl Principado de Asturias ha establecido una línea prioritaria de desarrollo de conocimientos en las áreas relacionadas con la IA para los estudiantes de Formación Profesional. Esto, unido al hecho de que algunas empresas de la región ya están implantando algunas experiencias en esta dirección, parece indicar la conveniencia de que los alumnos empiecen a familiarizarse con estos ámbitos de conocimiento.\nPython posee los elementos necesarios para construir los gráficos que se necesitan para una explicación sencilla de un conjunto de datos de producción alimentaria, y su sintaxis resulta fácil de comprender, aunque otras herramientas, como el lenguaje R, sean más utilizadas en el análisis estadístico y de datos.\nTener un contacto práctico con el lenguaje facilitará la adquisición de conocimientos suplementarios, ya sea en el ámbito del análisis estadístico de datos, de los métodos cuantitativos de mejora de procesos (Six Sigma), en la programación informática, o en la introducción a la IA.\nPython es una herramienta gratuita, por lo que tanto el lenguaje como las herramientas para trabajar con él están a disposición libre en la web. En nuestro caso, utilizaremos una herramienta de Google, llamada Google colab , que permite utilizar scripts Python mediante la interfase jupyter. Esta herramienta se usa dentro del entorno de Google Chrome, y no es necesario hacer ninguna instalación de software. Para el entorno de la enseñanza, Google Colab resulta conveniente y suficiente.\n\nEl entorno de la industria alimentaria produce un constante flujo de datos como resultado tanto de la implantación de sistemas de captura automáticos como del aumento de la tecnificación de los puestos de trabajo; se requiere por parte de los profesionales industriales que sean capaces de analizar esta enorme cantidad de datos para transformarlos en información para la decisión. En la empresa industrial actual, son los ingenieros y técnicos de planta, y no estadísticos o ingenieros informáticos, quienes participan diariamente en la presentacion y discusion de los datos y en la toma de decisiones operativas, tanto en los equipos de trabajo como ante la Dirección. Por esta razón, considero necesario proporcionar a los estudiantes de la Formación Profesional un conocimiento básico de los conceptos, herramientas y métodos del análisis de datos, así como de algunas técnicas de presentación y comunicación de la información.\nMi experiencia industrial me ha mostrado que en las situaciones reales, sobre el terreno, la comprensión práctica de los conceptos es más importante que su rigurosa formulación matemática. Por esta razón, en el desarrollo del contenido del libro insistiré más en la forma de aplicar las herramientas y entender los análisis que en el conocimiento formal de las fórmulas estadísticas y su deducción matemática, o de la programación informática en sí. He hecho especial hincapié en la utilización de herramientas sencillas, casi siempre gráficas, que son una ayuda valiosa para comprender la información contenida en un conjunto de datos. El objetivo es proporcionar al estudiante de industrias alimentarias las bases de la metodología para análisis de sus datos, y cómo puede aplicarse esta metoología a la resolución de problemas técnicos concretos, más que el conocimiento de la teoría matemática de la estadística o la programación.\nHe evitado las explicaciones formales sobre temas estadísticos. Así, por ejemplo, al explicar la media de un conjunto de datos considero más importante entender el concepto físico de “centro de gravedad” que los conceptos estadísticos de esperanza matemática, que no se tocan en este texto. En este sentido, he intentado que el alumno aprenda bien la diferencia entre “en qué consiste” un estadístico, y “cómo se calcula”. Comprender la diferencia entre el concepto y su fórmula de cálculo es fundamental para entender cómo aplicarlo.\nPor ejemplo, veremos cómo el modo de cálculo de la media aritmética hace que su valor sea equivalente al centro de gravedad de un conjunto de datos; si aproximamos la idea a la de un sistema de palanca en equilibrio sobre un fulcro, será muy fácil entender y hacer entender por qué los valores anómalos extremos desvían notablemente la media y deben analizarse con cuidado: una pequeña desviación en un item muy lejos de gravedad tiene un gran efecto sobre el punto de equilibrio del sistema, debido a la longitud del brazo de palanca; de la misma manera, los valores extremos pueden introducir desviaciones en el valor de la media aritmética que siempre hay que considerar.\nTambién será fácil comprender que otros estadísticos, como la mediana, que se calculan de forma diferente (la mediana es, simplemente, el valor central del conjunto de datos) resultan menos o nada afectados por los valores extremos,; veremos en la práctica las ventajas e inconvenientes de cada uno.\n\n\n\n\n\n\n\n\n\n\n\n(a) Diagrama de puntos (dotplot) mostrando la posición de la media aritmética\n\n\n\n\n\n\n\n\n\n\n\n(b) Palanca de primer grado con el fulcro como punto de equilibrio\n\n\n\n\n\n\n\nFigura 1.1: Comparación gráfica del significado de la media en un diagrama de puntos (dotplot) con una palanca de primer grado.\n\n\n\nUn curso de introducción al análisis de datos industriales debe ser, ante todo, práctico y orientado a su aplicación en el entorno industrial real. Los principales temas de trabajo de análisis de los datos en la industria tienen que ver con su captura, su almacenamiento y su depuración, su descripción utilizando gráficos, la inferencia (intervalos de confianza y tests), la construcción de modelos explicativos, el diseño de los experimentos industriales, el control estadístico de la calidad y la exposición y presentación de resultados. Dado el alcance limitado de este documento, no se entrará en los métodos estadísticos en sí, y necesitarán de un estudio posterior si el alumno tiene interés en profundizar en ellos.\nA pesar de que los temas más especializados puedan ser importantes en algunas aplicaciones específicas, no preparan al estudiante para lo que se va a encontrar en el terreno en la mayor parte de las ocasiones. En cambio, la resolución de problemas en equipo en un entorno de aprendizaje dinámico enfrentándose a problemas exigentes, y el desarrollo de las habilidades de análisis, de síntesis y de comunicación, tendrán un impacto mucho más positivo.\nHe intentado mostrar la necesidad de que los estudiantes comprendan y apliquen el método científico en el entorno industrial, y no sólo apliquen un recetario de procedimientos de manera automática. Son mucho mas importantes la comprensión y la utilización adecuada del método científico y de las herramientas y gráficos básicos, antes que la aplicación rutinaria y mecánica de determinadas fórmulas matemáticas o métodos sofisticados y complejos (como la IA) que el alumno puede no comprender en toda su profundidad.\nLas industrias líderes destacan por la aplicación intensiva de métodos tales como Six Sigma, Lean Manufacturing, diseño robusto de productos, y otros que hacen un uso intensivo de los datos, tanto de los obtenidos en producción como de los obtenidos en la realización de experimentos bien diseñados. Pero la mejora de la competitividad en estas empresas no se debe tanto a la aplicación de unos u otros métodos, como al desarrollo del juicio analítico de sus equipos humanos y a la aplicación de lo aprendido a la mejora continua de los procesos industriales. Veremos que la experiencia y el conocimiento tecnológico de estos procesos son fundamentales para el desarrollo del buen juicio analítico, y, en consecuencia, para la buena interpretación de los resultados que se obtienen con las herramientas estadísticas y de análisis.\nTratándose de un libro para el uso en la Formación Profesional, considero prioritario que su estudio se oriente al desarrollo de habilidades que sean de aplicación práctica directa en el puesto de trabajo y además faciliten la empleabilidad del estudiante, y no a la obtención de conocimiento abstracto. En la última edición del Informe sobre el futuro del empleo disponible cuando se edita este libro, publicada por el Foro Económico Mundial (WEF) en junio de 2025 (World Economic Forum 2025), el WEF considera que:\n\n…al igual que en las dos ediciones anteriores de este informe, el pensamiento analítico sigue siendo la principal habilidad básica para los empleadores, ya que siete de cada 10 empresas lo consideran esencial. A esto le siguen la resiliencia, la flexibilidad y la agilidad, junto con el liderazgo y la influencia social, lo que subraya el papel fundamental de la adaptabilidad y la colaboración junto con las habilidades cognitivas. El pensamiento creativo y la motivación y la autoconciencia ocupan el cuarto y quinto lugar, respectivamente. Esta combinación de habilidades cognitivas, de autoeficacia e interpersonales dentro de los cinco primeros enfatiza la importancia atribuida por los encuestados a tener una fuerza laboral ágil, innovadora y colaborativa, donde tanto la capacidad de resolución de problemas como la resiliencia personal son críticas para el éxito.\n\nA estas habilidades se suman las dos que más crecen en estos dos últimos años: la IA y el análisis de datos. La voluntad de este libro es proporcionar conocimientos que ayuden al estudiante a desarrollarse en esta dirección.\n\n\n\n\nWorld Economic Forum. 2025. «Future of Jobs Report, 2025». 91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World Economic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2025/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#a-quién-va-dirigido-este-libro",
    "href": "index.html#a-quién-va-dirigido-este-libro",
    "title": "Notas para el master de quesería",
    "section": "1.1 A quién va dirigido este libro",
    "text": "1.1 A quién va dirigido este libro\nEl libro está orientado a completar la formación técnica de los estudiantes de Formación Profesional, en las especialidades relacionadas con la actividad productiva industrial. También creo que será de utilidad para los técnicos industriales en activo que no han tenido la oportunidad de recibir una adecuada formación en estas metodologías, y que han encontrado dificultad para lanzarse a su aprendizaje mientras desarrollan so actividad profesional. Espero, también, que los profesores de la Formación Profesional en estos ámbitos de competencia encuentren en este documento los elementos de apoyo que les permitan integrar estas enseñanzas en sus respectivos ciclos formativos.\nEn todos los casos, el aprendizaje requerirá de un esfuerzo que quizás será mayor en los estudiantes que no tengan una base mínima en álgebra y cálculo. En estos casos, el trabajo en equipo y la discusión abierta entre compañeros y con los profesores ayudará a la comprensión de los conceptos.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#organización-del-libro",
    "href": "index.html#organización-del-libro",
    "title": "Notas para el master de quesería",
    "section": "1.2 Organización del libro",
    "text": "1.2 Organización del libro\nEl libro se estructura en varias partes:\nUna introducción, con dos capítulos:\n\nEl capítulo 1 hace una introducción general al pensamiento estadístico y su aplicación industrial, explica los tipos de datos que nos podemos encontrar en el entorno industrial y los estudios que dan lugar a estos tipos de datos.\nEl capítulo 2 presenta las dos herramientas básicas que se usarán en el libro, la hoja de cálculo y el programa estadístico de código abierto R. Se describen los aspectos básicos del software estadístico R y sus principales aplicaciones en la manipulación, limpieza y análisis de conjuntos de datos. Se introduce también el concepto actual de reproducibilidad, y se explica su importancia y el impacto de utilizar una hoja de cálculo o un análisis basado en un script programado.\n\nLa primera parte introduce las técnicas básicas:\n\nEn el capítulo 3 se presentan conceptos básicos, tales como población, muestra y variable, y se hace una introducción al concepto de flujo de trabajo y al concepto de datos ordenados o tidy data, que resulta fundamental para las fases posteriores de análisis. Se explica también cómo mover datos desde excel a R y viceversa.\nEl capítulo 4 habla de la exploración de los datos como primer paso en el análisis. Se introducen las distribuciones de frecuencia y los principales gráficos que se utilizarán, y se explica cómo hacerlos con Excel y con R.\nEn el capítulo 5 se presentan los principales estadísticos que describen una población, mediante un valor central y una medida de dispersión, tanto paramétricos (media y varianza) como no paramétricos (mediana y rango intercuartil), de forma sencilla y sin recurrir a explicaciones matemáticas o estadísticas avanzadas.\nEl capítulo 6 explica la importancia de comprender la forma de los datos, es decir, su distribución. Se ponen en práctica las herramientas estudiadas en los dos capítulos anteriores para estudiar algunos casos prácticos.\nEn el capítulo 7 se presentan los métodos para detectar la relación entre dos variables (correlación y regresión lineal), haciendo énfasis en los métodos gráficos, y se discuten las diferencias entre correlación y causalidad.\nEl capítulo 8 se hace una breve introducción a las técnicas de comunicación, tanto a la presentación de datos como a la preparación de informes. Se hace hincapié en las nuevas herramientas como Quarto, que facilitan la elaboración de informes automatizados.\n\nLa segunda parte del libro trata algunas conceptos avanzados, pero desarrollados de forma básica; he intentado que estos conceptos puedan comprenderse sin necesidad de desarrollos teóricos o formulaciones complejas.\n\nEl capítulo 9 introduce el concepto de probabilidad, así como las distribuciones de probabilidad, necesarias para la construcción de los tests de hipótesis y, en general, de la estadística inferencial. Este contenido se presenta de forma breve y, sobre todo, práctica.\nEl capítulo 10 introduce de manera sencilla el análisis de la varianza, necesario para métodos importantes en la industria como el control de la precisión analítica, que se trata en un capítulo posterior.\nEn el capítulo 11 se hace una introducción al diseño de experimentos. La utilidad de esta técnica es primordial para el industrial, sobre todo para el área de I+D y el diseño de productos. Dado que esta técnica puede ser muy compleja en su aplicación real, se facilitan enlaces a otros recursos, como cursos, que serán útiles a los que quieran profundizar más.\n\nLa tercera y última parte del libro desarrolla algunas aplicaciones prácticas al entorno industrial de lo estudiado en los capítulos anteriores.\n\nEl capítulo 12 presenta una de las aplicaciones más importantes de la estadística en el entorno industrial, el control estadístico de procesos. Dada la importancia de este capítulo, se refuerza su contenido con numerosos ejemplos y casos prácticos, y se incluye un caso extenso para su análisis.\nEl capítulo 13 trata del análisis del sistema de medición, la calidad de las medidas y la medida de la precisión analítica. Resulta sorprendente la cantidad de laboratorios que dan soporte analítico a procesos productivos de gran impacto económico en la vida de la empresa, sin realizar nunca un autocontrol sobre el nivel de precisión de sus análisis. En este capítulo se hace una presentación básica del tema con el objetivo de que resulte útil y práctica.\nFinalmente, el capítulo 14 explica la aplicación estructurada de los conceptos y técnicas estudiadas en el libro a la mejora de los procesos y la calidad industrial mediante el método Six Sigma.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#cómo-usar-el-libro",
    "href": "index.html#cómo-usar-el-libro",
    "title": "Notas para el master de quesería",
    "section": "1.3 Cómo usar el libro",
    "text": "1.3 Cómo usar el libro\nHe intentado que cada capítulo sea lo más autocontenido posible de forma que se facilite la organización pedagógica por temas. No obstante, a veces puede ser necesario conocer los contenidos de los capítulos anteriores, por lo que se sugiere estudiarlo en el orden presentado.\nEl libro es eminentemente práctico, con numerosos ejercicios; su resolución puede ser individual o en equipo.\nAlgunos recuadros utilizan códigos de color para indicar el objetivo de la información que contienen. Básicamente, los colores utilizados son:\n\n\n\n\n\n\nProblema o cuestión a resolver\n\n\n\nEl recuadro azul se utilizará para proponer problemas sencillos cuya respuesta se encuentra más adelante en el texto. El objetivo de estos problemas es estimular la reflexión, aunque puede ser necesario recurrir a cálculos sencillos ayudados por las herramientas disponibles.\n\n\n\n\n\n\n\n\nRespuesta al problema o explicación del código R\n\n\n\n\n\nEl recuadro verde se utilizará para dos cosas\n\nproponer una respuesta al problema planteado; respuesta que no tiene por qué ser la única posible, o bien\npara explicar las líneas de código R que se mostrarán a continuación\n\nNormalmente se presentará de forma desplegable para no entorpecer la lectura del texto.\n\n\n\nAdemás se incluyen diferentes tipos de avisos cada vez que se introduce algún concepto que es necesario resaltar.\n\n\n\n\n\n\nImportante\n\n\n\nEn este formato se indican cuestiones importantes\n\n\n\n\n\n\n\n\n¡Atención!\n\n\n\nEn este formato se indican cuestiones a las que hay que prestar especial atención o que pueden inducir a error",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#uso-del-ordenador-y-el-software-estadístico",
    "href": "index.html#uso-del-ordenador-y-el-software-estadístico",
    "title": "Notas para el master de quesería",
    "section": "1.4 Uso del ordenador y el software estadístico",
    "text": "1.4 Uso del ordenador y el software estadístico\nEn la práctica diaria, los técnicos industriales usan los ordenadores para almacenar y visualizar los datos de producción, para solucionar problemas mediante análisis estadísticos, y para presentar sus resultados de forma gráfica. De la misma forma que en el entorno industrial, en este libro se utilizarán también los ordenadores de forma habitual, y por esta razón es imprescindible que los estudiantes tengan acceso individual a un ordenador en el que esté instalado el software recomendado, y que se acostumbren a utilizarlo para resolver los problemas y casos planteados como ejercicios prácticos, individualmente y en grupo.\nEl estudiante que se incorpora a una empresa, sea en un laboratorio o en una planta de producción, se va a encontrar muy pronto delante de una hoja de cálculo, y debe saber cómo utilizarla correctamente. Actualmente, lo más probable es que esa hoja de cálculo sea Microsoft Excel, aunque hay otras alternativas posibles, como Google Sheets, Apple Numbers, OpenOffice Calc y algunas más. La gran dominancia en el mercado de Microsoft Excel ha hecho que todas estas herramientas sean totalmente compatibles o tengan modos de compatibilidad con Excel. Por esta razón, este libro se basa en la utilización de Excel como hoja de cálculo y herramienta principal para el almacenamiento de datos.\nA lo largo del libro se presentarán informes y gráficos obtenidos con Microsoft Excel, y también con el software estadístico R. Prácticamente todos ellos pueden ser exportados a otras herramientas, como Google Sheets, OpenOffice, Minitab o Matlab, o analizarse con otros lenguajes de programación, como Python o Julia. En realidad, el método de análisis y cómo obtener un resultado correcto son aspectos más importantes que la herramienta que se utilice para ello, por lo que queda en manos del instructor la decisión final sobre qué usar y cómo. Para facilitar este trabajo de conversión, en su caso, todo el material del libro y los datos de ejemplo estarán disponibles en un repositorio de GitHub.\nAlgunos ejercicios tienen que ver con la interpretación y presentación de los resultados. Es importante que estos trabajos se realicen en grupo y se haga énfasis en la comprensión del problema y en su correcta exposición; en los equipos industriales de hoy, la discusión de problemas y la exposición de resultados, en reuniones de trabajo o en paneles informativos a pie de planta, forma parte del trabajo diario. Estas habilidades de comunicación deben ser desarrolladas en los estudiantes de forma prioritaria.\nLa ventaja de R sobre Excel es que el código R, si está bien documentado, muestra cada paso realizado, y esto permite que otras personas puedan verificar el resultado y reproducirlo a partir de los datos originales, e incluso reutilizar los procedimientos. Utilizar código en vez de clicks de ratón es esencial para asegurar la reproducibilidad de los análisis de datos1. Por esta razón, recomiendo el uso del lenguaje R como complemento o alternativa a la hoja de cálculo, tanto para analizar como para visualizar datos. Sin embargo, como la realidad del mundo de la empresa es que los lenguajes como R están todavía poco introducidos, es inevitable mantener el uso de la hoja de cálculo; en el libro se explicarán algunas mejores prácticas, que permitirán el uso simultáneo de ambas herramientas de forma óptima.\nRespecto a la programación informática, en el libro no se hace énfasis en la programación R más que como sucesión de órdenes individuales en scripts sencillos. No se busca la eficiencia computacional ni la rapidez en el cálculo, sino la comprensión de la metodología de resolución de problemas y cómo ésta se apoya en las herramientas presentadas. De la misma manera, tampoco se hace ningún uso de la programación en Excel, ya sea con macros o con Visual Basic; estos temas quedan fuera del perímetro de este libro.\nUn paso en la dirección de la implantación de flujos de trabajo reproducibles es la elaboración de informes automatizados. Estos informes incluyen el código R, los comentarios del autor en forma de texto formateado en markdown, y los resultados del código. Herramientas como Quarto, o Google Colaboratory, que usa la interface Jupyter, son nuevas formas de elaborar y presentar los informes y resultados estadísticos. En el entorno docente, estas herramientas abren posibilidades muy interesantes en la presentación de un ejercicio o un exámen escrito, ya que el alumno puede detallar perfectamente todos los pasos hasta llegar al resultado final, y facilita la revisión por sus compañeros o por el profesor a cargo de la asignatura.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#sec-aprendizaje",
    "href": "index.html#sec-aprendizaje",
    "title": "Notas para el master de quesería",
    "section": "1.5 Recursos adicionales y cómo usarlos",
    "text": "1.5 Recursos adicionales y cómo usarlos\nEn este libro se hace una introducción muy general a R y a Excel; se presupone que el alumno tiene un conocimiento básico de ambas herramientas. Si no tiene ninguna formación sobre el lenguaje R y el entorno RStudio, recomiendo hacer alguna formación previa sencilla que introduzca los conceptos básicos. Datacamp tiene cursos gratuitos de introducción a R; también hay cursos de formación tanto de R como de Excel en otras plataformas web como edX, Udemy y Coursera, muchos de ellos gratuitos. El Gobierno de España, dentro de una de sus iniciativas de transformación digital, la iniciativa de datos abiertos, incluye también una amplia referencia a cursos de formación sobre R.\nTodos los datos presentados en los ejemplos se incluyen en hojas de cálculo que están disponibles en GitHub. También se incluyen fuentes de datos adicionales que pueden permitir plantear nuevos ejercicios.\nAl final del libro se incluye una bibliografía completa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#sobre-el-libro",
    "href": "index.html#sobre-el-libro",
    "title": "Notas para el master de quesería",
    "section": "1.6 Sobre el libro",
    "text": "1.6 Sobre el libro\nEl libro ha sido editado en Quarto. Está disponible en PDF.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#agradecimientos",
    "href": "index.html#agradecimientos",
    "title": "Notas para el master de quesería",
    "section": "Agradecimientos",
    "text": "Agradecimientos\n\n\n\n\n\nWorld Economic Forum. 2025. «Future of Jobs Report, 2025». 91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World Economic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2025/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Notas para el master de quesería",
    "section": "",
    "text": "El concepto de reproducibilidad, cada vez más importante, se desarrolla en el capítulo 1↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción general</span>"
    ]
  },
  {
    "objectID": "001-intro.html",
    "href": "001-intro.html",
    "title": "2  La estadística en el entorno industrial.",
    "section": "",
    "text": "2.1 Introducción\n¿Alguna vez te has preguntado cómo podemos resolver problemas en el trabajo de forma sistemática y ordenada? La estadística y el método científico nos ayudan a hacerlo, y son más sencillos de lo que parece.\nEl método científico es como una receta que seguimos para resolver problemas: primero observamos qué está pasando, luego pensamos qué puede estar causando el problema, después hacemos pruebas para comprobarlo, y finalmente sacamos conclusiones y tomamos acciones orientadas a eliminar el problema o conseguir una mejora, aunque sea parcial, y repetimos el ciclo. La estadística nos ayuda a entender si nuestras pruebas funcionaron o no, usando números y datos reales.\nPensar de forma estadística significa entender que todo lo que hacemos en el trabajo está conectado, y que es normal que las cosas cambien o varíen un poco. Lo importante es saber cuándo estos cambios son normales y cuándo indican un problema real. Es como cuando cocinamos: sabemos que cada plato puede salir un poco diferente, pero reconocemos cuando algo ha salido realmente mal.\nEn las fábricas y talleres, estas herramientas son muy útiles. Por ejemplo, si una máquina empieza a producir piezas defectuosas, podemos:\nLa experimentación es muy importante en la industria. En vez de cambiar las cosas al azar, hacemos pruebas organizadas para ver qué funciona mejor. Es como cuando ajustas la temperatura del horno y el tiempo de cocción para que un pastel salga perfecto: pruebas diferentes combinaciones y anotas los resultados.\nLo mejor de todo es que cada vez que hacemos estos experimentos, aprendemos algo nuevo. Incluso si algo no funciona como esperábamos, esa información nos ayuda a mejorar la próxima vez. Es un proceso continuo de aprendizaje y mejora.\nEsta forma de trabajar nos ayuda a:\nRecuerda: no necesitas ser un genio de las matemáticas para usar estas herramientas. Lo importante es ser organizado, observar bien lo que pasa y anotar los resultados de lo que hacemos. Así, poco a poco, podemos mejorar nuestro trabajo y resolver problemas de forma más eficiente.\nLos métodos estadísticos nos ayudan a describir y comprender la variabilidad. Cuando hablamos de variabilidad queremos decir que sucesivas observaciones de un mismo proceso o sistema no dan exactamente los mismos resultados. Por ejemplo, el consumo de gasolina de un coche no es siempre igual, sino que varía de manera considerable. Esta variación depende de muchos factores, como la forma de conducir, el tipo de carretera, la situación del propio vehículo (presión de neumáticos, compresión del motor, …), la marca de la gasolina, el octanaje, o incluso las condiciones meteorológicas. Todos estos factores son causas de variabilidad en el consumo de gasolina. La estadística nos permite analizar estos factores y determinar cuáles son los más importantes o tienen mayor impacto en el consumo; una vez conocidos, podemos actuar sobre ellos.\nEn este libro aprenderemos a utilizar herramientas diversas, tanto estadísticas como de la ciencia de datos, para realizar nuestro análisis. Para aprender de los datos necesitamos más que los simples números; para interpretarlos necesitaremos siempre el conocimiento del proceso industrial que estamos analizando.En un análisis de la producción de un producto lácteo, por ejemplo, los números significan poco sin un conocimiento del proceso; los valores de pH, temperatura o concentración de lactosa influyen en el resultado del proceso de forma diferente. Los datos son números dentro de un contexto, y necesitamos conocer este contexto para dar sentido a los números.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "001-intro.html#introducción",
    "href": "001-intro.html#introducción",
    "title": "2  La estadística en el entorno industrial.",
    "section": "",
    "text": "Observar qué está pasando\nPensar en posibles causas\nHacer pruebas controladas para encontrar el problema\nUsar números y datos para confirmar si lo hemos solucionado\n\n\n\n\n\nResolver problemas de forma ordenada\nTomar mejores decisiones basadas en datos reales\nMejorar la calidad de nuestro trabajo\nAhorrar tiempo y dinero evitando errores\n\n\n\n\n\n\n\n\n\n\nImportante\n\n\n\nEl objetivo principal de la mejora industrial es la reducción de la variabilidad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "001-intro.html#el-pensamiento-estadístico-y-el-método-científico.",
    "href": "001-intro.html#el-pensamiento-estadístico-y-el-método-científico.",
    "title": "2  La estadística en el entorno industrial.",
    "section": "2.2 El pensamiento estadístico y el método científico.",
    "text": "2.2 El pensamiento estadístico y el método científico.\nLa estadística y el método científico mantienen una relación fundamental que impulsa el avance del conocimiento y la innovación. El método científico, con sus pasos de observación, hipótesis, experimentación y conclusiones, encuentra en la estadística las herramientas necesarias para validar o refutar hipótesis de manera objetiva y cuantificable.\nEl pensamiento estadístico es una filosofía de aprendizaje y acción basada en tres principios fundamentales:\n\nque todo proceso industrial está compuesto a su vez por procesos interconectados,\nque la variabilidad existe y es inherente a estos procesos, y\nque entender y reducir la variación es clave para la mejora y el éxito\n\nEn el entorno industrial, esta integración entre estadística y método científico ha revolucionado los procesos de mejora continua. Por ejemplo, cuando una línea de producción enfrenta problemas de calidad, el método científico guía la investigación sistemática: se observa el proceso, se formulan hipótesis sobre las causas del problema, se diseñan experimentos controlados, y se analizan los resultados estadísticamente para determinar la validez de las soluciones propuestas.\nLa experimentación industrial, particularmente a través del diseño de experimentos (DOE), se ha convertido en una herramienta fundamental para la mejora continua. En lugar de modificar procesos basándose en intuiciones o experiencias pasadas, las empresas pueden realizar experimentos controlados que:\n\nOptimizan múltiples variables simultáneamente\nIdentifican interacciones entre factores que afectan el proceso\nReducen el tiempo y costo de las mejoras\nProporcionan conclusiones respaldadas por evidencia estadística\n\nAl realizar experimentos controlados, las empresas pueden probar hipótesis que permitan evaluar el impacto de diferentes variables en los procesos y productos y determinar los valores óptimos de los parámetros de los procesos para maximizar la calidad y la eficiencia. Esto genera conocimiento valioso sobre los procesos y productos, permite tomar decisiones más informadas, y ayuda a crear y mantener una ventaja competitiva.\nLa experimentación, combinada con el análisis estadístico, permite a las empresas implementar ciclos de mejora continua, donde los resultados de cada experimento se utilizan para refinar los procesos y productos.\nAl integrar el pensamiento estadístico con el método científico en el entorno industrial, las organizaciones pueden desarrollar una cultura de mejora continua basada en la evidencia de los datos y no en suposiciones o intuiciones. En una cultura industrial basada en el método científico, cada problema es una oportunidad para experimentar, aprender y optimizar. Esta aproximación sistemática no solo mejora la calidad y eficiencia de los procesos, sino que también fomenta la innovación y el aprendizaje organizacional continuo.\nLa clave del éxito radica en entender que la experimentación no es un evento aislado, sino un proceso continuo de aprendizaje y mejora. Cada experimento, exitoso o no, aporta información valiosa que, analizada correctamente mediante métodos estadísticos, contribuye al conocimiento colectivo de la organización y sienta las bases para futuras mejoras. La estadística y el método científico no solo son pilares fundamentales para la investigación académica, sino que también desempeñan un papel crucial en el entorno industrial. La aplicación sistemática del método científico, respaldada por herramientas estadísticas, permite a las empresas optimizar procesos, mejorar la calidad de sus productos y servicios, y fomentar la innovación.\n\n\n\n\n\n\nEl pensamiento estadístico en la toma de decisiones industriales\n\n\n\nEn el entorno industrial, el pensamiento estadístico es esencial para:\n\nTomar decisiones basadas en datos, evitando decisiones basadas en intuiciones o suposiciones.\nEvaluar riesgos, cuantificando la incertidumbre y permitiendo tomar decisiones que minimicen los riesgos.\nMejorar el conocimiento de los procesos para una toma de decisiones más eficaz.\n\nComunicar resultados, presentando los resultados de los análisis de manera clara y concisa.\n\nLa integración del método científico y el pensamiento estadístico en la cultura empresarial impulsa la innovación, la eficiencia y la competitividad.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "001-intro.html#los-datos-industriales",
    "href": "001-intro.html#los-datos-industriales",
    "title": "2  La estadística en el entorno industrial.",
    "section": "2.3 Los datos industriales",
    "text": "2.3 Los datos industriales\nEn el entorno industrial, podemos organizar los datos en varias categorías, considerando tanto su naturaleza como la forma en que se obtienen:\nSegún su estructura:\n\nDatos estructurados:\n\nSon datos organizados en un formato definido, como tablas de bases de datos, hojas de cálculo o archivos CSV. Su estructura facilita el análisis y la consulta.\nEjemplos: lecturas de sensores, registros de producción, datos de control de calidad, información de inventario.\n\nDatos no estructurados:\n\nSon datos que no tienen un formato predefinido, como texto, imágenes, audio o video.Su análisis requiere técnicas más avanzadas, como el procesamiento de lenguaje natural o la visión artificial.\nEjemplos: registros de mantenimiento, informes de incidentes, imágenes de inspección visual, grabaciones de audio de maquinaria.\n\nDatos semiestructurados:\n\nSon datos que tienen cierta estructura, pero no tan rígida como los datos estructurados, y permiten una mayor flexibilidad en el almacenamiento y el intercambio de información.\nEjemplos: archivos XML o JSON, registros de eventos.\n\n\nSegún su origen y método de obtención:\n\nDatos históricos (Estudios retrospectivos):\n\nSon datos recopilados en el pasado, que se utilizan para analizar tendencias, identificar patrones y predecir el comportamiento futuro. Permiten comprender la evolución de los procesos.\nEjemplos: registros de producción de años anteriores, datos de fallos de maquinaria, históricos de ventas.\n\nDatos observacionales (Estudios observacionales):\n\nSon datos recopilados mediante la observación de procesos o sistemas, sin intervenir en ellos. Permiten identificar relaciones entre variables y comprender el comportamiento de los procesos en condiciones reales.\nEjemplos: mediciones de temperatura, vibración o presión de maquinaria, registros de tiempo de ciclo de producción.\n\nDatos experimentales (Experimentos diseñados):\n\nSon datos recopilados mediante la realización de experimentos controlados, en los que se manipulan variables para evaluar su efecto. Permiten establecer relaciones de causa y efecto, estudiar las interacciones entre las variables y optimizar el rendimiento de los procesos.\nEjemplos: datos de pruebas de rendimiento de nuevos materiales, resultados de experimentos de optimización de procesos.\n\nDatos de monitorización o control de procesos en tiempo real:\n\nson datos que se recaban de manera instantanea, y permiten actuar de manera casi inmediata en los procesos; permiten implementar el mantenimiento predictivo, y evitar perdidas de produccion.\nEjemplos: datos de sensores que detectan fallos en maquinaria, alarmas de procesos fuera de control.\n\n\nEn el contexto de la Industria 4.0, la cantidad y variedad de datos industriales está aumentando exponencialmente.\n\nEstudios retrospectivos o históricos\nUn estudio retrospectivo o histórico es el que utiliza una muestra o todos los datos históricos de un proceso, recogidos en el pasado durante un período determinado de tiempo. El objetivo de un estudio de este tipo puede ser la investigación sobre la relación entre algunas variables, o explorar la calidad de la información disponible, o construir un modelo que permita explicar el proceso tal como es actualmente, o saber si se ha desviado. Estos modelos del proceso se denominan modelos empíricos, porque están basados en los propios datos del proceso y no en una formulación teórica sobre el mismo.\nUn estudio retrospectivo tiene la ventaja de tener a su disposición un gran número de datos que ya han sido recogidos, minimizando el esfuerzo de obtenerlos. Sin embargo, tiene varios problemas potenciales:\n\nSi no disponemos de detalles suficientes, es posible que no podamos determinar si las condiciones de variación de los valores obtenidos responden a las mismas causas que en la situación actual.\nEs posible que nos falte algún valor clave que no haya sido recogido o que lo haya sido de manera defectuosa\nAlgunas veces, la fiabilidad y validez de los datos de proceso históricos son dudosas, o al menos, cuestionables.\nLos datos históricos no siempre se han recogido con la perspectiva actual del proceso, y es posible que no nos proporciones explicaciones adecuadas del proceso en su situación actual.\nA veces queremos utilizar los datos históricos de proceso para fines que no estaban previstos cuando se recogieron\nLas notas sobre los valores del proceso, incluyendo los valores anormales, pueden ser insuficientes o inexistentes, y no tenemos ninguna explicación sobre los posibles valores anómalos que detectamos en el análisis.\n\nUsar datos históricos siempre tiene el riesgo de que, por la razón que sea, no se hayan recogido datos importantes, o que estos datos se hayan perdido, o se hayan transcrito de forma inadecuada o incorrecta. Es decir, los datos históricos pueden tener problemas de calidad de datos.\nEl hecho de que algunos datos se hayan recogido históricamente no siempre quiere decir que estos datos sean relevantes o útiles. Cuando el grado de conocimiento del proceso no es suficiente, o no se basa en un análisis metódico y riguroso de los datos, es posible que no se hayan recogido algunos datos que pueden ser importantes para el proceso, a veces simplemente porque son complejos o difíciles de analizar. Los datos históricos no pueden proporcionar la información que buscamos si la información de las variables clave nunca se ha recogido o se ha hecho sin una buena base experimental.\nEl propósito del análisis de los datos industriales es aislar las causas que están detrás de los sucesos que afectan e influyen en los procesos. En los datos históricos, estos sucesos pueden haber ocurrido semanas, meses o incluso años antes, sin que haya registros ni notas que hayan intentado explicar estas causas, y los recuerdos de las personas que han participado en ellos se pierden con el tiempo, o se alteran involuntariamente, proporcionando explicaciones supuestamente válidas pero que en realidad son incorrectas. Por eso, con frecuencia, el análisis de los datos históricos puede poner de manifiesto hechos interesantes, pero sus causas quedan sin explicar.\nLos estudios históricos pueden requerir una fase previa de preparación y depuración de datos que puede llegar a ser muy larga y tediosa. Se estima que en muchos estudios de ciencia de datos, el tiempo de preparación de los datos puede llegar al \\(60\\%\\) del tiempo total empleado en el estudio. Las herramientas de análisis de datos son de gran ayuda en esta fase del proceso, aunque en muchas ocasiones será necesario un trabajo manual de recolección de datos en papel, hojas de cálculo diversas y otras fuentes. Esta fase es muy útil no sólo para la preparación de datos para el estudio, sino para mejorar el conocimiento de los datos, cómo se originan y cómo se almacenan. Este conocimiento siempre es de gran utilidad para mejorar los procedimientos actuales de captura de datos, facilitando la fiabilidad de los análisis futuros.\n\n\nEstudios observacionales\nComo su nombre indica, un estudio observacional simplemente observa un proceso durante un tiempo de operación en rutina. Normalmente, el ingeniero o técnico interfiere lo mínimo posible en el proceso; sólo lo suficiente para recoger la información que necesita, si piensa que esa información puede ser relevante. En muchas ocasiones, el estudio no forma parte de los controles de rutina, y representa un trabajo adicional.\nSi se planifican adecuadamente, los estudios observacionales proporcionan datos fiables, precisos y completos para documentar un proceso. Por otra parte, estos estudios proporcionan una información limitada sobre las relaciones entre las variables del proceso, porque es posible que durante el tiempo limitado de observación, el rango de variación de las variables no recoja todas las situaciones posibles; por ejemplo, las situaciones extraordinarias.\n\n\nExperimentos diseñados\nLa tercera forma de recoger información de un proceso son los experimentos diseñados. En un experimento de este tipo, el ingeniero o técnico hace un cambio deliberado en las variables que controla (llamadas factores), observa el resultado, y toma una decisión respecto a qué variable o variables son responsables de los cambios que observa en el proceso.\nUna diferencia importante respecto a los estudios históricos y los observacionales es que las diferentes combinaciones de factores se aplican al azar sobre un conjunto de unidades experimentales. Esto permite establecer con precisión las relaciones causa-efecto, cosa que no suele ser posible ni en los estudios históricos ni en los observacionales.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>La estadística en el entorno industrial.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html",
    "href": "005-herramientas.html",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "",
    "text": "3.1 Introducción\nEn este capítulo veremos las herramientas que utiizaremos en el análisis de datos industriales. Nos enfocaremos en dos de ellas: la hoja de cálculo Microsoft Excel, y el software estadístico R. Ambas herramientas están ampliamente extendidas en las empresas y en las instituciones docentes de todo el mundo. R es, además, un software libre, y por lo tanto, con un coste de adquisición cero, lo que facilita su utilización. Microsoft Excel tiene versiones web que se pueden utilizar para un uso básico también sin coste. No son las únicas opciones: hay otros programas estadísticos y de análisis muy utilizados y potentes, tales como Matlab o Minitab, que son de gran interés en ingeniería, aunque tienen un coste bastante elevado, y hojas de cálculo como Google Docs o LibreOffice, casi totalmente compatibles con Microsoft Excel.\nEste capítulo no es un manual de aprendizaje de estas herramientas; se supone que se dispone del conocimiento básico para comprender las instrucciones que se proporcionarán aqui. Como se indicó en el prefacio (ver Sección 1.5), en internet hay multitud de alternativas de calidad para aprender tanto Excel como R de forma gratuita, en ese capítulo se presentaron algunas recomendaciones.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#la-hoja-de-cálculo",
    "href": "005-herramientas.html#la-hoja-de-cálculo",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.2 La hoja de cálculo",
    "text": "3.2 La hoja de cálculo\nLa hoja de cálculo es una herramienta presente hoy día en todos los ámbitos de trabajo y educativos. Desde la aparición de Visicalc, en 1978, ha contribuido a la gestión de miles de empresas, se ha utilizado de manera general en análisis de datos y sus gráficos se han utilizado y se utilizan en publicaciones e informes de todas clases. En la década de los años 80 del pasado siglo, la hoja de cálculo Lotus 1-2-3 fue la aplicación más utilizada en los ordenadores IBM-PC y compatibles, y consiguió facturaciones millonarias para la empresa matriz. Lotus 1-2-3 dominó el mercado hasta la aparición de Microsoft Windows a finales de los años 80; este nuevo sistema operativo favoreció la implantación de Microsoft Excel, que desde entonces se convirtió en la hoja de cálculo dominante.\n\n\n\n\n\n\nVisicalc, primera hoja de cálculo para el ordenador Apple II (1979)\n\n\n\n\n\n\n\nHoja de cálculo Lotus 1-2-3 para MS-DOS (1983)\n\n\n\n\n\n\n\n\n\nMicrosoft Excel (2023)\n\n\n\n\n\n\nAplicaciones de la hoja de cálculo\nLas hojas de cálculo son muy útiles para recoger la información de un conjunto de observaciones. Entre sus principales usos, están:\n\nLa introducción, edición y almacenamiento datos.\nEl filtrado y corrección de errores.\nLa manipulación básica, por ejemplo, mediante tablas dinámicas\nLa preparación y edición de gráficos, incluyendo gráficos dinámicos\nLa presentación de la información, con el apoyo opcional de herramientas adicionales como Microsoft PowerPoint.\n\nLos datos se pueden recoger y guardar de múltiples formas. Cuando la recogida de datos se hace de forma manual en papel, es necesario registrar en el ordenador los datos recogidos. Lo más frecuente es que este registro se haga en hojas de cálculo, como Microsoft Excel o Google Sheets. En algunos casos, el almacenamiento se hace sobre bases de datos, genéricas o desarrolladas a medida.\nActualmente, la tendencia es recoger los datos o bien de forma automática, o bien de forma manual sobre sistemas informatizados (pantallas), lo que permite eliminar el papel y disponer directamente de los datos en un formato digitalizado.\nEn la actualidad, la mayoría de los equipos y líneas de producción se interconectan con los sistemas de información (ver IoT) y almacenan en tiempo real todos los datos necesarios, lo que libera al operario de la pesada tarea de reintroducirlos manualmente, a la vez que reduce los errores debidos a la imputación incorrecta.\nEn todos los casos, es imprescindible asegurar que los sistemas de información pueden exportar sus datos a ficheros de texto tipo fichero plano o tipo CSV, de forma que podamos importarlos tanto a Excel como a R, como veremos más adelante. Estos sistemas de exportación de datos deben diseñarse de forma flexible y abierta, para que tanto la captura como la exportación puedan modificarse y adaptar la recogida de la información a las necesidades de cada momento.\nEn este libro trataremos exclusivamente de lo que llamaremos datos rectangulares: grupos de valores que están asociados a una o más variables, y a varias observaciones. Hay muchos más datos que no se ajustan a esta organización tabular, es el caso de imágenes, sonidos o archivos documentales de texto. Pero la forma más común de almacenar datos industriales es la de las tablas rectangulares, organizadas según el principio de los datos arreglados, que detallaremos en el siguiente capítulo.\n\n\nEl uso correcto de una hoja de cálculo en el análisis de datos industriales\n[…]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#el-software-estadístico-r",
    "href": "005-herramientas.html#el-software-estadístico-r",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.3 El software estadístico R",
    "text": "3.3 El software estadístico R\nR es un lenguaje de programación y un entorno de software utilizado para el análisis estadístico, la visualización de datos y la modelización. Algunas de las características clave de R son:\n\nAmplio espectro de funcionalidades: R abarca una amplia gama de herramientas y paquetes diseñados para realizar diversos análisis estadísticos, exploración de datos y modelización.\nHerramientas gráficas: R dispone de algunas de las bibliotecas gráficas más potentes para la exploración y descripción de datos.\nEstadística descriptiva: R ofrece funciones para calcular estadísticas descriptivas básicas, como la media, mediana, desviación estándar, varianza, rango, cuartiles y percentiles. Estas funciones son esenciales para explorar y resumir datos.\nContrastes de hipótesis: R proporciona funciones para realizar pruebas estadísticas, como t-tests, test chi-cuadrado, ANOVA y pruebas no paramétricas. Estas pruebas permiten evaluar hipótesis y comparar grupos de datos.\nDistribuciones de probabilidad: R incluye una amplia variedad de funciones para trabajar con distribuciones de probabilidad (por ejemplo, normal, uniforme, binomial, Poisson). Esto es útil para generar números aleatorios, calcular probabilidades y cuantiles.\nBibliotecas de funciones (librerías): Además de las amplias funciones básicas de las que dispone, R es capaz de utilizar bibliotecas de funciones (llamadas librerías) que han sido desarrolladas por los propios usuarios, y que amplían sus funcionalidades a todos los campos imaginables, desde el análisis genético al análisis de riesgos bancarios o el control estadístico de procesos.\n\n\nUsos y aplicaciones de R en la estadística industrial\nEn el contexto de la estadística industrial, R se utiliza para:\n\nControl de calidad: R permite analizar datos de procesos industriales, identificar desviaciones y controlar la calidad de los productos.\nOptimización de procesos: Mediante técnicas estadísticas avanzadas, R ayuda a optimizar procesos industriales, reducir costos y mejorar la eficiencia.\nAnálisis de fiabilidad: R se utiliza para evaluar la confiabilidad de sistemas y componentes en la industria.\n\n\n\nUtilización práctica de R en el entorno industrial\n[..a desarrollar..]\n\nImportación de datos y exportación de datos\nManipulación de datos: depuración, corrección, filtrado de datos.\nExploración gráfica de los datos\nAnálisis estadísticos específicos.\nGráficos de control\nInformes automatizados",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#otras-herramientas-python-julia",
    "href": "005-herramientas.html#otras-herramientas-python-julia",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.4 Otras herramientas: Python, Julia",
    "text": "3.4 Otras herramientas: Python, Julia\nTanto Python como Julia son excelentes alternativas a R en el análisis de datos industriales y la estadística industrial, cada uno con sus propias ventajas:\n\nPython\nPython es uno de los lenguajes de programación más populares en el análisis de datos industriales debido a su versatilidad y la amplia disponibilidad de bibliotecas como pandas, NumPy, SciPy, scikit-learn y Matplotlib. Su integración con otras tecnologías (como bases de datos y servicios en la nube) y su facilidad de uso lo convierten en una opción preferida para tareas de análisis de datos, machine learning y visualización de datos.\n\nVentajas:\n\nVersatilidad: Más allá del análisis de datos, Python es un lenguaje generalista con una amplia gama de aplicaciones, desde desarrollo web hasta machine learning. Esto lo convierte en una herramienta altamente versátil para diversas tareas industriales.\nEcosistema de librerías: Cuenta con una vasta colección de librerías especializadas en ciencia de datos (Pandas, NumPy, SciPy), machine learning (Scikit-learn, TensorFlow), y visualización (Matplotlib, Seaborn).\nIntegración con otras tecnologías: Se integra fácilmente con otros lenguajes y herramientas, lo que facilita la automatización de procesos y la construcción de pipelines de datos.\nComunidad activa: Tiene una comunidad enorme y en constante crecimiento, lo que significa una gran cantidad de recursos, tutoriales y soporte disponible.\n\nRecientemente, Microsoft ha introducido una nueva funcionalidad que permite a los usuarios escribir código Python directamente en las celdas de Excel. Esta integración permite a los usuarios aprovechar la potencia de Python para procesar datos en Excel, realizar cálculos complejos y visualizar datos mediante gráficos de matplotlib insertados directamente en la hoja de cálculo. Para usar esta funcionalidad, los usuarios pueden seleccionar una celda y, en la pestaña Fórmulas, seleccionar Insertar Python. También se puede habilitar Python en una celda escribiendo =PY y eligiendo PY en el menú de autocompletar.\n\n\n\nJulia\nJulia es un lenguaje más reciente, diseñado específicamente para el cálculo numérico y la ciencia de datos. Ofrece un rendimiento cercano al de lenguajes de bajo nivel como C, pero con la sintaxis y facilidad de uso de lenguajes de alto nivel como Python y R. Julia es especialmente útil en aplicaciones donde el rendimiento es crítico, como en simulaciones industriales y análisis de grandes volúmenes de datos.\n\nVentajas\n\nRendimiento: Diseñado específicamente para computación numérica de alto rendimiento, Julia ofrece velocidades comparables a lenguajes compilados como C o Fortran, pero con una sintaxis más cercana a los lenguajes de scripting. Es excelente para cálculos numéricos intensivos y proporciona soporte nativo para paralelismo y computación distribuida.\nSintaxis expresiva: Su sintaxis es intuitiva y similar a la de lenguajes matemáticos, lo que facilita la escritura de código conciso y legible.\nInteroperabilidad: Permite llamar a código escrito en otros lenguajes como C, Python o R, lo que facilita la integración con herramientas existentes.\nEn crecimiento: Aunque más joven que Python y R, Julia está ganando rápidamente popularidad en la comunidad científica y de datos.\n\n\nEn resumen, mientras que R sigue siendo una opción poderosa y preferida para la estadística industrial, Python y Julia se presentan como alternativas viables y, en algunos casos, superiores, dependiendo de los requisitos específicos del proyecto.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#el-concepto-de-reproducibilidad",
    "href": "005-herramientas.html#el-concepto-de-reproducibilidad",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.5 El concepto de reproducibilidad",
    "text": "3.5 El concepto de reproducibilidad\nLa reproducibilidad de un ensayo o experimento es la capacidad de ser reproducido o replicado por otros, en particular, por la comunidad científica. La reproducibilidad se refiere a la capacidad de obtener resultados consistentes al replicar un estudio o experimento utilizando los mismos datos, metodología original y, en su caso, el mismo código informático empleado para los análisis.En otras palabras, cuando se replica un análisis de datos o un experimento, los resultados deben ser alcanzados nuevamente con un alto grado de confiabilidad.\nLa repetibilidad o replicabilidad se refiere a la posibilidad de obtener resultados consistentes al replicar un estudio con un conjunto distinto de datos, pero obtenidos siguiendo el mismo diseño experimental. Implica obtener resultados consistentes utilizando nuevos datos o nuevos resultados computacionales para responder a la misma pregunta científica.\nEl químico irlandés Robert Boyle, en el siglo XVII, subrayó la importancia de la reproducibilidad en la ciencia. Boyle sostenía que los fundamentos del conocimiento debían basarse en hechos producidos experimentalmente, que pudieran volverse creíbles para la comunidad científica por su reproducibilidad. La bomba de aire de Boyle, un aparato científico complicado y costoso en ese momento, condujo a una de las primeras disputas documentadas sobre la reproducibilidad de un fenómeno científico.\n\nImportancia en la Ciencia\nLa reproducibilidad es esencial para la investigación científica, ya que permite validar y verificar los resultados obtenidos. En las últimas décadas, ha habido una creciente preocupación por la falta de reproducibilidad en muchos resultados científicos publicados, lo que ha llevado a una crisis de reproducibilidad o replicación. La reproducibilidad garantiza que los resultados científicos sean confiables y puedan ser validados por otros investigadores. Es un pilar fundamental para el avance del conocimiento en todas las disciplinas.\n\n\nReproducibilidad en metrología\nEn metrología, la reproducibilidad es la capacidad de un instrumento de dar el mismo resultado en mediciones diferentes, realizadas en las mismas condiciones a lo largo de periodos dilatados de tiempo. Esta cualidad debe evaluarse a largo plazo.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "005-herramientas.html#ventajas-de-r-frente-a-la-hoja-de-cálculo-en-la-reproducibilidad-de-un-análisis",
    "href": "005-herramientas.html#ventajas-de-r-frente-a-la-hoja-de-cálculo-en-la-reproducibilidad-de-un-análisis",
    "title": "3  Herramientas para el análisis. La reproducibilidad.",
    "section": "3.6 Ventajas de R frente a la hoja de cálculo en la reproducibilidad de un análisis",
    "text": "3.6 Ventajas de R frente a la hoja de cálculo en la reproducibilidad de un análisis\nLas ventajas de utilizar R en lugar de hojas de cálculo tradicionales (como Microsoft Excel) incluyen:\n\nCódigo abierto (scripts): R permite crear flujos de trabajo basados en código, lo que mejora la reproducibilidad de los análisis y facilita la colaboración entre investigadores. Puedes escribir scripts en R para automatizar tareas y asegurar la reproducibilidad. Los scripts son transparentes y pueden ser compartidos y verificados por otros investigadores.\nFlexibilidad estadística: R es especialmente útil para técnicas avanzadas de análisis, lo que lo convierte en una excelente opción para investigadores que buscan análisis de vanguardia.\nGráficos muy potentes y completos: Los gráficos disponibles en R son una de las fortalezas de esta herramienta, no sólo por su variedad sino también por su flexibilidad.\nCapacidad para tratar grandes cantidades de datos: R puede manejar grandes conjuntos de datos sin problemas, lo que es fundamental en la estadística industrial.\nPrecisión: R está diseñado específicamente para análisis estadístico, lo que lo hace más preciso que Excel en ciertos casos, como en análisis de regresión lineal.\nCapacidad avanzada: R ofrece una amplia gama de paquetes y funciones para realizar análisis estadísticos avanzados, como modelos lineales, series temporales, análisis multivariante y más.\n\nPor su parte, la hoja de cálculo tiene una curva de aprendizaje más sencilla y es en general más fácil de usar, pero tiene algunos inconvenientes:\n\nInexactitudes: Estudios han demostrado que Excel puede mostrar ciertas inexactitudes en análisis de regresión lineal y otros métodos estadísticos.\nLimitaciones gráficas: Los gráficos de Excel son bastante limitados cuando se trata de presentar información sobre un análisis de datos.\nLimitaciones estadísticas: Excel no está diseñado específicamente para análisis estadístico avanzado, por lo que puede carecer de algunas capacidades necesarias para investigaciones más complejas.\nFalta de transparencia para la auditoría: Las fórmulas y cálculos en Excel pueden ser difíciles de rastrear y verificar, lo que afecta la reproducibilidad.\n\nPor estas razones usaremos Excel para el almacenamiento de datos y el análisis básico, y usaremos R para el análisis gráfico más detallado y el análisis numérico y estadístico.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Herramientas para el análisis. La reproducibilidad.</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html",
    "href": "010-introduccion-a-python.html",
    "title": "4  Introducción a Python",
    "section": "",
    "text": "4.1 Cómo usar este cuaderno\nPython es un lenguaje de programación. Para trabajar con el lenguaje, se han desarrollado diferentes interfaces que mejoran el manejo del programa y facilitan el trabajo.\nNosotros utilizaremos Google Colaborate, conocido como Colaboratory o simplemente, Google Colab, basado en el entorno Jupyter, originalmente desarrollado para Python y que se ha convertido en interfase universal de muchos lenguajes de programación.\nEn este entorno, podemos escribir código y ejecutarlo. También podemos escribir en celdas de texto, lo que nos permite intercalar explicaciones a nuestros cálculos o cualquier otro contenido escrito.\nHaciendo click en la barra de opciones a la izquierda sobre la primera opción de índice podemos tener la visión de conjunto del cuaderno",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#cómo-usar-este-cuaderno",
    "href": "010-introduccion-a-python.html#cómo-usar-este-cuaderno",
    "title": "4  Introducción a Python",
    "section": "",
    "text": "Si has abierto un cuaderno ya creado, del que te han pasado un enlace, y estás abriéndolo por primera vez, sigue estos pasos:\n\nGuarda una copia en tu carpeta de Google Drive. Para ello, vete a Archivo/Guardar una copia en Drive y guárdalo.\nUna vez guardada la copia, puedes cambiar el nombre en ’Archivo/Cambiar nombre`, o bien directamente haciendo click sobre el nombre en la parte superior de la hoja.\n\nSi vas a crear una hoja nueva (en blanco) mediante el enlace a Google Colaborate, sigue estos pasos:\n\nAbre la hoja de Google Colaborate escribiendo en la barra de direcciones del navegador: colab.to\nGuarda una copia en Archivo/Guardar una copia como\nRenombra la hoja en Archivo/Cambiar nombre usando el nombre que quieras",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#primeros-pasos",
    "href": "010-introduccion-a-python.html#primeros-pasos",
    "title": "4  Introducción a Python",
    "section": "4.2 Primeros pasos",
    "text": "4.2 Primeros pasos\nEn estos ejercicios usaremos varios archivos de datos de demostración. En el primer cuaderno de introducción veremos archivos ya existentes en diversas fuentes de Python, que nos servirán para hacer diferentes ejercicios numéricos y gráficos. En el segundo cuaderno, utilizaremos exclusivamente un archivo de datos en formato CSV (camembert.csv), que contiene los datos analíticos diarios, de producto terminado, de una fabricación de un queso camembert durante un año; son datos reales.\nEn el entorno jupyter, el texto se introduce en celdas.\nHay dos tipos de celdas: texto y código. En la parte superior de la ventana, puedes elegir el tipo que quieres insertar haciendo click en + Código o bien en + Texto\nPara editar una celda de texto existente, haz doble click sobre ella. Ahora puedes escribir usando las convenciones de código markdown. Cuando hayas terminado, ejecuta la celda para que Colab formatee y presente el texto.\nPara ejecutar una celda de código, haz click sobre ella.\n\nShift-Enterejecuta el código y pasa a la siguiente celda.\nCtrl-Enterejecuta el código y permanece en la misma celda\n\nRecuerda también que las celdas de código deben ejecutarse en orden secuencial desde el principio\nSi la ejecución de una celda de código produce un error, no te preocupes, no tiene consecuencias graves. Simplemente, edita el código y corrige el error. En muchos casos, Google Gemini te puede ayudar a detectar los errores en el código y reescribirlo correctamente.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#cálculos-simples-con-python",
    "href": "010-introduccion-a-python.html#cálculos-simples-con-python",
    "title": "4  Introducción a Python",
    "section": "4.3 Cálculos simples con python",
    "text": "4.3 Cálculos simples con python\nEn su forma más básica, python se puede utilizar como una simple calculadora, utilizando los siguientes operadores aritméticos:\nAdición: \\(+\\) (Ejemplo: \\(2+2\\))\nResta: \\(-\\) (Ejemplo: \\(2-2\\))\nMultiplicación: \\(*\\) (Ejemplo: \\(2*2\\))\nDivisión: \\(/\\) (Ejemplo: \\(2/2\\))\nExponenciación: ** (Ejemplo: \\(2\\)**\\(2\\))\nPrueba a ejecutar las siguientes celdas:\n\n2+2\n\n4\n\n\n\n2-2\n\n0\n\n\n\n2*2\n\n4\n\n\n\n2/2\n\n1.0\n\n\n\n2**2\n\n4\n\n\n\n(3*4)/2\n\n6.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#creación-y-asignación-de-variables",
    "href": "010-introduccion-a-python.html#creación-y-asignación-de-variables",
    "title": "4  Introducción a Python",
    "section": "4.4 Creación y asignación de variables",
    "text": "4.4 Creación y asignación de variables\nUna variable permite almacenar un valor (por ejemplo, 4) o un objeto (por ejemplo, una descripción de función). Más tarde se puede usar el nombre de esta variable para acceder fácilmente al valor o al objeto que está almacenado dentro de ella.\nPodemos imaginar este concepto como una estantería llena de celdas vacías, en las cuales podemos colocar diferentes objetos: números, letras, frases, etc La variable es el espacio que almacena un valor, y que podemos llamar u obtener simplemente escribiendo su nombre.\nEn python, una variable es un objeto. Podemos asignar valores a ese objeto, como si colocásemos libros en nuestra estantería. Esta asignación se hace siempre con el operador de asignación =, en la forma\n\\(nombre\\_de\\_objeto = valor\\)\nPor ejemplo, asignaremos un valor 4 a una variable que se llame mi_var con el operador de asignación de la siguiente forma:\n\\(mi\\_var = 4\\)\nRecuerda que ahora estamos en una celda de texto; prueba a hacer la asignación en la celda de código a continuación, añadiendo el operador de asignación y ejecutando la celda con Mayus-Intro:\n\nmi_var 4\n\nUna vez que asignamos un valor a una variable, python recuerda su valor hasta que lo cambiemos mediante una nueva asignación, borremos la variable, o finalicemos nuestra sesión.\nPara ver el contenido del objeto(lo que hemos almacenado en mi_var), escribimos el nombre y ejecutamos la celda (sólo si hemos editado y ejecutado correctamente la celda anterior, en caso contrario obtendremos un error porque mi_var no está definida):\n\nmi_var",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#cómo-establecer-los-nombres-de-variables",
    "href": "010-introduccion-a-python.html#cómo-establecer-los-nombres-de-variables",
    "title": "4  Introducción a Python",
    "section": "4.5 Cómo establecer los nombres de variables",
    "text": "4.5 Cómo establecer los nombres de variables\nPara facilitar la legibilidad del código, los nombres de variables en python deben cumplir algunas normas generales. Buscaremos siempre que los nombres de los objetos sean descriptivos, por lo que necesitamos una forma de unir varias palabras.\nLas convenciones más aceptadas para la designación de nombres de variables en Python se basan principalmente en la PEP 8, la guía de estilo oficial para el código Python. Aquí te resumo los puntos clave:\n\nSnake Case (Recomendado para variables y funciones):\n\nPara nombres de variables y funciones que constan de varias palabras, se utiliza snake_case, donde las palabras se escriben en minúscula y se separan por guiones bajos.\nEjemplos: mi_variable, nombre_completo, calcular_total.\n\nCamel Case (Para clases):\n\nPara nombres de clases, se utiliza CamelCase (también conocido como PascalCase), donde cada palabra comienza con mayúscula y no hay separadores.\nEjemplos: MiClase, GestionDeUsuarios, HttpRequest.\n\nConstantes (Mayúsculas con guiones bajos):\n\nPara constantes (variables cuyo valor no debería cambiar durante la ejecución del programa), se utiliza ALL_CAPS con guiones bajos para separar las palabras.\nEjemplos: MAX_VALOR, PI, TASA_INTERES.\n\nReglas generales (obligatorias por el lenguaje):\n\nLongitud: Las variables pueden tener cualquier longitud.\nCaracteres permitidos: Pueden contener letras (mayúsculas y minúsculas), números y guiones bajos (_).\nNo pueden empezar con un número: Un nombre de variable no puede comenzar con un dígito.\nSensibilidad a mayúsculas y minúsculas: Python distingue entre mayúsculas y minúsculas. mi_variable y Mi_Variable son nombres de variables diferentes.\nEvitar palabras reservadas: No se pueden usar palabras reservadas de Python (como if, else, for, while, etc.) como nombres de variables.\nNo usar caracteres especiales: No se permiten espacios, signos de puntuación u otros símbolos especiales.\n\nRecomendaciones de estilo:\n\nNombres descriptivos: Elige nombres de variables que sean claros y descriptivos, que reflejen su propósito en el programa. Esto mejora la legibilidad del código.\nEvitar nombres de una sola letra: A menos que sean contadores simples (i, j, k en bucles) o iteradores, es mejor evitar nombres de variables de una sola letra.\nEvitar nombres que se confundan: Ten cuidado con letras que pueden confundirse fácilmente, como la l minúscula y el número 1, o la O mayúscula y el número 0.\nGuiones bajos al principio (_nombre): Un solo guion bajo al principio (_) suele indicar que una variable o función es “interna” o “no pública” dentro de un módulo o clase. Es una convención, no una restricción estricta.\nDobles guiones bajos al principio (__nombre): Los dobles guiones bajos al principio de un nombre de atributo de clase (__nombre) activan el “name mangling” (mutilación de nombre) de Python, que hace que el atributo sea más difícil de acceder directamente desde fuera de la clase, actuando como una forma de “privacidad”.\n\n\nSeguir estas convenciones, especialmente las de la PEP 8, ayuda a que tu código sea más consistente, legible y fácil de entender por otros desarrolladores (y por ti mismo en el futuro).\nUsemos un nombre realmente largo para designar a una variable:\n\neste_es_un_nombre_realmente_largo = 5.3\n\nAhora recuperemos el valor que hemos almacenado\n\neste_es_un_nombre_realmente_largo\n\n5.3\n\n\nRecuerda que python utiliza la escritura anglosajona para la separación de decimales, usando un punto, y no una coma como en España. ¿Qué pasaría si utilizamos una coma para separar los decimales?\n\nmi_var = 2,5\n\nPython acepta la asignación, pero cuando investigamos la variable, el resultado es diferente de lo que esperábamos, no obtenemos un valor numérico:\n\nmi_var\n\n(2, 5)\n\n\nPython ha entendido que estábamos creando una tupla de dos valores (2 y 5) (en python, una tupla es una colección ordenada e inmutable de elementos)\nPara python las mayúsculas y minúsculas son diferentes: mi_var y mi_Var son variables diferentes.\nEn una variable python podemos almacenar también texto:\n\nmi_var = \"Esto es una frase 12345\"\n\nPython almacena esta cadena alfanumérica exactamente igual que antes hizo con los valores numéricos:\n\nmi_var\n\n'Esto es una frase 12345'\n\n\n\nmi_var = 123.45\n\n\nmi_var\n\n123.45\n\n\n\nmi_var*5\n\n617.25\n\n\nPara almacenar una cadena alfanumérica necesitamos encerrarla entre comillas, ya sean sencillas o dobles. De hecho, si almacenamos un número entre comillas, python no va aidentificarlo como un número, sino como un texto:\n\nmi_var = \"123.45\"\n\n\nmi_var\n\n'123.45'\n\n\n\nmi_var*5\n\n'123.45123.45123.45123.45123.45'\n\n\nComo ahora mi_var es una cadena de letras, python interpreta mi_var*5no como una multiplicación, sino como una instrucción para repetir esa cadena de caracteres cinco veces.\nEn el ejemplo que acabamos de hacer, seguro que te has dado cuenta de que python permite reutilizar los objetos simplemente reasignándoles el valor correspondiente. Al hacerlo, perdemos el valor original y lo sustituimos por el nuevo valor.\nHay que tener atención con las reglas de escritura: para asignar un texto a un objeto debemos tener cuidado de cerrar las comillas, si no lo hacemos, python nos advertirá del error:\n\nmi_var = \"Esto es una cadena alfanumerica que no hemos cerrado\n\n\nAlgunos ejercicios\nIntenta detectar los errores en las siguientes celdas de código:\n\nmi_variable = 10\nmi_varıable\n\n\nmi_var = 5\nMi_Var\n\nPodemos asignar un objeto a otro objeto con el operador de asignación:\n\nx = 5\ny = 3\n\n\nx + y\n\n8\n\n\n\ny = x\n\n\nx + y\n\n10\n\n\nSupongamos que tenemos 5 peras y 4 manzanas. Crea una variable que se llame fruta que sume el total de unidades de fruta que tenemos, insertando debajo de esta celda todas las celdas de código que necesites (Pista: ¡En python sí podemos sumar peras con manzanas!)\n\n# Celda para el ejercicio de las frutas\nperas = 5\nmanzanas = 4\nfruta = peras + manzanas\nprint(fruta)\n\n9",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#los-tipos-de-datos",
    "href": "010-introduccion-a-python.html#los-tipos-de-datos",
    "title": "4  Introducción a Python",
    "section": "4.6 Los tipos de datos",
    "text": "4.6 Los tipos de datos\nHemos visto que python puede trabajar con diferentes tipos de datos, tales como números y textos.\nLos valores decimales como 4.5 se llaman flotantes(float). Los números enteros como 4 se llaman enteros(ìnt). Los valores booleanos (VERDADERO o FALSO) se denominan booleanos(bool o logical). Los valores de texto (o cadena alfanumérica) se denominan cadenas de caracteres. También se les llama simplemente cadenas. Las comillas en el editor indican que “un texto entre comillas” es una cadena de caracteres. En python, una cadena de caracteres puede escribirse entre comillas dobles, como \"pera\" o simples, como 'pera'\nAlgunos ejemplos:\n\nmi_var_numero = 4.5\nmi_var_texto = \"esto es un texto\"\nmi_var_texto_2 = 'esto también es un texto'\nmi_var_logica = False\n\nLas variables lógicas pueden tomar los valores True o False (recuerda que las mayúsculas son significativas en python: False no es lo mismo que false o FALSE). Python responde también con un valor lógico cuando hacemos una evaluación lógica. Por ejemplo,\n\nmi_var_numero == 4.5\n\nTrue\n\n\n\nmi_var_texto == 4.5\n\nFalse\n\n\nComo en python se usa = como operador de asignación, el operador lógico que prueba una igualdad es ==, un doble igual, y no un =, un igual sencillo. La desigualdad se verifica con el operador != que significa “no es igual a”.\nVeamos otros dos ejemplos que proporcionan una respuesta de valor lógico:\n\nmi_var_numero == 10\n\nFalse\n\n\n\nmi_var_numero &gt; 5\n\nFalse\n\n\n\nmi_var_numero != 5 # el operador != significa \"no es igual a\"\n\nTrue",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#las-funciones-en-python-qué-son-y-por-qué-las-usamos",
    "href": "010-introduccion-a-python.html#las-funciones-en-python-qué-son-y-por-qué-las-usamos",
    "title": "4  Introducción a Python",
    "section": "4.7 Las Funciones en Python: ¿Qué son y por qué las usamos?",
    "text": "4.7 Las Funciones en Python: ¿Qué son y por qué las usamos?\nHasta ahora hemos usado operadores aritméticos para realizar cálculos básicos y hemos asignado valores a variables. Pero, ¿qué pasa si queremos realizar tareas más complejas o repetir una misma operación muchas veces? Aquí es donde entran las funciones.\nUna función es un bloque de código organizado y reutilizable que se utiliza para realizar una única acción relacionada. Piensa en una función como una “receta” o un “mini-programa” que encapsula una serie de pasos. En lugar de escribir esos pasos una y otra vez, simplemente “llamamos” a la función por su nombre.\n¿Por qué son útiles las funciones?\n\nReutilización de Código: Evitan tener que escribir el mismo código varias veces.\nModularidad: Dividen el programa en partes más pequeñas y manejables.\nClaridad: Hacen el código más fácil de leer y entender.\n\nPython ya trae consigo muchas funciones integradas (built-in functions) que podemos usar directamente. Veamos algunos ejemplos muy comunes:\n\nprint(): Mostrar información\nEsta función ya la hemos usado. Nos permite mostrar valores, texto o el contenido de variables en la consola\n\nprint(\"¡Hola, esto es una función!\")\nmi_nombre = \"Juan\"\nprint(\"Mi nombre es:\", mi_nombre)\n\n¡Hola, esto es una función!\nMi nombre es: Juan\n\n\n\n\nlen(): Obtener la longitud\nEsta función nos devuelve el número de elementos de un objeto, como el número de caracteres en una cadena de texto o el número de elementos en una lista.\n\nfrase = \"Programar en Python es divertido\"\nlongitud_frase = len(frase)\nprint(\"La frase tiene\", longitud_frase, \"caracteres.\")\n\nmi_lista_numeros = [10, 20, 30, 40]\nprint(\"Mi lista tiene\", len(mi_lista_numeros), \"elementos.\")\n\nLa frase tiene 32 caracteres.\nMi lista tiene 4 elementos.\n\n\n\n\ntype(): Conocer el tipo de dato\nÚtil para verificar qué tipo de dato tiene una variable.\n\nmi_numero = 100\nmi_texto = \"Cien\"\nmi_booleano = True\n\nprint(type(mi_numero))\nprint(type(mi_texto))\nprint(type(mi_booleano))\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n&lt;class 'bool'&gt;\n\n\n\n\n¿Cómo funcionan las funciones? Argumentos y Retorno\n\nArgumentos: Muchas funciones necesitan información para poder trabajar. Esta información se les pasa entre paréntesis, y se llaman argumentos. Por ejemplo, a print() le pasamos lo que queremos mostrar.\nRetorno de Valores: Algunas funciones realizan un cálculo o una operación y nos devuelven un resultado. Por ejemplo, len() nos devuelve un número.\n\nEntender las funciones es clave, porque las librerías que usaremos a continuación son, en esencia, grandes colecciones de funciones especializadas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#importando-herramientas-bibliotecas-y-módulos",
    "href": "010-introduccion-a-python.html#importando-herramientas-bibliotecas-y-módulos",
    "title": "4  Introducción a Python",
    "section": "4.8 Importando Herramientas: Bibliotecas y Módulos",
    "text": "4.8 Importando Herramientas: Bibliotecas y Módulos\nPython es un lenguaje muy potente, pero su verdadera fuerza reside en su vasto ecosistema de bibliotecas (también llamadas módulos o paquetes).\nUna biblioteca es una colección de código (funciones, clases, etc.) escrito por otros programadores y disponible para que tú lo uses en tus propios proyectos. Imagina que son “cajas de herramientas” especializadas:\n\n¿Necesitas hacer cálculos matemáticos complejos? Hay una librería para eso.\n¿Necesitas manipular datos en tablas? Hay una librería para eso.\n¿Necesitas crear gráficos? ¡También hay librerías para eso!\n\nUsar librerías nos permite evitar “reinventar la rueda”, ahorrando mucho tiempo y esfuerzo, y aprovechando código que ya ha sido probado y optimizado.\nPara poder usar las funciones y herramientas que están dentro de una librería, primero debemos importarla a nuestro entorno de trabajo. La forma más común de hacerlo es con la instrucción import.\n\nSintaxis de import\nLa sintaxis básica es:\nimport nombre_de_la_libreria\nSin embargo, para las librerías de análisis de datos, es muy común y recomendado usar un alias (un nombre corto) para hacer el código más conciso y fácil de leer. Esto se hace con la palabra clave as:\nimport nombre_de_la_libreria as alias\nAhora vamos a importar las librerías que serán esenciales para trabajar con datos, realizar cálculos estadísticos y crear gráficos en Python.\n\n# Importando las librerías fundamentales para el análisis de datos\n\n# Pandas: Es la librería principal para trabajar con estructuras de datos tabulares\n#         como Series (vectores) y DataFrames (tablas).\nimport pandas as pd\n\n# NumPy: Proporciona soporte para arreglos (arrays) y matrices multidimensionales,\n#        además de funciones matemáticas de alto nivel. Pandas se construye sobre NumPy.\nimport numpy as np\n\n# Matplotlib.pyplot: Es la librería base para crear gráficos en Python.\n#                    Seaborn la usa \"por debajo\" para dibujar.\nimport matplotlib.pyplot as plt\n\n# Seaborn: Es una librería de visualización de datos de alto nivel\n#          basada en Matplotlib. Facilita la creación de gráficos estadísticos atractivos.\nimport seaborn as sns\n\nprint(\"¡Librerías principales para el análisis de datos importadas correctamente!\")\nprint(\"Ahora podemos acceder a sus funciones usando sus alias (pd, np, plt, sns).\")\n\n¡Librerías principales para el análisis de datos importadas correctamente!\nAhora podemos acceder a sus funciones usando sus alias (pd, np, plt, sns).\n\n\nUna vez que hemos importado una librería con un alias (por ejemplo, pandas as pd), para usar una función de esa librería, escribimos el alias seguido de un punto y el nombre de la función: pd.nombre_de_la_funcion().",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#las-principales-estructuras-de-datos-en-python-para-el-análisis",
    "href": "010-introduccion-a-python.html#las-principales-estructuras-de-datos-en-python-para-el-análisis",
    "title": "4  Introducción a Python",
    "section": "4.9 Las principales estructuras de datos en Python para el análisis",
    "text": "4.9 Las principales estructuras de datos en Python para el análisis\nAhora que entendemos las funciones y cómo importar librerías, podemos introducir las estructuras de datos que nos permitirán organizar y manipular nuestros datos de manera eficiente. Las dos más importantes en el contexto del análisis de datos con Python (y con la librería Pandas) son las Series y los DataFrames.\n\nSeries de pandas (El “vector” en Python para datos)\nSi en otros lenguajes de programación o herramientas estadísticas has trabajado con el concepto de “vector” (una secuencia de valores de un mismo tipo), el equivalente más directo en Python, usando pandas, es una serie.\n\nConcepto: Una serie de pandas es un arreglo unidimensional (como una columna de una hoja de cálculo o un vector). Puede contener cualquier tipo de dato (números enteros, flotantes, texto, booleanos, etc.) y tiene un índice asociado para acceder a sus elementos.\n\nCreación de una serie: Podemos crear una Serie a partir de una lista de Python.\n\n# Crear una lista de edades\nlista_edades = [25, 30, 22, 35, 28, 40, 33, 29]\n\n# Convertir la lista a una Serie de Pandas\n# Usamos 'pd.' porque la función Series() viene de la librería Pandas. Atención: la función se escribe con mayúsculas.\nedades_serie = pd.Series(lista_edades)\n\nprint(\"Mi Serie de Edades:\")\nprint(edades_serie)\n\nMi Serie de Edades:\n0    25\n1    30\n2    22\n3    35\n4    28\n5    40\n6    33\n7    29\ndtype: int64\n\n\nOperaciones básicas con series: Las series tienen muchas funciones y métodos útiles. Por ejemplo, podemos calcular la media:\n\nprint(\"\\nEdad promedio:\", edades_serie.mean()) # Usando el método .mean() de la Serie\nprint(\"Edad máxima:\", edades_serie.max())\nprint(\"Edad mínima:\", edades_serie.min())\n\n\nEdad promedio: 30.25\nEdad máxima: 40\nEdad mínima: 22\n\n\n\n\nDataFrames de Pandas (La “Tabla” o “Hoja de Cálculo”)\nEl DataFrame es la estructura de datos más importante y utilizada en pandas para el análisis de datos. Es el equivalente a una tabla en una base de datos, una hoja de cálculo de Excel, o un “data frame” en R.\n\nConcepto: Un DataFrame es una estructura bidimensional (filas y columnas). Piensa en ella como una colección de series (columnas) que comparten el mismo índice (las filas). Cada columna en un DataFrame es una serie de pandas.\n\nCreación de un DataFrame (a partir de un diccionario): Una forma sencilla de crear un DataFrame pequeño es a partir de un diccionario de Python, donde las claves del diccionario se convierten en los nombres de las columnas y los valores son listas que se convierten en las columnas.\n\n# Crear un diccionario con datos\ndatos_alumnos = {\n    'Nombre': ['Ana', 'Luis', 'Marta', 'Pedro', 'Sofía'],\n    'Edad': [24, 30, 28, 35, 26],\n    'Ciudad': ['Madrid', 'Barcelona', 'Valencia', 'Sevilla', 'Málaga'],\n    'Calificación': [8.5, 7.2, 9.1, 6.8, 8.9]\n}\n\n# Crear un DataFrame a partir del diccionario\ndf_alumnos = pd.DataFrame(datos_alumnos)\n\nprint(\"\\nDataFrame de Alumnos:\")\nprint(df_alumnos)\n\n\nDataFrame de Alumnos:\n  Nombre  Edad     Ciudad  Calificación\n0    Ana    24     Madrid           8.5\n1   Luis    30  Barcelona           7.2\n2  Marta    28   Valencia           9.1\n3  Pedro    35    Sevilla           6.8\n4  Sofía    26     Málaga           8.9\n\n\nVisualización y exploración básica de DataFrames:\nEs fundamental poder echar un vistazo rápido a los datos una vez que están cargados en un DataFrame.\n\ndf.head(): Muestra las primeras 5 filas (útil para ver la estructura).\ndf.tail(): Muestra las últimas 5 filas.\ndf.info(): Proporciona un resumen conciso del DataFrame, incluyendo el número de entradas, columnas, tipos de datos no nulos y uso de memoria.\ndf.describe(): Genera estadísticas descriptivas (conteo, media, desviación estándar, etc.) de las columnas numéricas.\n\n\nprint(\"\\nPrimeras 3 filas del DataFrame de alumnos:\")\nprint(df_alumnos.head(3)) # Podemos especificar el número de filas\n\nprint(\"\\nInformación del DataFrame de alumnos:\")\ndf_alumnos.info()\n\nprint(\"\\nEstadísticas descriptivas del DataFrame de alumnos:\")\nprint(df_alumnos.describe())\n\n\nPrimeras 3 filas del DataFrame de alumnos:\n  Nombre  Edad     Ciudad  Calificación\n0    Ana    24     Madrid           8.5\n1   Luis    30  Barcelona           7.2\n2  Marta    28   Valencia           9.1\n\nInformación del DataFrame de alumnos:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 5 entries, 0 to 4\nData columns (total 4 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   Nombre        5 non-null      object \n 1   Edad          5 non-null      int64  \n 2   Ciudad        5 non-null      object \n 3   Calificación  5 non-null      float64\ndtypes: float64(1), int64(1), object(2)\nmemory usage: 292.0+ bytes\n\nEstadísticas descriptivas del DataFrame de alumnos:\n            Edad  Calificación\ncount   5.000000      5.000000\nmean   28.600000      8.100000\nstd     4.219005      1.036822\nmin    24.000000      6.800000\n25%    26.000000      7.200000\n50%    28.000000      8.500000\n75%    30.000000      8.900000\nmax    35.000000      9.100000\n\n\nSelección de Columnas en un DataFrame:\nPara trabajar con una columna específica de un DataFrame, la seleccionamos usando corchetes [] y el nombre de la columna entre comillas. El resultado será una Serie de Pandas.\n\n# Seleccionar la columna 'Edad'\nedades_alumnos = df_alumnos['Edad']\nprint(\"\\nColumna 'Edad' (como Serie):\")\nprint(edades_alumnos)\n\n# Calcular la media de la columna 'Calificación'\nmedia_calificacion = df_alumnos['Calificación'].mean()\nprint(f\"\\nLa calificación promedio de los alumnos es: {media_calificacion:.2f}\")\n\n\nColumna 'Edad' (como Serie):\n0    24\n1    30\n2    28\n3    35\n4    26\nName: Edad, dtype: int64\n\nLa calificación promedio de los alumnos es: 8.10",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#el-data-frame-continuación-con-ejemplo-real",
    "href": "010-introduccion-a-python.html#el-data-frame-continuación-con-ejemplo-real",
    "title": "4  Introducción a Python",
    "section": "4.10 El data frame (continuación con ejemplo real)",
    "text": "4.10 El data frame (continuación con ejemplo real)\nAhora que conocemos los DataFrames, podemos entender mejor cómo leer datos reales. El marco de datos o data frame es el objeto más útil y más usado en el análisis de datos. Consiste en una estructura de dos dimensiones, formada por una serie de vectores de igual longitud, igual que una tabla de una hoja de cálculo, en la que cada columna es una variable y cada fila, un caso, observación o individuo. Algunas bibliotecas Python incluyen data frames de muestra, que son útiles para entender cómo están formados.\nVeamos uno de ellos, el famoso conjunto de datos iris, creada por el investigador y estadístico Ronald Fisher, que contiene un conjunto de medidas realizadas sobre flores del género Iris realizadas por este investigador en los años 30 del siglo XX.\nPara cargar este conjunto de datos de ejemplo, vamos a utilizar una función de la librería seaborn que ya importamos:\n\n# Cargamos el dataset 'iris' usando una función de Seaborn\ndf_iris_sns = sns.load_dataset('iris')\n\nprint(\"Primeras 5 filas del dataset Iris:\")\nprint(df_iris_sns.head())\n\nPrimeras 5 filas del dataset Iris:\n   sepal_length  sepal_width  petal_length  petal_width species\n0           5.1          3.5           1.4          0.2  setosa\n1           4.9          3.0           1.4          0.2  setosa\n2           4.7          3.2           1.3          0.2  setosa\n3           4.6          3.1           1.5          0.2  setosa\n4           5.0          3.6           1.4          0.2  setosa\n\n\nLa función head() es útil para presentarnos sólo el encabezado del data frame. Veamos la estructura de este data frame más a fondo:\n\nprint(\"\\nInformación general del DataFrame Iris:\")\ndf_iris_sns.info()\n\n\nInformación general del DataFrame Iris:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\n\nprint(\"\\nEstadísticas descriptivas de las columnas numéricas del DataFrame Iris:\")\nprint(df_iris_sns.describe())\n\n\nEstadísticas descriptivas de las columnas numéricas del DataFrame Iris:\n       sepal_length  sepal_width  petal_length  petal_width\ncount    150.000000   150.000000    150.000000   150.000000\nmean       5.843333     3.057333      3.758000     1.199333\nstd        0.828066     0.435866      1.765298     0.762238\nmin        4.300000     2.000000      1.000000     0.100000\n25%        5.100000     2.800000      1.600000     0.300000\n50%        5.800000     3.000000      4.350000     1.300000\n75%        6.400000     3.300000      5.100000     1.800000\nmax        7.900000     4.400000      6.900000     2.500000\n\n\nPara utilizar una columna de un dataframe, la seleccionamos por su nombre:\n\nprint(\"\\nContenido de la columna 'sepal_length':\")\nprint(df_iris_sns[\"sepal_length\"]) # podemos usar también la doble comilla para designar las columnas\n\n\nContenido de la columna 'sepal_length':\n0      5.1\n1      4.9\n2      4.7\n3      4.6\n4      5.0\n      ... \n145    6.7\n146    6.3\n147    6.5\n148    6.2\n149    5.9\nName: sepal_length, Length: 150, dtype: float64\n\n\nAhora que tenemos una columna seleccionada (que es una serie de pandas), podemos aplicar directamente funciones estadísticas sobre ella:\n\n# Calculamos la media de la columna 'sepal_length'\nmedia_sepal_length = df_iris_sns[\"sepal_length\"].mean()\nprint(f\"\\nLa longitud media del sépalo es: {media_sepal_length:.2f}\") # esta es una forma de dar formato al número para imprimirlo con dos decimales fijos\n\n\nLa longitud media del sépalo es: 5.84\n\n\n\n# También podemos calcular la desviación estándar\ndesviacion_sepal_length = df_iris_sns[\"sepal_length\"].std()\nprint(f\"La desviación estándar de la longitud del sépalo es: {desviacion_sepal_length:.2f}\")\n\nLa desviación estándar de la longitud del sépalo es: 0.83\n\n\nCon nuestros DataFrames cargados y la comprensión de las Series, podemos realizar rápidamente cálculos estadísticos básicos sobre nuestras columnas numéricas. Pandas hace que esto sea muy sencillo.\n\nEstadísticas Descriptivas Comunes:\n\n.mean(): Media\n.median(): Mediana\n.std(): Desviación estándar\n.min(): Valor mínimo\n.max(): Valor máximo\n.sum(): Suma de todos los valores\n.count(): Número de valores no nulos\n\n\nVamos a aplicar algunas de estas operaciones al DataFrame df_iris_sns:\n\nprint(\"Estadísticas de 'petal_width' en el DataFrame Iris:\")\nprint(f\"Media de petal_width: {df_iris_sns['petal_width'].mean():.2f}\")\nprint(f\"Mediana de petal_width: {df_iris_sns['petal_width'].median():.2f}\")\nprint(f\"Desviación estándar de petal_width: {df_iris_sns['petal_width'].std():.2f}\")\nprint(f\"Valor mínimo de petal_width: {df_iris_sns['petal_width'].min():.2f}\")\nprint(f\"Valor máximo de petal_width: {df_iris_sns['petal_width'].max():.2f}\")\nprint(f\"Número de observaciones de petal_width: {df_iris_sns['petal_width'].count()}\")\n\nEstadísticas de 'petal_width' en el DataFrame Iris:\nMedia de petal_width: 1.20\nMediana de petal_width: 1.30\nDesviación estándar de petal_width: 0.76\nValor mínimo de petal_width: 0.10\nValor máximo de petal_width: 2.50\nNúmero de observaciones de petal_width: 150",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#visualización-de-datos-con-seaborn-y-matplotlib",
    "href": "010-introduccion-a-python.html#visualización-de-datos-con-seaborn-y-matplotlib",
    "title": "4  Introducción a Python",
    "section": "4.11 Visualización de Datos con Seaborn y Matplotlib",
    "text": "4.11 Visualización de Datos con Seaborn y Matplotlib\nUna imagen vale más que mil palabras, y en el análisis de datos, la visualización es fundamental para entender patrones, distribuciones y relaciones en nuestros datos. En Python, las librerías Matplotlib y Seaborn son las herramientas estándar para crear gráficos.\n\nMatplotlib.pyplot (alias plt): Es la base, una librería de bajo nivel que da un control muy granular sobre cada aspecto del gráfico.\nSeaborn (alias sns): Construye sobre Matplotlib, ofreciendo una interfaz de alto nivel para crear gráficos estadísticos complejos y estéticamente agradables con menos código. Es ideal para explorar distribuciones, relaciones entre variables y comparaciones entre grupos.\n\nSiempre importamos ambas, ya que Seaborn a menudo usa funciones de Matplotlib para mostrar y personalizar los gráficos (como plt.title() para el título o plt.show() para mostrar el gráfico).\n\nEjemplo 1: Histograma de la longitud del sépalo (Distribución de una variable numérica)\nUn histograma nos permite ver cómo se distribuyen los valores de una variable numérica.\n\n# Crear un histograma de la columna 'sepal_length'\nplt.figure(figsize=(8, 5)) # Define el tamaño de la figura (ancho, alto)\nsns.histplot(data=df_iris_sns, x='sepal_length', kde=True) # kde=True añade una estimación de densidad\nplt.title('Distribución de la Longitud del Sépalo') # Título del gráfico\nplt.xlabel('Longitud del Sépalo (cm)') # Etiqueta del eje X\nplt.ylabel('Frecuencia') # Etiqueta del eje Y\nplt.grid(True, linestyle='--', alpha=0.7) # Añadir una cuadrícula opcional\nplt.show() # Muestra el gráfico\n\n\n\n\n\n\n\n\n\n\nEjemplo 2: Diagrama de dispersión de la longitud vs. ancho del sépalo (Relación entre dos variables numéricas)\nUn diagrama de dispersión es excelente para visualizar la relación entre dos variables numéricas. Podemos añadir un color (hue) para diferenciar por una tercera variable categórica (como la especie de la flor).\n\n# Crear un diagrama de dispersión de sepal_length vs. sepal_width\nplt.figure(figsize=(9, 6))\nsns.scatterplot(data=df_iris_sns, x='sepal_length', y='sepal_width', hue='species', s=80, alpha=0.7)\nplt.title('Longitud vs. Ancho del Sépalo por Especie de Iris')\nplt.xlabel('Longitud del Sépalo (cm)')\nplt.ylabel('Ancho del Sépalo (cm)')\nplt.legend(title='Especie') # Mostrar leyenda para la especie\nplt.grid(True, linestyle='--', alpha=0.7)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEjemplo 3: Gráfico de cajas (boxplot) de la longitud del pétalo por especie (Comparación de una numérica por una categórica)\nLos boxplots son útiles para visualizar la distribución de una variable numérica para diferentes categorías.\n\n# Crear un box plot de petal_length agrupado por 'species'\nplt.figure(figsize=(9, 6))\nsns.boxplot(data=df_iris_sns, x='species', y='petal_length')\nplt.title('Distribución de la Longitud del Pétalo por Especie')\nplt.xlabel('Especie de Iris')\nplt.ylabel('Longitud del Pétalo (cm)')\nplt.grid(axis='y', linestyle='--', alpha=0.7) # Cuadrícula solo en el eje Y\nplt.show()\n\n\n\n\n\n\n\n\n\n\nEjemplo 4: Gráfico de series utilizando pandas.\nUsa la biblioteca pandas y el dataset air_quality que tenemos que bajar del sitio oficial de pandas\n\n# Cargar el dataset desde URL oficial de pandas\nurl = \"https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/air_quality_no2.csv\"\ndf = pd.read_csv(url, parse_dates=[\"datetime\"])\n\n# Establecer fecha como índice\ndf.set_index(\"datetime\", inplace=True)\nprint(df.head())\n\n                     station_antwerp  station_paris  station_london\ndatetime                                                           \n2019-05-07 02:00:00              NaN            NaN            23.0\n2019-05-07 03:00:00             50.5           25.0            19.0\n2019-05-07 04:00:00             45.0           27.7            19.0\n2019-05-07 05:00:00              NaN           50.4            16.0\n2019-05-07 06:00:00              NaN           61.9             NaN\n\n\n\nprint(df.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 1035 entries, 2019-05-07 02:00:00 to 2019-06-21 02:00:00\nData columns (total 3 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   station_antwerp  95 non-null     float64\n 1   station_paris    1004 non-null   float64\n 2   station_london   969 non-null    float64\ndtypes: float64(3)\nmemory usage: 32.3 KB\nNone\n\n\n\ndf.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\ndf.station_paris.plot()\nplt.show()\n\n\n\n\n\n\n\n\nVamos a probar con un dataset diferente, el set flights incluido en seaborn, que proporciona los pasajeros por mes en un aeropuerto, durante varios años.\n\n# Cargar dataset\ndf = sns.load_dataset('flights') # load_dataset() es una función de seaborn\ndf.head()\n\n\n\n\n\n\n\n\nyear\nmonth\npassengers\n\n\n\n\n0\n1949\nJan\n112\n\n\n1\n1949\nFeb\n118\n\n\n2\n1949\nMar\n132\n\n\n3\n1949\nApr\n129\n\n\n4\n1949\nMay\n121\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 144 entries, 0 to 143\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype   \n---  ------      --------------  -----   \n 0   year        144 non-null    int64   \n 1   month       144 non-null    category\n 2   passengers  144 non-null    int64   \ndtypes: category(1), int64(2)\nmemory usage: 2.9 KB\n\n\nComo la columna month está en inglés y tiene formato category, vamos a crear una nueva columna fecha con formato DateTime, que además convertiremos en index, de manera que pandas pueda representar las series temporales correctamente:\n\ndf['fecha'] = pd.to_datetime(df['year'].astype(str) + '-' + df['month'].astype(str), format='%Y-%b')\n\n¿Qué hace esta línea?\nCrea una nueva columna llamada fecha en el DataFrame df, combinando las columnas year y month para formar una fecha completa, y luego la convierte al tipo datetime64[ns], que es el formato estándar de fechas en pandas.\n🔍 Paso a paso\n\ndf['year'].astype(str) Convierte la columna year (numérica) a texto. Ejemplo: 1949 → \"1949\"\ndf['month'].astype(str) Convierte la columna month (tipo category) a texto. Ejemplo: \"Jan\" → \"Jan\"\ndf['year'].astype(str) + '-' + df['month'].astype(str) Concatena las dos columnas con un guion. Resultado: \"1949-Jan\"\npd.to_datetime(..., format='%Y-%b') Convierte esa cadena a una fecha usando el formato especificado:\n\n\n%Y → año con cuatro cifras (1949)\n%b → mes abreviado en inglés (Jan, Feb, Mar, etc.)\n\nVeamos ahora la columna que acabamos de crear:\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nmonth\npassengers\nfecha\n\n\n\n\n0\n1949\nJan\n112\n1949-01-01\n\n\n1\n1949\nFeb\n118\n1949-02-01\n\n\n2\n1949\nMar\n132\n1949-03-01\n\n\n3\n1949\nApr\n129\n1949-04-01\n\n\n4\n1949\nMay\n121\n1949-05-01\n\n\n\n\n\n\n\n\ndf['fecha_index'] = df['fecha']\n\n# Establecer 'fecha_index' como índice\ndf.set_index('fecha_index', inplace=True)\n\n# Verificar resultado\ndf.head()\n\n\n\n\n\n\n\n\nyear\nmonth\npassengers\nfecha\n\n\nfecha_index\n\n\n\n\n\n\n\n\n1949-01-01\n1949\nJan\n112\n1949-01-01\n\n\n1949-02-01\n1949\nFeb\n118\n1949-02-01\n\n\n1949-03-01\n1949\nMar\n132\n1949-03-01\n\n\n1949-04-01\n1949\nApr\n129\n1949-04-01\n\n\n1949-05-01\n1949\nMay\n121\n1949-05-01\n\n\n\n\n\n\n\nAhora ya podemos hacer un gráfico de series, y pandas colocará automáticamente las fechas en el eje X usando la columna index fecha_index:\n\ndf['passengers'].plot()\nplt.show()\n\n\n\n\n\n\n\n\nVamos a rotular el gráfico para mejorar la legibilidad\n\ndf['passengers'].plot(title='Pasajeros mensuales en vuelos comerciales', figsize=(10, 4))\nplt.ylabel('Número de pasajeros')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\ndf['media_movil'] = df['passengers'].rolling(window=12).mean()\n\ndf[['passengers', 'media_movil']].plot(title='Pasajeros y media móvil (12 meses)', figsize=(10, 4))\nplt.ylabel('Pasajeros')\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "010-introduccion-a-python.html#resumen",
    "href": "010-introduccion-a-python.html#resumen",
    "title": "4  Introducción a Python",
    "section": "4.12 Resumen",
    "text": "4.12 Resumen\nEn este capítulo hemos tomado contacto con las funciones básicas de Python y su utilización a través de un interface Jupyter, en este caso Google Colaborate. Hemos visto los principales tipos de datos, particularmente el data frame, y hemos aprendido a leer datos que previamente se habían introducido en una hoja de cálculo. Ya estamos listos para empezar a utilizar python en el análisis de nuestros datos de producción.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Introducción a Python</span>"
    ]
  },
  {
    "objectID": "020-intro-colab.html",
    "href": "020-intro-colab.html",
    "title": "5  Trabajando con Google Colaborate",
    "section": "",
    "text": "5.1 Cómo leer una hoja de cálculo en Google Colaborate\nEn este ejercicio usaremos un fichero o archivo de datos en formato CSV que habremos exportado previamente desde nuestra hoja de cálculo. Más adelante aprenderemos cómo leer directamente los datos de nuestras hojas Excel.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Trabajando con Google Colaborate</span>"
    ]
  },
  {
    "objectID": "020-intro-colab.html#cómo-leer-una-hoja-de-cálculo-en-google-colaborate",
    "href": "020-intro-colab.html#cómo-leer-una-hoja-de-cálculo-en-google-colaborate",
    "title": "5  Trabajando con Google Colaborate",
    "section": "",
    "text": "Opción 1: Subir los datos al espacio de trabajo de Google Colaborate\nLos pasos son los mismos tanto si queremos trabajar con un CSVo directamente con una hoja Excel: 1. Seleccionamos el icono de carpeta a la izquierda, que nos abre la barra lateral de Archivos 2. Hacemos click sobre el icono de la hoja con la flecha vertical, lo que nos abre una ventana de selección de archivos. 3. Seleccionamos la hoja de cálculo o CSVcon la que vamos a trabajar y la subimos al espacio de trabajo de Google Colaborate\nUna vez la hoja de cálculo o el CSV en nuestro espacio de trabajo, procedemos a leer los datos.\nEl mayor inconveniente de esta forma de trabajo es que cada vez que salimos de la sesión, Google Colab borra todos nuestros archivos del espacio de trabajo; cada vez que iniciemos una sesión, tendremos que repetir el proceso de subir nuestros datos al espacio de trabajo. Por eso, la mejor opción es tener nuestros datos en una carpeta de Google Drive, que puede utilizarse desde Colab tal como vamos a ver a continuación.\n\n\nOpción 2: Leer directamente los datos de nuestra carpeta de Google Drive\nEsta es una opción mucho más cómoda que nos evita los pasos intermedios, ya que no necesitamos subir la hoja de cálculo al espacio de trabajo.\n\nimport pandas as pd\n\n# Montar Google Drive si tu archivo está allí\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Ruta de tu archivo de datos. Asegúrate de que esta ruta sea correcta.\n# Si está en la raíz de tu Drive, sería algo así:\narchivo_csv = '/content/drive/MyDrive/camembert.csv'\n\n# Si el archivo está directamente en el entorno de Colab (subido o creado allí),\n# la ruta podría ser simplemente el nombre del archivo si estás en el mismo directorio:\n# archivo_csv = 'camembert.csv'\n\n\n# Leer el archivo CSV\narchivo_csv = '/content/drive/MyDrive/camembert.csv'\n\n# Mostrar las primeras filas del DataFrame para verificar\nprint(df.head())\n\n\n\nExplicación de los pasos:\n\nimport pandas as pd:\n\nEsta línea importa la librería pandas, que es fundamental para el manejo y análisis de datos en Python. Es una convención llamarla pd.\n\nMontar Google Drive (si es necesario):\n\nSi tu archivo camembert.csv está almacenado en tu Google Drive, necesitas montar Drive en tu entorno de Colab para poder acceder a él. Las líneas from google.colab import drive y drive.mount('/content/drive') se encargan de esto.\nUna vez montado, tus archivos de Drive serán accesibles a través de la ruta /content/drive/MyDrive/.\n\narchivo_csv = '/content/drive/MyDrive/camembert.csv':\n\nAquí defines la ruta completa de tu archivo Excel.\n¡Importante! Debes ajustar esta ruta a la ubicación real de tu archivo.\n\nSi lo tienes en Google Drive (y lo montaste), la ruta será similar a /content/drive/MyDrive/nombre_de_tu_carpeta/camembert.csv.\nSi subiste el archivo directamente a Colab (usando el ícono de la carpeta en el panel izquierdo y luego “Subir”), y está en el directorio raíz de Colab, simplemente el nombre del archivo (camembert.csv) debería funcionar.\n\n\ndf = pd.read_csv (archivo_csv, sep = \";\", decimal = \",\"):\n\nEsta es la función clave.\npd.read_csv es el equivalente directo a read_excel() de readxl en R.\nEl primer argumento es la ruta al archivo CSV, que proporcionamos en la variable archivo_csv\nLos dos argumentos siguientes, sep = \";\", decimal = \",\", indican a pandasque nuestro archivo CSV se ha generado utilizando el formato europeo, es decir, la coma como separador decimal, y el punto y coma como separador de datos.\n\nprint(df.head()):\n\nUna vez que los datos se han cargado en un DataFrame de pandas (que es el objeto df), puedes usar df.head() para ver las primeras 5 filas y asegurarte de que los datos se cargaron correctamente. Esto es similar a head(df) en R.\n\n\nCon estos pasos, tendrás tus datos cargados en un DataFrame de pandas, listos para ser manipulados y analizados en Python.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Trabajando con Google Colaborate</span>"
    ]
  },
  {
    "objectID": "020-intro-colab.html#importando-nuestros-datos-a-colab.",
    "href": "020-intro-colab.html#importando-nuestros-datos-a-colab.",
    "title": "5  Trabajando con Google Colaborate",
    "section": "5.2 Importando nuestros datos a Colab.",
    "text": "5.2 Importando nuestros datos a Colab.\nComo siempre, empezamos nuestro cuaderno importando las bibliotecas que vamos a usar en el ejercicio.\n\nimport os                       # # manejo de funciones del sistema\nimport sys                      # manejo de funciones del sistema\nimport pandas as pd             # manejo de dataframes y series\nimport numpy as np              # funciones de cálculo numérico de la biblioteca `numpy`\nimport matplotlib.pyplot as plt # funciones específicas de `matplotlib.pyplot`\nfrom scipy.stats import norm    # para hacer la curva de distribución normal\nimport locale                   # para poner las fechas en formato español\n\n\n# ajustamos fechas a formato español, el punto y coma evita la impresion de salida de la funcion\nlocale.setlocale(locale.LC_TIME, 'Spanish_Spain.1252');\n\nImportamos también las funciones gráficas de la biblioteca seaborny establecemos un estilo por defecto, con el fondo blanco.\n\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nEs necesario cargar el fichero camembert.csv en el espacio de trabajo de Google Colaborate. Para ello, montamos nuestro drive y damos a Python el path (la direccion de la carpeta).\nUna vez cargado, verificamos que existe y hay acceso:\n\n# 1. Ejecuta esta celda para montar tu Google Drive.\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# 2. Verifica y ajusta esta ruta de archivo si es necesario:\ngoogle_drive_path_folder = '/content/drive/MyDrive/Colab Notebooks/master-queseria/datos/'\nnombre_archivo_base = 'camembert.csv'\nPATH_TO_READ = os.path.join(google_drive_path_folder, nombre_archivo_base)\n\nAunque podríamos hacer la lectura del fichero de datos con la instruccion simple,\n\ndf = pd.read_csv(\n        PATH_TO_READ, \n        sep = \";\", \n        decimal = \",\", \n        encoding = 'utf-8' # codificacion usada por Windows\n    )\n\nen Python siempre se prefiere encapsular la instrucción en una estructura try...except, que maneja los posibles errores o excepciones que pueden producirse en la lectura, por ejemplo, que nuestro path sea incorrecto, o que hayamos escrito mal el nombre del fichero, o cualquier otro error que haga que la instruccion de lectura falle. Añadimos también algunas instrucciones de print() que nos ayuden a saber que todo ha ido bien (o mal).\n\n# ----------------------------------------\n# Lectura de datos\n# ----------------------------------------\n\nprint(f\"Intentando leer el archivo desde la ruta: {PATH_TO_READ}\\n\")\n\ntry:\n    df = pd.read_csv(\n        PATH_TO_READ, \n        sep = \";\", \n        decimal = \",\", \n        encoding = 'utf-8'\n    )\n    print(\"--- LECTURA EXITOSA ---\")\n    print(f\"Filas cargadas: {len(df)}\")\n    df.head()\n    \nexcept FileNotFoundError:\n    print(\"\\n--- ERROR DE ARCHIVO ---\")\n    print(ERROR_MESSAGE_HELP)\n        \nexcept Exception as e:\n    print(\"\\n--- ERROR AL LEER EL CSV ---\")\n    print(\"Ocurrió un error al leer el archivo. Error: {e}\")\n\nIntentando leer el archivo desde la ruta: camembert.csv\n\n--- LECTURA EXITOSA ---\nFilas cargadas: 211\n\n\nAhora convertimos la primera columna, que pandas ha leido como texto, en una columna de fecha, y la asignamos como index del dataframe. Esto tendrá interés especial cuando usemos las funciones de pandaspara hacer nuestros gráficos de series; el formateo de series temporales en esta biblioteca es uno de sus puntos más fuertes.\n\ndf['fecha']= pd.DatetimeIndex(df.fecha).normalize()\ndf.set_index('fecha',inplace=True)\ndf.sort_index(inplace=True)\n\nPodemos mostrar el dataframe que hemos leído, mediante la funcion .head(), que nos muestra las cinco primeras lineas.\n\ndf.head()\n\n\n\n\n\n\n\n\nfabricacion\nest\nmg\nph\ncloruros\ncoliformes\n\n\nfecha\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n1\n46.22\n23.0\n4.61\n1.88\n0.0\n\n\n2020-01-03\n1\n45.28\n23.0\n4.78\n1.62\n0.0\n\n\n2020-01-06\n1\n45.11\n23.0\n4.72\n1.69\n2000.0\n\n\n2020-01-08\n1\n49.05\n23.5\n4.68\n1.65\n6000.0\n\n\n2020-01-09\n1\n47.82\n25.0\n4.66\n1.37\n100.0\n\n\n\n\n\n\n\nVemos que el nombre de la columna fecha está colocada en una línea inferior respecto a los otros nombres de columna. Esto se debe a que la hemos designado como ìndex, y, por lo tanto, ya no es una columna ordinaria para pandas (podemos volver a convertirla en columna normal o de texto en cualquier momento según nuestros intereses, como veremos más adelante).\nTambién podemos usar la función .info(), que nos dice la estructura interna de nuestro dataframe y el tipo de los datos (entero, numérico, carácter…). Dado que la fecha, como hemos visto, está formateada como fecha y asignada como ìndex, ya no aparece en el listado de columnas de datos, sino que aparece en la primera línea como DateTimeIndex, y la informacion nos dice los límites de esas fechas.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 211 entries, 2020-01-02 to 2020-12-29\nData columns (total 6 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   fabricacion  211 non-null    int64  \n 1   est          211 non-null    float64\n 2   mg           211 non-null    float64\n 3   ph           211 non-null    float64\n 4   cloruros     211 non-null    float64\n 5   coliformes   210 non-null    float64\ndtypes: float64(5), int64(1)\nmemory usage: 11.5 KB\n\n\nUna vez leído correctamente el DataFrame, podemos hacer algunos gráficos de sus columnas numéricas. También usaremos las funciones de seaborn que producen salidas muy atractivas y son funcionjes fáciles de manejar.\n\ndf[\"est\"].hist() # histograma bássico de matplotlib\n\nplt.show()\n\n\n\n\n\n\n\n\nFíjate en la forma correcta de designar una columna en un DataFramede pandas, usando su nombre.\n\ndf[\"est\"].hist(bins = 5)\n\nplt.show()\n\n\n\n\n\n\n\n\nVamos a repetir el histograma con seaborn, que nos permite incluir una curva de densidad fácilmente (pregunta: ¿qué es una curva de densidad?)\n\nsns.displot(df[\"est\"], kde = True)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(df['est'])\n\nplt.show()\n\n\n\n\n\n\n\n\nAquí puedes ver la potencia de pandasparamanejar y agrupar las series. Para representar los datos por mes, sólo tenemos que crear una nueva columna mes indicando a pandas que extraiga del índice la parte de fecha que corresponde al mes. ¿Fácil, no?\n\ndf['mes'] = df.index.month\nsns.boxplot(x='mes', y='est', data=df)\n\nplt.show()\n\n\n\n\n\n\n\n\nEn vez de usar el número para el mes, podemos usar el código de letras abreviado (en este caso, seabornutiliza la abreviatura en inglés, pero hemos cambiado la codificación para que lo represente en español, en la instruccion locale que hemos usado en la primera casilla). Aprovechamos para personalizar un poco el gráfico.\n\ndf['mes_abreviado'] = df.index.strftime('%b')\nsns.boxplot(x='mes_abreviado', y='est', data=df)\n# Opcional: Personalizar el gráfico\nplt.title('Boxplot de la variable \"est\" por Mes')\nplt.xlabel('Mes')\nplt.ylabel('Valor de \"est\"')\nplt.grid(axis='y', linestyle='--', alpha=0.7) # Añadir una cuadrícula para mejor lectura\nplt.xticks(rotation=45) # Rotar las etiquetas del eje X si son muchas\nplt.tight_layout() # Ajusta automáticamente los parámetros de la subtrama para un diseño ajustado\n\nplt.show()\n\n\n\n\n\n\n\n\nLa función .plot() nos representa los valores de la columna en orden secuencial:\n\ndf[\"est\"].plot()\nplt.show()\n\n\n\n\n\n\n\n\nSi utilizamos las funciones de pandas, podemos reformatear las fechas como serie temporal. Creamos una serie temporal y la remuestreamos para hacer las medias semanales del extracto seco total, eliminando las semans en las que no hay valores con .dropna(). La función resample('W-MON') formatea las fechas para que las semanas empiecen en lunes, como es el caso en Europa (en USA la semana empieza el domingo). Aprovechamos para mostrar una de las potencias de Python: podemos hacer que varios cálculos se hagan a continuacion de los otros, simplemente enlazando las funciones, hasta el .plot()\n\nplt.rcParams['figure.figsize'] = (10.0,4.0)\nts = pd.Series(df[\"est\"].dropna())\nts.resample('W-MON').mean().plot(title=\"Media del extracto seco total semanal\")\nplt.show()\n\n\n\n\n\n\n\n\nVeamos a continuación otras formas de formatear la serie sobre la marcha, pero esta vez representando la desviación típicaen vez de la media.\n\nts.resample('W-MON').std().plot(title=\"Desviación típica del extracto seco total semanal\");\n\n\n\n\n\n\n\n\nO podemos representar un período específico de la serie indicando a pandas los límites inferior y superior de las fechas que queremos.\n\nts[\"2020-05\":\"2020-08\"].resample('W-MON').mean().plot(title=\"Media del extracto seco total semanal\");\n\n\n\n\n\n\n\n\nA modo ilustrativo, aunque sin un interés prioritario, muestro un grafico jointplot() de seaborn que muestra la facilidad con la que esta biblioteca puede hacer un gráfico complejo de dispersión e histograma simultáneamente.\n\nsns.jointplot(x=\"est\", y=\"mg\", data = df[~df.index.duplicated(keep='first')])\n\n\n\n\n\n\n\n\nUna alternativa más moderna a los boxplots son los llamados violin plots, que tienenla ventaja sobrelos primeros de mostrar la curva de distribución de los datos.\n\nsns.violinplot(y=df[\"est\"], x=df.index.month)\n\n\n\n\n\n\n\n\nFinalmente, una serie de cálculos más complejos para obtener los gráficos de capacidad de un proceso, como muestra de cómo se pueden usar las funciones y gráficos de Python para prácticamente cualquier necesidad.\n\n# codigo de Roberto Salazar\n# https://medium.com/geekculture/process-capability-analysis-with-python-a0f3aed8e578\n\n# Set specification limits\ntarget = 5\nLSL = 3\nUSL = 7\n\n# Generate normally distributed data points\ndata = np.random.normal(loc=target,scale=1,size=100)\n\n\n# Generate probability density function\nx = np.linspace(min(data), max(data), 1000)\ny = norm.pdf(x, loc=5, scale=1)\n\n# Plot histogram for data along with probability density functions and specification limits\nplt.figure(figsize=(10,5))\nplt.hist(data, color=\"lightgrey\", edgecolor=\"black\", density=True)\nsns.kdeplot(data, color=\"blue\", label=\"Density ST\")\nplt.plot(x, y, linestyle=\"--\", color=\"black\", label=\"Theorethical Density ST\")\nplt.axvline(LSL, linestyle=\"--\", color=\"red\", label=\"LSL\")\nplt.axvline(USL, linestyle=\"--\", color=\"orange\", label=\"USL\")\nplt.axvline(target, linestyle=\"--\", color=\"green\", label=\"Target\")\nplt.title('Process Capability Analysis')\nplt.xlabel(\"Measure\")\nplt.ylabel(\"\")\nplt.yticks([])\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Calculate Cp\nCp = (USL-LSL)/(6*np.std(data))\n\n# Calculate Cpk\nCpk = min((USL-data.mean())/(3*data.std()), (data.mean()-LSL)/(3*data.std()))\n\n# Calculate z-value\nz = min((USL-data.mean())/(data.std()), (data.mean()-LSL)/(data.std()))\n\n# Get data summary statistics\nnum_samples = len(data)\nsample_mean = data.mean()\nsample_std = data.std()\nsample_max = data.max()\nsample_min = data.min()\nsample_median = np.median(data)\n\n# Get percentage of data points outside of specification limits\npct_below_LSL = len(data[data &lt; LSL])/len(data)*100\npct_above_USL = len(data[data &gt; USL])/len(data)*100\n\n# \"\"\" # Write .txt file with results\n# with open('/content/process_results.txt', \"w\") as results:\n#     results.write(\"PROCESS CAPABILITY ANALYSIS\\n\")\n\n#     results.write(\"-----------------------------------\\n\")\n#     results.write(f\"Specifications\\n\")\n#     results.write(f\"\\nTarget: {target}\\n\")\n#     results.write(f\"LSL: {LSL}\\n\")\n#     results.write(f\"USL: {USL}\\n\")\n\n#     results.write(\"-----------------------------------\\n\")\n#     results.write(f\"Indices\\n\")\n#     results.write(f\"\\nCp: {round(Cp,2)}\\n\")\n#     results.write(f\"Cpk: {round(Cpk,2)}\\n\")\n#     results.write(f\"z: {round(z,2)}\\n\")\n\n#     results.write(\"-----------------------------------\\n\")\n#     results.write(f\"Summary Statistics\\n\")\n#     results.write(f\"\\nNumber of samples: {round(num_samples,2)}\\n\")\n#     results.write(f\"Sample mean: {round(sample_mean,2)}\\n\")\n#     results.write(f\"Sample std: {round(sample_std,2)}\\n\")\n#     results.write(f\"Sample max: {round(sample_max,2)}\\n\")\n#     results.write(f\"Sample min: {round(sample_min,2)}\\n\")\n#     results.write(f\"Sample median: {round(sample_median,2)}\\n\")\n\n#     results.write(f\"Percentage of data points below LSL: {round(pct_below_LSL,2)}%\\n\")\n#     results.write(f\"Percentage of data points above USL: {round(pct_above_USL,2)}%\\n\") \"\"\"\n\nver https://seaborn.pydata.org/generated/seaborn.displot.html#seaborn.displot\nA continuación, una serie de celdas que realizan gráficos diversos, puedes dedicar un rato a estudiarlas e intentar comprender bien su programación.\n\nlimite_rechazo = 231    ##\nlimite_deficientes = 242    ##\n\nLSL = df.est.mean() - 3 * df.est.std()    ## lower specification limit\nUSL = df.est.mean() + 3 * df.est.std()    ## upper specification limit\n\ndf.insert(6,'LSL', LSL)\ndf.insert(7,'USL', USL)\n\n\ndf['fecha'] = df.index\n\n\ndf.head()\n\n\n\n\n\n\n\n\nfabricacion\nest\nmg\nph\ncloruros\ncoliformes\nLSL\nUSL\nmes\nmes_abreviado\nfecha\n\n\nfecha\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n1\n46.22\n23.0\n4.61\n1.88\n0.0\n42.279767\n50.994641\n1\nene.\n2020-01-02\n\n\n2020-01-03\n1\n45.28\n23.0\n4.78\n1.62\n0.0\n42.279767\n50.994641\n1\nene.\n2020-01-03\n\n\n2020-01-06\n1\n45.11\n23.0\n4.72\n1.69\n2000.0\n42.279767\n50.994641\n1\nene.\n2020-01-06\n\n\n2020-01-08\n1\n49.05\n23.5\n4.68\n1.65\n6000.0\n42.279767\n50.994641\n1\nene.\n2020-01-08\n\n\n2020-01-09\n1\n47.82\n25.0\n4.66\n1.37\n100.0\n42.279767\n50.994641\n1\nene.\n2020-01-09\n\n\n\n\n\n\n\n\ndf2 = pd.melt(df, id_vars= ['fecha'], value_vars=[\"est\",\"LSL\", \"USL\"],  value_name=\"valores\")\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nfecha\nvariable\nvalores\n\n\n\n\n0\n2020-01-02\nest\n46.22\n\n\n1\n2020-01-03\nest\n45.28\n\n\n2\n2020-01-06\nest\n45.11\n\n\n3\n2020-01-08\nest\n49.05\n\n\n4\n2020-01-09\nest\n47.82\n\n\n\n\n\n\n\n\nplt.rcParams['figure.figsize'] = (10.0,4.0)\ng = sns.lineplot(data=df2, x=\"fecha\", y=\"valores\", hue=\"variable\")\n\n\n\n\n\n\n\n\n\ndf3 = df2.groupby([df2['fecha'].dt.isocalendar().week, \"variable\"]).mean()\n\n\ndf3.reset_index(inplace=True)\n\n\ndf3.head()\n\n\n\n\n\n\n\n\nweek\nvariable\nfecha\nvalores\n\n\n\n\n0\n1\nLSL\n2020-01-02 12:00:00\n42.279767\n\n\n1\n1\nUSL\n2020-01-02 12:00:00\n50.994641\n\n\n2\n1\nest\n2020-01-02 12:00:00\n45.750000\n\n\n3\n2\nLSL\n2020-01-08 18:00:00\n42.279767\n\n\n4\n2\nUSL\n2020-01-08 18:00:00\n50.994641\n\n\n\n\n\n\n\n\nplt.rcParams['figure.figsize'] = (10.0,4.0)\nsns.set_style(\"whitegrid\")\ng = sns.lineplot(data=df3, x=\"week\", y=\"valores\", hue=\"variable\")\n\n\n\n\n\n\n\n\n\ndf = pd.read_csv(\"camembert.csv\" , decimal = \",\", sep=\";\")\ndf['fecha']= pd.DatetimeIndex(df.fecha).normalize()\ndf.drop(['fabricacion','mg', 'cloruros','coliformes'], axis=1, inplace = True)\n\n\ndf2 = df['est'].groupby(df['fecha'].dt.isocalendar().week).agg(['mean','std'])\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nmean\nstd\n\n\nweek\n\n\n\n\n\n\n1\n45.7500\n0.664680\n\n\n2\n47.3125\n1.646134\n\n\n3\n45.7700\n0.547494\n\n\n4\n46.5400\n0.893476\n\n\n5\n46.0150\n0.522986\n\n\n\n\n\n\n\n\ndf2['mean'].plot();\n\n\n\n\n\n\n\n\n\ndf2['std'].plot();\n\n\n\n\n\n\n\n\n\nLSL = df2['mean'] - 3 * df2['std']    ## lower specification limit\nUSL = df2['mean'] + 3 * df2['std']    ## upper specification limit\n\ndf2.insert(2,'LSL', LSL)\ndf2.insert(3,'USL', USL)\n\n# limite_rechazo = 231    ##\n# limite_deficientes = 242    ##\n# df3.insert(5,'rechazo', limite_rechazo)\n# df3.insert(6,'deficientes', limite_deficientes)\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nmean\nstd\nLSL\nUSL\n\n\nweek\n\n\n\n\n\n\n\n\n1\n45.7500\n0.664680\n43.755959\n47.744041\n\n\n2\n47.3125\n1.646134\n42.374097\n52.250903\n\n\n3\n45.7700\n0.547494\n44.127517\n47.412483\n\n\n4\n46.5400\n0.893476\n43.859571\n49.220429\n\n\n5\n46.0150\n0.522986\n44.446042\n47.583958\n\n\n\n\n\n\n\n\ndf2['semana'] = df2.index # necesitamos la semana en una columna de valor\ndf3 = pd.melt(df2, id_vars= ['semana'], value_vars=[\"mean\",\"LSL\", \"USL\"],  value_name=\"valores\")\n\n\ng = sns.lineplot(data=df3, x=\"semana\", y=\"valores\", hue=\"variable\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Trabajando con Google Colaborate</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html",
    "href": "030-organizacion.html",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "",
    "text": "6.1 Definiciones y términos útiles",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#definiciones-y-términos-útiles",
    "href": "030-organizacion.html#definiciones-y-términos-útiles",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "",
    "text": "Población y muestra\nUna población es un conjunto de de personas, cosas o, en general, objetos en estudio. A veces, una población es demasiado grande para que podamos abarcarla completa; para poder estudiarla, obtenemos una muestra, que consiste en un subconjunto de la población que hemos seleccionado para su estudio. El proceso de obtener una muestra se llama muestreo, y se realiza de acuerdo con normas y procedimientos específicos.\nEn muchas ocasiones, cuando se recogen los datos como resultado de una experimentación, definimos la población como todos los resultados que podríamos haber obtenido. Llamamos a este conjunto de posibles resultados una población conceptual. Por ejemplo, cuando medimos el \\(pH\\) de varias muestras de leche, la población es el conjunto de todos los resultados posibles que podríamos haber tenido. Muchos problemas de ingeniería y tecnología se refieren a poblaciones conceptuales.\n\n\n\n\n\n\nRecuerda\n\n\n\nEn la mayoría de las ocasiones, nuestros datos provienen de una muestra obtenida de una población,\n\n\nCuando tomamos una muestra, debemos estar seguros de que contiene las propiedades que queremos estudiar en la población. En ese caso, decimos que la muestra es representativa: los individuos de la muestra son representativos de la población. Para que la muestra sea representativa, debe ser obtenida mediante un muestreo aleatorio. Una muestra aleatoria simple de tamaño \\(n\\) consiste en \\(n\\) individuos de una población, elegidos de forma que cada conjunto posible de \\(n\\) individuos tiene la misma probabilidad de ser elegido.\n\n\n\n\n\n\nEjemplo 1: Muestreando una cámara de maduración de queso\n\n\n\nImagina que debes analizar el contenido de extracto seco de una producción de quesos en fase de maduración en una cámara frigorífica. Dado que la cámara está muy llena y es complicado acceder a su interior, decides tomar tu muestra de los quesos que están más a tu alcance, situados cerca de la puerta y a la altura de la vista.\n¿Consideras que esta es una forma correcta de obtener una muestra representativa?\n¿Cómo definirías la población en este caso?\n\n\n\n\n\n\n\n\nRespuesta al ejemplo 1: Muestreando una cámara de maduración de queso\n\n\n\n\n\nLa forma de tomar la muestra no es correcta, ya que al seleccionar únicamente los quesos más accesibles introduces un sesgo en la muestra. Las condiciones de humedad, temperatura y circulación de aire podrían variar dentro de la cámara, afectando el contenido de extracto seco según la ubicación de los quesos.\nPara obtener resultados fiables, es fundamental que la muestra sea representativa de toda la producción. Para ello, se debe aplicar un método de muestreo aleatorio, donde todos los quesos tengan la misma probabilidad de ser seleccionados.\nEn este caso, la población está formada por el conjunto total de quesos en proceso de maduración dentro de la cámara.\n\n\n\n\n\n\n\n\n\n¿Qué es la probabilidad?\n\n\n\nEl concepto de probabilidad se explica de forma detallada en el capítulo 9\n\n\n\n\nParámetro y estadístico\nUn parámetro es una característica de una población. Podemos estimar su valor mediante la extracción de una muestra, que utilizaremos para calcular un estadístico muestral. Llamamos estadístico a un número que representa una propiedad o característica de la muestra, y constituye una estimación del valor de un parámetro de la población que estamos estudiando.\n\n\nVariables y casos\nA los objetos descritos en un conjunto de datos los llamamos casos, de forma genérica. A veces, estos casos pueden corresponder a personas; en ese caso podemos llamarlos individuos. Cuando los objetos que estudiamos no son personas, como es lo habitual en el entorno industrial, utilizamos la nomenclatura genérica.\nUn atributo es una característica que define una propiedad de un objeto, persona o cosa. Por ejemplo, edad, peso, altura, sexo, color de ojos, son atributos de una persona. Llamamos variable a una característica cualquiera de un individuo o caso que puede ser medida u observada. Una variable puede tomar diferentes valores en diferentes individuos o casos.\nSegún estas definiciones que acabamos de ver, una muestra está formada por un conjunto de casos, y cada caso contiene un determinado número de variables, que contienen los valores que hemos analizado o medido.\n\n\nTipos de variables\nAlgunas variables, como el color, sirven para clasificar los individuos en categorías. Otras, como la altura o el peso de un individuo, pueden tomar valores numéricos con los que podemos hacer cálculos. Por ejemplo, podemos sumar la altura de varias personas, pero no tiene sentido sumar los colores del arco-iris (aunque sí podemos contarlos, y hacer cálculos con estos recuentos). También podemos categorizar variables continuas: podemos clasificar nuestro grupo de personas en altas o bajas, y podemos contar cuántas personas entran en cada categoría.\n\n\n\n\n\n\n\n\n\nVariables cualitativas  o categóricas\n\nVariables cuantitativas  o métricas\n\n\n\n\n\nNominales\nOrdinales\nDiscretas\nContinuas\n\n\nValores en categorías arbitrarias\nValores en categorías ordenadas\nValores enteros en escala numérica\nValores continuos en escala numérica\n\n\n(sin unidades)\n(sin unidades)\nUnidades contadas\nUnidades medidas\n\n\n\nUna variable categórica coloca a un individuo en uno o más grupos o categorías\nUna variable métrica toma valores numéricos con los que tiene sentido realizar cálculos aritméticos como sumar, restar, etc.\nLas variables categóricas se conocen también como variables cualitativas porque indican cualidades.\nLas variables métricas se conocen también como variables cuantitativas porque indican cantidades.\n\n\n\n\n\n\nComentario: ¿Cualitativo quiere decir “que tiene calidad”?\n\n\n\nAlgunas veces se usa el término cualitativo de forma incorrecta para referirse a la calidad de algo, por ejemplo, cuando alguien dice: “Este envase es muy cualitativo”. La forma correcta sería: “Este envase es de gran calidad”.\nEn realidad, cualitativo no proviene calidad sino de cualidad, y hace referencia a las características o propiedades de un objeto o fenómeno, sin implicar juicio de valor. Como hemos visto, este término se usa en estadística para describir variables que representan categorías o atributos, como el color, el tipo de material o la marca, sin medir cantidades.\n\n\n\n\n\n\n\n\nPara resolver\n\n\n\nEjemplo 1. Tiramos un dado al aire. Describe a qué corresponde la variable y el caso.\nEjemplo 2. Durante un proceso de envasado de un producto que dura una hora, controlamos el peso de cada envase cada minuto. Describe la variable y el caso. ¿Puede haber más de una variable?\n\n\n\n\n\n\n\n\nRespuestas: Para resolver\n\n\n\n\n\nEjemplo 1: La variable es el resultado que obtenemos cada vez; podríamos denominarla, por ejemplo, \\(resultado\\_obtenido\\). Colocaríamos este nombre en el encabezado de una columna en una hoja de cálculo. Cada tirada que hacemos es un caso; iríamos colocando el resultado que obtenemos cada vez en una nueva fila de nuestra hoja de cálculo.\nEjemplo 2. En este caso, la variable es el \\(peso\\_obtenido\\), y cada pesada constituye un caso. Si registrásemos, además, la hora y el minuto en el que que hemos hecho cada control de peso, podríamos definir una nueva variable, que podríamos llamar \\(hora\\), y que colocaríamos en una columna al lado del \\(peso\\_obtenido\\). Incluso podríamos definir otra variable adicional, el \\(numero\\_de\\_pesada\\), que sería un número secuencial empezando en \\(1\\) y que se incrementaría en cada pesada, de forma que al final esta variable nos daría el número de pesadas realizadas, y nos indicaría además el orden en el que las hemos realizado. Puesto que hemos realizado una pesada cada minuto, tendríamos tres variables y 61 líneas (un encabezado y 60 líneas correspondientes una a cada minuto)\n\n\n\n\n\nReglas básicas para establecer los nombres de las variables.\nSegún hemos visto, existen diferentes tipos de variables, cualitativas (categóricas) y cuantitativas (métricas). Normalmente, los valores de las variables categóricas se describen mediante textos del tipo “color blanco”, “hombre”, “mujer”, “alto”, “bajo”, etc.  Suelen corresponder con características descriptivas, y por lo tanto, no puede hacerse cálculos directamente con ellos, a menos que se hayan resumido, por ejemplo, mediante un conteo. Las variables métricas consisten en valores numéricos, que pueden ser enteros o continuos, y pueden utilizarse directamente para hacer cálculos tales como sumas, etc.A partir de aquí utilizaremos una nomenclatura compatible con las hojas de cálculo en formato europeo para escribir los números; usaremos la coma para la separación decimal y el punto y coma para la separación de los números cuando los escribamos de forma seriada.\nUna variable está descrita siempre por un nombre, que designa la variable, y un valor o conjunto de valores, que corresponden a los casos. Este conjunto de valores, como acabamos de ver, pueden ser textos o números.\nExisten también otros tipos de variables que veremos más tarde, como variables lógicas o fechas, según el tipo de dato que almacenemos en esa variable.\nEjemplos de valores de texto: “Carlos”, “fruta”, “Lluvia fuerte”, “muy ácido”, “sabor a fresa”\nEjemplos de valores numéricos: \\(1\\); \\(7\\); \\(10,65\\)\nSiempre que sea posible, utilizaremos el nombre del atributo o característica que estamos midiendo o analizando, o su abreviatura, para designar una variable; por ejemplo, si estamos recogiendo la altura de una serie de personas, llamaremos altura a la variable; si estamos recogiendo el peso, usaremos el nombre peso, etc.\nA veces, asignar un nombre a una variable no es todo lo fácil que podría parecer a simple vista. Por ejemplo, ¿qué nombre daríamos a una variable que va a recoger los valores de \\(pH\\) de la leche en una cuba de queso en el momento de añadir el cuajo? Está claro que \\(pH\\) no es suficiente, porque en el proceso hay varias medidas de \\(pH\\) y sería bueno que pudiésemos diferenciarlas con facilidad. En un caso como éste, es probable que necesitemos utilizar varias palabras o abreviaturas que describan mejor el nombre de la variable.\nPara la construcción correcta de estos nombres, se han establecido un conjunto de reglas, con el objetivo de evitar errores y facilitar el intercambio de los datos entre diferentes programas de análisis. Son éstas:\n\nUn nombre válido consiste en una combinación de letras, números y signo de subrayado (\\(\\_\\))\nUn nombre de variable no puede empezar por un número, un punto o un signo de subrayado (\\(\\_\\)); debe empezar siempre por una letra.\nLos nombres de variables irán siempre en minúsculas. Según esta regla, \\(Peso\\) no es un nombre válido, pero \\(peso\\) si lo es.\nNo utilizaremos espacios en blanco, acentos ni caracteres especiales como \\(\\tilde{n}\\), \\(\\%\\), guiones o paréntesis.\nHay veces en que nos interesa unir varias palabras para construir un nombre de variable. Se utilizan diferentes formas de unir palabras, por ejemplo:\n\nun punto, como en \\(peso.en.cm\\),\nlo que se ha llamado escritura de camello (camelCase), que se llama así por el uso de mayúsculas y minúsculas mezcladas (\\(PesoEnCm\\))\nel signo de subrayado \\(\\_\\), como en \\(peso\\_en\\_cm\\)\n\nAlgunas de estas opciones son utilizadas en distintas comunidades de usuarios, por ejemplo la opción 1 es utilizada en la guía de estilo de Google, y la opción 2 es muy utilizada por los programadores del entorno de los lenguajes de Microsoft. Nosotros utilizaremos el signo de subrayado (\\(\\_\\)), que es la forma más usada en el entorno de programación de R.\nSiempre se separarán las palabras mediante el signo de subrayado (_) para facilitar la lectura. Así, aunque \\(temperatura1\\) es un nombre válido, preferiremos \\(temp\\_1\\); es más corto y de lectura más clara. Igualmente, preferiremos \\(peso\\_empaquetado\\) a \\(pesoempaquetado\\)\nMantendremos los nombres razonablemente cortos para facilitar la lectura. Aunque podemos hacer los nombres todo lo largos que queramos, es más cómodo utilizar nombres cortos. Por ejemplo, podríamos utilizar \\(temperatura\\_de\\_la\\_leche\\_al\\_cuajar\\), pero preferiremos abreviarlo como \\(temp\\_cuajo\\).\n\nNombres no válidos:\n\n\\(peso\\ en\\ gramos\\) (contiene espacios)\n\\(pH\\_de\\_la\\_leche\\_en\\_Recepci\\acute{o}n\\) (demasiado largo, tiene un acento, tiene mayúsculas)\n\\(extracto\\_seco\\_total\\_a\\_la\\_salida\\_de\\_la\\_salmuera\\) (demasiado largo)\n\nAlternativas válidas:\n\n\\(peso\\_g\\)\n\\(pH\\_leche\\_rec\\) (en este caso, de manera excepcional, podemos mantener el uso de la mayúscula por corrección formal)\n\\(est\\_salida\\_sal\\)\n\nUn caso particular es el uso de la \\(\\tilde{n}\\), ya que no hay una alternativa fácil para el uso en las fechas (\\(a\\tilde{n}o\\)). R admite el uso de la \\(\\tilde{n}\\) en los nombres de variables, por lo que podremos usarlo con cuidado, poniendo atención a los posibles errores que se pudiesen producir en algunas librerías.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#el-flujo-de-trabajo",
    "href": "030-organizacion.html#el-flujo-de-trabajo",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.2 El flujo de trabajo",
    "text": "6.2 El flujo de trabajo\nUn flujo de trabajo en análisis de datos es un proceso sistemático y estructurado que guía la manipulación, exploración y análisis de datos desde su recolección hasta la obtención de resultados finales y su comunicación. Es una hoja de ruta que asegura que cada paso se realice de manera ordenada, eficiente y reproducible, facilitando la comprensión y utilización de los datos.\nHadley Wickham (Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel 2023) ha propuesto un método de flujo de trabajo que se ha convertido en estándar en la ciencia de datos (hay versión en español: (Garret Grolemund Hadley Wickham 2023))\n\nEste flujo de trabajo abarca diversas actividades como la importación de datos, su limpieza y transformación, el análisis exploratorio, y el modelado, culminando en la interpretación y presentación de los resultados. Todo esto se hace siguiendo metodologías específicas para asegurar la calidad y precisión del análisis.\nLa importancia de seguir un flujo de trabajo bien definido radica en la capacidad de replicar estudios, minimizar errores y fomentar la transparencia, permitiendo que cualquier persona pueda entender y validar las decisiones tomadas durante el análisis. Además, mejora la eficiencia al estandarizar procedimientos y facilita la colaboración entre diferentes analistas o equipos de trabajo.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#etapas-en-un-flujo-de-trabajo-estructurado.",
    "href": "030-organizacion.html#etapas-en-un-flujo-de-trabajo-estructurado.",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.3 Etapas en un flujo de trabajo estructurado.",
    "text": "6.3 Etapas en un flujo de trabajo estructurado.\n\nRecolección de datos\nLa primera etapa es la recolección de datos. Esto implica obtener datos desde diversas fuentes como archivos CSV, bases de datos, APIs, etc. La recolección de datos es fundamental porque la calidad del análisis depende de la calidad de los datos recolectados.\n\n\nInspección de los datos\nUna vez recolectados, se procede a inspeccionar los datos para entender su estructura y contenido. Esto incluye examinar los tipos de datos, la presencia de valores faltantes, duplicados y la distribución de las variables.\n\n\nLimpieza de los datos\nLa limpieza de datos es crucial para asegurar que la información sea precisa y esté en el formato adecuado. Esta etapa incluye:\n\nManejo de valores faltantes.\nEliminación de duplicados.\nCorrección de inconsistencias.\nTransformación de datos a un formato adecuado para el análisis.\n\n\n\nTransformación de los datos\nTransformar los datos a un formato ordenado o arreglado (tidy) es esencial. Según Wickham, los datos arreglados tienen una estructura clara: cada variable es una columna, cada observación es una fila, y cada valor tiene su propia celda. Este formato facilita el análisis y la visualización de datos.\n\n\nAnálisis exploratorio de datos (EDA)\nEl Análisis Exploratorio de Datos (EDA) busca entender los patrones y relaciones en los datos mediante estadísticas descriptivas y visualizaciones. Durante esta etapa se realizan:\n\nEstadísticas básicas (media, mediana, desviación estándar).\nGráficos y diagramas para visualizar la distribución de los datos y las relaciones entre variables.\n\n\n\nModelado de datos\nDependiendo del objetivo del análisis, se pueden aplicar diversos modelos estadísticos para extraer información y hacer predicciones. Esto puede incluir:\n\nModelos de regresión.\nAnálisis de clasificación.\nModelos de series temporales, entre otros.\n\n\n\nComunicación de resultados\nFinalmente, es fundamental comunicar los resultados de manera clara y efectiva. Esto se hace a través de:\n\nTablas y resúmenes interpretativos.\nGráficos y visualizaciones.\nInformes y presentaciones que expliquen los hallazgos y sus implicaciones.\n\nSiguiendo estos pasos, puedes manejar y analizar datos de manera organizada y reproducible, facilitando la colaboración y la toma de decisiones informadas. Este flujo de trabajo asegura que los datos se traten de manera sistemática, desde su recolección hasta la comunicación de los resultados.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#razones-para-seguir-un-flujo-de-trabajo",
    "href": "030-organizacion.html#razones-para-seguir-un-flujo-de-trabajo",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.4 Razones para seguir un flujo de trabajo",
    "text": "6.4 Razones para seguir un flujo de trabajo\n\nReproducibilidad: Un flujo de trabajo organizado permite que los análisis sean reproducibles. Otros pueden seguir los mismos pasos para obtener resultados similares, lo que es crucial en la investigación y en la toma de decisiones basadas en datos.\nConsistencia: Ayuda a asegurar que los pasos se realizan de manera consistente cada vez que se ejecuta el análisis, reduciendo la posibilidad de errores humanos.\nTransparencia: Proporciona un registro claro de los pasos tomados durante el análisis, facilitando la revisión y validación de los resultados.\nEficiencia: Mejora la eficiencia al estandarizar el proceso, permitiendo a los analistas concentrarse en el análisis y la interpretación de los datos en lugar de tareas repetitivas.\nColaboración: Facilita la colaboración entre equipos, ya que los flujos de trabajo bien documentados permiten que otros comprendan fácilmente los métodos y pasos utilizados.\nAdaptabilidad: Permite adaptar y ajustar el análisis de manera más fácil cuando se presentan nuevos datos o cuando cambian los objetivos del análisis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#problemas-de-no-seguir-un-flujo-de-trabajo-estructurado",
    "href": "030-organizacion.html#problemas-de-no-seguir-un-flujo-de-trabajo-estructurado",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.5 Problemas de no seguir un flujo de trabajo estructurado",
    "text": "6.5 Problemas de no seguir un flujo de trabajo estructurado\n\nErrores y Sesgos: La falta de un enfoque estructurado puede resultar en errores y sesgos inadvertidos en el análisis, lo que puede llevar a conclusiones incorrectas.\nDificultad para Replicar Resultados: Sin un flujo de trabajo claro, replicar resultados se vuelve complicado, lo que puede afectar la credibilidad del análisis y la capacidad de validación por otros.\nFalta de Documentación: La ausencia de una documentación adecuada dificulta entender los pasos y las decisiones tomadas durante el análisis, lo que puede ser un obstáculo en auditorías y revisiones.\nIneficiencia: Sin una estructura clara, los analistas pueden gastar tiempo valioso realizando tareas repetitivas y resolviendo problemas que podrían haberse evitado con un enfoque más organizado.\nProblemas de Colaboración: La colaboración se vuelve más difícil si los miembros del equipo no pueden seguir o entender los pasos tomados por otros, lo que puede llevar a malentendidos y duplicación de esfuerzos.\nDificultad para Adaptarse a Cambios: Sin un flujo de trabajo definido, adaptar el análisis a nuevos datos o cambios en los objetivos puede ser más complejo y propenso a errores.\n\nEn resumen, seguir un flujo de trabajo estructurado es esencial para garantizar la precisión, eficiencia, y reproducibilidad del análisis de datos, evitando problemas que puedan comprometer la integridad y utilidad de los resultados. .",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#un-ejemplo-revisando-los-datos-existentes.",
    "href": "030-organizacion.html#un-ejemplo-revisando-los-datos-existentes.",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.6 Un ejemplo: revisando los datos existentes.",
    "text": "6.6 Un ejemplo: revisando los datos existentes.\nCuando nos incorporamos a un equipo de trabajo existente, lo más seguro es que ya se disponga de un sistema de archivo de los datos, de acuerdo con los métodos habituales del equipo. En muchos casos, el diseño de la captura de datos sigue aproximadamente el modelo manual en papel; se introducen los datos en la hoja de cálculo y una vez completados, se imprime el documento para su archivo.\nEl error más común que se suele cometer es, precisamente, tratar la hoja de cálculo como un bloc de notas, es decir, hacer anotaciones de forma libre, colocar los datos y el resultado de los análisis al lado y en cualquier parte de la hoja, y apoyarnos en el contexto para interpretar lo que hemos guardado. Pero para que el ordenador sea capaz de analizar nuestros datos de manera eficiente, debemos estructurarlos de tal forma que el programa use la información tal como nosotros queremos.\nEs común utilizar una hoja para guardar múltiples tablas de datos, tal como vemos en la Figura 6.1. Esta estructura, sin embargo, resulta enormemente confusa para su análisis, o lo imposibilita completamente.\n\n\n\n\n\n\nFigura 6.1: Hoja Excel desordenada: ¡No hagas esto!\n\n\n\nEn otros casos, los datos se guardan en hojas de cálculo que se componen de diferentes pestañas para cada semana, cada mes o cada año, como vemos en la Figura 6.2. Sin embargo, esta forma de almacenar los datos tampoco es la óptima para su análisis.\n\n\n\n\n\n\nFigura 6.2: Hoja Excel con una estructura no ordenada\n\n\n\nSi las diferentes tablas presentan situaciones diferentes, o datos que no están relacionados, podemos utilizar diferentes pestañas. Pero si los datos están vinculados, por ejemplo, se corresponden con las mismas medidas, hechas en fechas diferentes (meses, años), la respuesta es que las pestañas no son la forma correcta de almacenarlos datos; lo mejor es añadir una variable que nos permita diferenciar los datos por fecha; nuestro programa de análisis nos permitirá filtrar los datos según la fecha que deseemos, y todos estarán en una única tabla, facilitando la coherencia y el análisis posterior.\nHay muchas formas de almacenar la información en una hoja de cálculo, pero sólo la estructura de datos ordenados o arreglados facilita la utilización de los datos tanto por la hoja de cálculo como por otros programas de análisis.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#los-datos-ordenados-o-arreglados-tidy-data",
    "href": "030-organizacion.html#los-datos-ordenados-o-arreglados-tidy-data",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.7 Los datos ordenados o arreglados (tidy data)",
    "text": "6.7 Los datos ordenados o arreglados (tidy data)\nDe la misma manera que la gramática permite ordenar y estructurar un escrito de acuerdo a reglas comunes, hay reglas para que el almacenamiento de los datos sea lo más homogéneo posible y se reduzcan los errores al mínimo.\nLas reglas principales al almacenar nuestros datos en una hoja de cálculo son tres:\n\ncolumnas=variables,\nfilas=observaciones,\nceldas=valores.\n\nCada variable debe tener su propia columna, cada observación debe tener su propia fila, y cada valor debe tener su propia celda o casilla .\nEstas tres reglas básicas son las que hacen que nuestro conjunto de datos sea ordenado (o arreglado)(Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel 2023) (hay edición online en español: (Garret Grolemund Hadley Wickham 2023):\nLa Figura 6.3 muestra estas reglas de forma visual.\n\n\n\n\n\n\nFigura 6.3: Sigue estas tres reglas para tener un conjunto de datos ordenado: las variables están en columnas, las observaciones están en filas, y los valores están en celdas. Fuente de la imagen: Garret Grolemund Hadley Wickham Mine Çetinkaya-Rundel (2023)\n\n\n\nEstas tres reglas están interrelacionadas porque es imposible satisfacer sólo dos de tres.\nEn una hoja de cálculo, una tabla de datos arreglada tendría este aspecto:\n\n\n\n\n\n\nFigura 6.4: Hoja Excel con estructura rectangular de datos ordenados\n\n\n\nDatos rectangulares: formato tabla",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#qué-es-un-fichero-plano-y-un-fichero-csv",
    "href": "030-organizacion.html#qué-es-un-fichero-plano-y-un-fichero-csv",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.8 Qué es un fichero plano y un fichero CSV",
    "text": "6.8 Qué es un fichero plano y un fichero CSV\nSe suele llamar fichero plano a un fichero de datos de texto sin ningún tipo de formato, donde los datos están separados por espacios o tabulaciones. Muchos equipos automáticos, como balanzas de laboratorio o básculas de proceso, producen ficheros planos de texto, que se pueden importar a Excel o R. Un fichero CSV es un fichero plano en el que los valores están separados por un carácter especial, llamado separador. Este separador puede ser una coma , (cuando los decimales se separan mediante un punto, como en EEUU) o un punto y coma ; (cuando los decimales se separan mediante una coma, como en España)\n\n\n\n\n\n\n\n\nFichero plano separado por espacios\n\n\n\n\n\n\n\nFichero CSV separado por comas (USA, GB)\n\n\n\n\n\n\n\n\n\nFichero CSV separado por puntos y comas (Europa, España)\n\n\n\n\n\n\nFigura 6.5: Tres tipos de ficheros planos de texto.\n\n\n\nEn un fichero plano o en un fichero CSV, la primera fila puede contener los nombres de las columnas. En algunos casos, los elementos de texto pueden estar entre comillas. En estos casos, los programas de importación se ocupan de la conversión de formatos.\nLa importación de un fichero CSV en Excel en español es directa si se ha generado con puntos y comas como separador y comas para los decimales; si no es así, nos aparecerá como un fichero plano de texto sin formato, y tendremos que realizar una conversión.\n\nCómo exportar un fichero CSV desde Excel.\nUna vez que tenemos nuestros datos en Excel, hay dos formas en las que podemos poner los datos a disposición de Python para su análisis: exportarlos a un archivo de texto con formato CSV, o leer directamente los datos de Excel desde Python utilizando las funciónes adecuadas. En ambos casos, el resultado es un DataFrame o cuadro de datos, que es una estructura equivalente a la de nuestra tabla de datos en Excel. En el capçitulo siguiente veremos cómo realizar esta importación de datos Python desde nuestro fichero CSV\nEn el ejemplo, crearemos un fichero CSV desde Excel y haremos un manejo básico de los datos.\n\nPaso 1: Guardar el Fichero CSV desde Excel\n\nAbre tu fichero en Excel.\nSelecciona Archivo &gt; Guardar como.\nElige el formato CSV (delimitado por comas) (*.csv).\nGuarda el archivo.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#practica-con-el-csv",
    "href": "030-organizacion.html#practica-con-el-csv",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.9 Practica con el CSV",
    "text": "6.9 Practica con el CSV\n\nEn Excel, prueba a guardar y recuperar tus datos en formato CSV.\n\nUna vez cargados los datos en Excel,\n\nselecciona el menú Datos &gt; Filtro\n\ny prueba a filtrar y seleccionar los datos según diferentes criterios",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "030-organizacion.html#para-resolver-1",
    "href": "030-organizacion.html#para-resolver-1",
    "title": "6  La organización de los datos y el flujo de trabajo.Los datos ordenados o arreglados (tidy data)",
    "section": "6.10 Para resolver",
    "text": "6.10 Para resolver\nPoner aquí distintos ejemplos de nombres de variables para ver si son válidos o no. Describir medidas y preguntar cómo llamaríamos a esa variable (por ejemplo, temperatura de la leche que acabamos de descargar de una cisterna) :::\n\n\n\n\nHadley Wickham, Garret Grolemund. 2023. «R Para Ciencia de Datos». 2023. https://es.r4ds.hadley.nz/.\n\n\nHadley Wickham, Garret Grolemund, Mine Çetinkaya-Rundel. 2023. R for Data Science, 2nd ed. 1005 Gravenstein Highway North, Sebastopol, CA95472: O’Reilly Media Inc. https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>La organización de los datos y el flujo de trabajo.Los datos ordenados o *arreglados* (*tidy data*)</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html",
    "href": "040-exploracion.html",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "7.1 Explorando los datos\nEn este capítulo estudiaremos cómo describir conjuntos de datos de forma visual, utilizando varios tipos de gráficos distintos:\nVeremos la relación visual entre un histograma y un diagrama de caja, y aprenderemos también a construir tablas de frecuencias en Excel y en Python. Finalmente, veremos algunos otros tipos de gráficos que son útiles para aplicaciones concretas, como los gráficos de densidad.\nUtilizaremos dos grupos de datos,\n- las alturas de un grupo de alumnos y alumnas, recogidos en un fichero CSV denominado `aula1.csv`,\n- los datos analíticos de una fabricación de camembert a lo largo de un año, recogidos en un fichero CSV denominado `camembert.csv`",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#explorando-los-datos-con-excel-y-python.",
    "href": "040-exploracion.html#explorando-los-datos-con-excel-y-python.",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "El diagrama de tallo y hojas (stem and leaf plot o stemplot)\nEl diagrama de tallo y hojas, también conocido como stemplot, es una herramienta gráfica utilizada en estadística para representar la distribución de un conjunto de datos. Es especialmente útil para conjuntos de datos pequeños y proporciona una forma rápida y efectiva de visualizar la forma de los datos y su dispersión. El stemplot recibe este nombre porque el dibujo que resulta se asemeja a un tallo el que le salen las hojas que son los datos individuales.\nLos componentes de un stemplot son:\n\nTallo: Representa el grupo principal de los valores de los datos. Generalmente, se usa la parte más significativa del número. Por ejemplo, en el número 43, el tallo podría ser 4.\nHojas: Representan los dígitos finales o menos significativos de los valores de los datos. Siguiendo el ejemplo anterior, la hoja sería 3.\n\n\nConstrucción del diagrama\nVamos a utilizar los datos de medidas de altura de nuestro grupo de alumnos. Quitamos el último dígito a la derecha de nuestros valores y colocamos verticalmente los valores resultantes ordenándolos de menor a mayor, y evitando las repeticiones. Para evitar errores en la escala, debemos incluir los valores intermedios aunque no haya ninguno en nuestros datos (en el ejemplo, el valor 16 que correspondería a los 160). Esto forma el “tallo” de nuestro diagrama:\n\nA continuación añadimos las hojas en la celda a la derecha, que consisten en los valores que hemos “cortado” de nuestro árbol, uno al lado de otro, incluyendo esta vez los valores repetidos, en orden de menor a mayor. Por ejemplo, para el valor 135, descartamos 13 y utlizamos 5; para el valor 138, descartamos 13 y utilizamos 8, y así sucesivamente para todos los valores.\n\n\n\n\nInterpretación del diagrama de tallo y hojas\n\nTallo: Los números a la izquierda del símbolo | representan los valores base (o tallos), en este caso, las decenas de las alturas.\nHojas: Los números a la derecha del símbolo | representan los dígitos adicionales (o hojas). Por ejemplo, en la línea 13 | 58, el tallo es 13 (130), y las hojas son 5 y 8, que corresponden a los datos 135 y 138.\n\nEl diagrama nos dice que los valores en torno a 150 cm son los más frecuentes, y que hay un valor alto (175) que se separa un poco del resto.\n\n\nResumen\nEl stemplot es muy sencillo de hacer y nos da una visión rápida y compacta de la distribución de nuestros valores, así como de la posible existencia de valores que se separan del conjunto. Estos valores alejados, que se conocen en inglés como outliers, tienen mucha importancia en el analisis e interpretación de los datos, como veremos más adelante.\nLa ventaja principal del stemplot es que mantiene los valores originales de las observaciones, y puede hacerse fácilmente con bolígrafo y papel, sin necesidad de más herramientas.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#distribuciones-de-frecuencias",
    "href": "040-exploracion.html#distribuciones-de-frecuencias",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.2 Distribuciones de frecuencias",
    "text": "7.2 Distribuciones de frecuencias\nUna distribución de frecuencias es una tabla que muestra la frecuencia con la que ocurren los valores diferentes en un conjunto de datos. Esta herramienta es fundamental en la estadística descriptiva y permite resumir y visualizar cómo se distribuyen los datos de manera clara y comprensible. A partir de una tabla de frecuencias se pueden construir diagramas de barra o histogramas para visualizar la tabla de forma gráfica.\nPara construir una distribución de frecuencias, agrupamos nuestros valores por intervalos, y contamos el número de observaciones que aparecen en cada intervalo. Los componentes de una distribución de frecuencias son:\n\nlas categorías o clases son los intervalos o valores específicos de los datos que se están analizando. Cada categoría representa un rango de valores en caso de datos continuos, o valores específicos en caso de datos discretos.\nla frecuencia absoluta es un recuento simple de cuántas veces aparece cada valor en un conjunto de datos.\nla frecuencia relativa nos muestra la proporción de cada valor frente al total. Puede expresarse como fracción (entre 0 y 1) o como porcentaje (respecto a 100), y se calcula como: \\[\n\\text{Frecuencia Relativa} = \\frac{\\text{Frecuencia Absoluta}}{\\text{Número Total de Observaciones}}\n\\]\nla frecuencia acumulada nos dice cuántas observaciones están por debajo de un cierto valor.\nla frecuencia relativa acumulada es la proporción de valores que están por debajo de un cierto valor\n\n\nConstrucción en Excel\nLa tabla a continuación muestra una distribución de frecuencias de las alturas de nuestro grupo de alumnos, calculada mediante una tabla dinámica de Excel.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#instrucciones-paso-a-paso-en-excel",
    "href": "040-exploracion.html#instrucciones-paso-a-paso-en-excel",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.3 Instrucciones paso a paso en Excel",
    "text": "7.3 Instrucciones paso a paso en Excel\nPara crear una tabla de frecuencias de la variable altura_cm mediante tablas dinámicas en Excel, sigue estos pasos:\n1. Selecciona los Datos\n\nAbre tu archivo de Excel y selecciona toda la tabla que incluye los encabezados (nombre y altura_cm).\n\n2. Inserta una tabla dinámica\n\nVe a la pestaña Insertar en la barra de herramientas de Excel.\nHaz clic en Tabla Dinámica.\nEn el cuadro de diálogo que aparece, asegúrate de que el rango de datos seleccionado es correcto y elige dónde deseas colocar la tabla dinámica (en una nueva hoja de cálculo o en la hoja actual).\n\n3. Añade la frecuencia absoluta\n\nEn el panel de campos de la tabla dinámica, arrastra el campo altura_cm a la sección Filas.\nArrastra nuevamente el campo altura_cm a la sección Valores.\n\n4. Ajusta la configuración de valores\n\nHaz clic en el campo altura_cm en la sección Valores.\nSelecciona Configuración de campo de valor.\nEn el cuadro de diálogo que aparece, asegúrate de que esté seleccionada la opción Recuento\nAcepta todo hasta volver a Excel.\n\n5. Añade la frecuencia relativa\n\nEn el panel de campos de la tabla dinámica, arrastra de nuevo el campo altura_cm a la sección Valores. Ahora la variable aparecerá como altura_cm2.\n\n6. Ajusta de nuevo la configuración de valores\n\nHaz clic en el campo altura_cm2 en la sección Valores.\nSelecciona Configuración de campo de valor.\nEn el cuadro de diálogo que aparece, asegúrate de que esté seleccionada la opción Recuento.\nEn ese mismo cuadro, haz click en el botón Formato de número, selecciona Número y 2 decimales, y acepta.\nEn ese mismo cuadro, selecciona la pestaña que dice Mostrar valores como\nEn el menú desplegable, escoge la opción % del total de columnas.\nAcepta todo hasta volver a Excel.\n\n7. Ordena y formatea\n\nPuedes ordenar las alturas en orden ascendente o descendente haciendo clic en la flecha junto a altura_cm en la tabla dinámica.\nTambién puedes cambiar el formato de la tabla dinámica para que sea más fácil de leer.\nPuedes renombrar los encabezados de la tabla para que sea más fácil de leer, rotulando las columnas, por ejemplo, como frec_absy frec_rel, o cualquier otro encabezado que te resulte claro y útil.\n\n\nTambién podemos calcular las frecuencias absolutas de nuestra tabla en Python, de forma bastante sencilla, sin agrupar.\n\nimport pandas as pd\n\naltura_cm = [153,135,140,140,175,138,145,154,152,159,154]\nfrecuencias = pd.Series(altura_cm).value_counts()\nprint(frecuencias)\n\n140    2\n154    2\n153    1\n135    1\n175    1\n138    1\n145    1\n152    1\n159    1\nName: count, dtype: int64\n\n\nCon un poco más de código podemos hacer la tabla agrupando los valores en clases de amplitud 10, con las frecuencias absolutas y relativas. Es un poco más complicado, tómate tu tiempo para entender cada paso de las instrucciones.\n\nimport pandas as pd\nimport numpy as np\n\naltura_cm = [153,135,140,140,175,138,145,154,152,159,154]\nserie = pd.Series(altura_cm)\n\n# Crear intervalos automáticamente con amplitud 10\nbins = np.arange(min(altura_cm)//10*10, max(altura_cm)//10*10 + 20, 10)\n\n# Agrupar y contar\ntabla = pd.cut(serie, bins=bins).value_counts(sort=False)\n\n# Convertir a DataFrame con acumulados y relativos\ndf = tabla.to_frame(name='Frecuencia')\ndf['Frecuencia acumulada'] = df['Frecuencia'].cumsum()\ndf['Frecuencia relativa (%)'] = (df['Frecuencia'] / df['Frecuencia'].sum() * 100).round(2)\ndf['Acumulada (%)'] = df['Frecuencia relativa (%)'].cumsum()\n\nprint(df)\n\n            Frecuencia  Frecuencia acumulada  Frecuencia relativa (%)  \\\n(130, 140]           4                     4                    36.36   \n(140, 150]           1                     5                     9.09   \n(150, 160]           5                    10                    45.45   \n(160, 170]           0                    10                     0.00   \n(170, 180]           1                    11                     9.09   \n\n            Acumulada (%)  \n(130, 140]          36.36  \n(140, 150]          45.45  \n(150, 160]          90.90  \n(160, 170]          90.90  \n(170, 180]          99.99  \n\n\nVamos a explicar el código línea a línea.\n\nEste script en Python construye una tabla de frecuencias agrupadas a partir de un conjunto de datos numéricos (altura_cm), usando pandas y numpy.\nimport pandas as pd\nimport numpy as np\n🔹 Importa las bibliotecas necesarias: - pandas: para manipulación tabular. - numpy: para cálculos numéricos y generación de rangos.\n\naltura_cm = [153,135,140,140,175,138,145,154,152,159,154]\n🔹 Define el conjunto de datos como una lista de alturas en centímetros.\n\nserie = pd.Series(altura_cm)\n🔹 Convierte la lista en una Series de pandas, lo que permite aplicar funciones estadísticas fácilmente.\n\nbins = np.arange(min(altura_cm)//10*10, max(altura_cm)//10*10 + 20, 10)\n🔹 Genera los límites de clase con amplitud 10: - min(altura_cm)//10*10: redondea hacia abajo el mínimo a la decena más cercana. - max(altura_cm)//10*10 + 20: redondea hacia arriba el máximo y añade 10 extra para incluir el último dato. - np.arange(...): crea un array de límites desde el mínimo hasta el máximo, en pasos de 10.\n📌 Ejemplo de salida: [130, 140, 150, 160, 170, 180]\n\ntabla = pd.cut(serie, bins=bins).value_counts(sort=False)\n🔹 Agrupa los datos en los intervalos definidos: - pd.cut(...): asigna cada dato a un intervalo. - .value_counts(sort=False): cuenta cuántos datos hay en cada intervalo, sin reordenarlos.\n\ndf = tabla.to_frame(name='Frecuencia')\n🔹 Convierte la serie de frecuencias en un DataFrame con una columna llamada 'Frecuencia'.\n\ndf['Frecuencia acumulada'] = df['Frecuencia'].cumsum()\n🔹 Calcula la frecuencia acumulada sumando progresivamente los valores de la columna 'Frecuencia'.\n\ndf['Frecuencia relativa (%)'] = (df['Frecuencia'] / df['Frecuencia'].sum() * 100).round(2)\n🔹 Calcula la frecuencia relativa en porcentaje: - Divide cada frecuencia por el total. - Multiplica por 100. - Redondea a 2 decimales.\n\ndf['Acumulada (%)'] = df['Frecuencia relativa (%)'].cumsum()\n🔹 Calcula el porcentaje acumulado, útil para análisis tipo Pareto.\n\nprint(df)\n🔹 Muestra la tabla final con: - Intervalos - Frecuencia absoluta - Frecuencia acumulada - Frecuencia relativa (%) - Porcentaje acumulado\n\nComo ves en el resultado, a veces se utilizan los símbolos ( y [ para definir los intervalos, tal como se hace en matemáticas.\n\nIntervalo abierto: El símbolo ( se utiliza para denotar un intervalo abierto. El límite correspondiente no está incluuido en el intervalo.\nIntervalo cerrado o semiabierto:El símbolo [ se utiliza para denotar un intervalo cerrado o semiabierto. EL límite correspondiente sí está incluido en el intervalo.\n\nEjemplos:\n\n\\((a, b)\\) representa todos los números reales mayores que \\(a\\) y menores que \\(b\\) (excluye los valores \\(a\\) y \\(b\\)).\n\\([a, b]\\) representa todos los números reales mayores o iguales que \\(a\\) y menores o iguales que \\(b\\) (incluye \\(a\\) y \\(b\\)).\n\\([a, b)\\) representa todos los números reales mayores o iguales que \\(a\\) y menores que \\(b\\) (incluye \\(a\\), pero excluye \\(b\\))\n\\((a, b]\\) representa todos los números reales mayores que \\(a\\) y menores o iguales que \\(b\\) (excluye \\(a\\), pero incluye \\(b\\)). :::\n\nSi comparamos los dos métodos que hemos utilizado para construir la tabla de frecuencias, vemos que:\n\nen Excel los pasos que hemos dado no están registrados y a la vista, y, por lo tanto, no son fácilmente revisables\nen Python, todos los pasos y opciones que hemos utilizado están a la vista en el código del script\n\nSi otra persona quisiera modificar la tabla, le sería fácil editar el código y relanzar el script, mientras que en Excel no sería fácil asegurarse de todos y cada uno de los pasos y clicks de ratón que hemos dado para construir y formatear la tabla.\nEs el caso, por ejemplo, de que enviásemos la tabla a otra persona y ésta tuviese que editarla en nuestra ausencia. En Excel, tendríamos que enviar a esa persona una explicación con las instrucciones oportunas; en cambio, en Python, una vez que se comprende el código, no hacen falta más explicaciones adicionales. Incluso sin una comprensión total del código, se podría duplicar exactamente la tabla copiando y ejecutando el código.\nEsta es una de las principales razones de la conveniencia del aprendizaje de Python incluso para las actividades más sencillas.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#ejercicio-propuesto",
    "href": "040-exploracion.html#ejercicio-propuesto",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.4 Ejercicio propuesto",
    "text": "7.4 Ejercicio propuesto\nEn la tabla de frecuencias anterior, calcular frecuencia absoluta acumulada y frecuencia relativa acumulada en Excel, e incluirlas en la tabla como dos columnas adicionales. La frecuencia relativa acumulada nos permitirá crear diagramas de Pareto, muy útiles en los procesos de mejora de calidad.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#respuesta-al-ejercicio-propuesto-en-r",
    "href": "040-exploracion.html#respuesta-al-ejercicio-propuesto-en-r",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.5 Respuesta al ejercicio propuesto (en R)",
    "text": "7.5 Respuesta al ejercicio propuesto (en R)\nPara añadir las columnas de frecuencias absolutas acumuladas (frec_abs_acum) y frecuencias relativas acumuladas (frec_rel_acum) a tu dataframe df_aula1, puedes usar las funciones cumsum() y algunas mutaciones adicionales. Aquí tienes el código actualizado:\nLas distribuciones de frecuencias se pueden visualizar mediante varios tipos de gráficos, como histogramas, gráficos de barras y polígonos de frecuencias.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#diagrama-de-barras",
    "href": "040-exploracion.html#diagrama-de-barras",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.5 Diagrama de barras",
    "text": "7.5 Diagrama de barras\nCuando nuestra variable es discreta, podemos representar las frecuencias de cada valor de forma gráfica utilizando un diagrama de barras. Este diagrama utiliza barras rectangulares para representar la frecuencia de cada categoría.\nEste gráfico es muy utilizado para representar, por ejemplo, resultados de encuestas, como el número de votos que han obtenido los diferentes partidos políticos en unas elecciones, o tablas discretas, como los kilos fabricados por meses en una fábrica.\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nEjemplos de diagramas de barras",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#histograma",
    "href": "040-exploracion.html#histograma",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.6 Histograma",
    "text": "7.6 Histograma\nPara visualizar las variables continuas se utiliza el histograma, que es un diagrama que utiliza las barras rectangulares para hacer un gráfico de la distribución de valores continuos, previamente agrupados en intervalos (bins en inglés), tal como se ha hecho en la tabla de frecuencias.\n\nComponentes de un histograma\n\nEje horizontal (X): Representa los intervalos de valores de la variable. Cada intervalo abarca un rango específico de valores.\nEje vertical (Y): Muestra la frecuencia o el número de veces que los valores caen dentro de cada intervalo.\nBarras: Cada barra en el histograma representa un intervalo. La altura de la barra indica la frecuencia de los valores dentro de ese intervalo.\n\n\n\nCómo interpretar un histograma\n\nSi las barras son altas en un intervalo específico, significa que muchos valores del conjunto de datos caen dentro de ese rango.\nUn histograma puede ayudar a identificar patrones, como si los datos están distribuidos de manera simétrica, sesgada, o si hay picos y valles significativos.\n\n\n\nConstrucción de un histograma en Excel\nUsaremos el ejemplo de la altura en cm. de un grupo de alumnos, cuyos datos hemos guardado en el fichero csv aula1.csv.\nPodemos utilizar dos métodos para hacer un histograma en Excel\n\nMétodo 1: histograma directo\n\nSeleccionamos el rango de datos. Podemos hacerlo mediante un click en el encabezado de la columna de la variable altura_cm, en este caso, la columna B.\nEn la opción de menú Insertar, seleccionamos el icono Seleccionar gráficos de estadística\nSeleccionamos Histograma\n\n\n\n\n\n\nExcel calcula automáticamente la amplitud de los intervalos y el número de columnas; estas opciones pueden modificarse seleccionando con el botón derecho del ratón el elemento a modificar. En este caso, utilizamos estas opciones:\n\nEliminamos título del gráfico\nModificamos anchura de las barras seleccionando Dar formato a serie de datos, Opciones de serie, Ancho del rango= 50%\nModificamos los intervalos seleccionando el eje X: Dar formato al eje, Opciones de eje, Ancho del rango = 10\n\nLa descripción de los intervalos utiliza los mismos símbolos que hemos visto en las tablas de frecuencias de R.\nEn el momento de escribir este manual, Excel no permite hacer histogramas múltiples ni agrupados por otra variable, por lo que para diseños más complejos, no hay más remedio que recurrir a otras opciones como las tablas dinámicas de Excel o, mejor aún, R.\n\n\nMétodo 2: utilizar una tabla dinámica\nLa tabla dinámica que hemos construido en Excel ha convertido nuestra variable continua, altura_cm, en una tabla de valores discretos, al agrupar los valores en intervalos. En Excel podemos representar las frecuencias absolutas de nuestra tabla gráficamente, insertando un gráfico de barras a partir de la tabla:\n\nCon el cursor dentro de la tabla, seleccionamos la opción de menú Insertar\nInsertamos un gráfico de barras\n\nOpcionalmente, aplicamos las siguientes opciones de formato:\n\nHacemos click sobre uno de los botones del gráfico dinámico con el boton derecho del ratón, y seleccionamos la opción ocultar todos los botones del gráfico.\nEliminamos el título del gráfico\nAbrimos el formato de la serie de datos, e introducimos en la opción Ancho del rango el valor \\(50\\%\\) para ensanchar las barras.\n\n\n\n\n\n\nAl utiizar la tabla dinámica para construir el gráfico, Excel utiliza las categorías de la tabla dinámica. Dado que estas categorías (los intervalos que ha formado la tabla dinámica) son discretas, Excel utiliza el resultado de la tabla dinámica para hacer el gráfico con un diagrama de barras. No es posible insertar un histograma a partir de una tabla dinámica.\n\n\n\nAnálisis de un caso real\nEl histograma muestra su utilidad cuando representamos la distribución de un conjunto de valores más grande que nuestros once alumnos. Veamos su aplicación a los datos diarios de una fabricación de queso camembert a lo largo de un año.Los datos de esta fabricación están en el fichero camembert.csv.\nLa tabla de datos tiene esta estructura:\n\n\n\n\n\nLa tabla está formada por 211 casos.\n\nConstrucción del histograma con Excel\n\nMétodo directo\nSeleccionamos el rango del extracto seco (columna est) e insertamos el histograma, ajustando el ancho de intervalo a \\(1\\) en las opciones de gráfico:\n\n\n\n\n\n\n\nMétodo 2: mediante una tabla dinámica\nLos pasos a seguir son:\n\nConstruir la tabla dinámica\nAgrupar los datos\nInsertar el gráfico a partir de la tabla\n\nCon una agrupación de datos en intervalos de 1, esta es nuestra tabla dinámica:\n\nEn la figura siguiente vemos el histograma correspondiente a la tabla anterior, con un intervalo de clase de 1 punto de extracto seco total, y otras dos alternativas si el intervalo de clase fuese de 2 puntos o de de 0,5 puntos.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLa decisión de cambiar la amplitud del intervalo en un histograma influye en el aspecto del gráfico y es una opción personal; lo mejor es utilizar la que en nuestra opinión refleje mejor el aspecto de la distribución de datos, ni demasiado grande ni demasiado pequeña. En todo caso, debemos ser capaces de interpretar que la distribución de los valores es la misma en los tres casos: hay una mayoría de casos con valores entre 46 y 48, y muy pocos casos con valores muy bajos o muy altos. En este caso, la distribución de los valores es aproximadamente simétrica, y se reparten alrededor de una mayoría de valores centrales.\n\n\n\nCreación del histograma en Python\nLas funciones de histograma de Python permiten construir el histograma directamente sin necesidad de hacer previamente una tabla de frecuencias (en realidad, la tabla de frecuencias se calcula internamente). Es mucho más sencillo utilizar estas funciones, ya que el código se simplifica mucho.\nComo siempre, presentamos la opción básica de R junto con la opción de la función ggplot() de tidyverse, y utilizamos las opciones de personalización de ggplot()para trazar un histograma de aspecto semejante al gráfico básico; en esta ocasión, utilizamos el color de relleno gris.\nVemos que los dos gráficos no son idénticos a pesar de provenir de los mismos datos, porque la construcción de los intervalos subyacente es ligeramente diferente. Esto no debe preocuparnos, porque el aspecto general de la distribución de los datos es el mismo.\nEn la fase de exploración nos importa más entender estas propiedades de los datos (aspecto, forma de la distribución) que la precisión en la construcción del gráfico. En todo caso, en qqplot()podemos usar infinidad de opciones que nos permiten configurar el gráfico hasta los más mínimos detalles, por lo que esta función es mucho más útil que las funciones básicas para preparar gráficos que vayan destinados a informes o presentaciones. La función básica hist(), por su parte, es más sencilla de usar al principio cuando no tenemos todavía experiencia con ggplot(), y también para una visualización rápida.\n\n\n\nOtros ejemplos\nEn ocasiones, nos encontramos con datos que son asimétricos: hay una mayoría de valores bajos o bien de valores altos. Veamos un caso: los recuentos bacterianos de bacterias coliformes, que tenemos en la última columna a la derecha de nuestra tabla, en la variable ´coliformes´.\n\nEn este caso, vemos que la mayoría de los casos tienen valor cero. Es el caso de los recuentos de bacterias contaminantes, en el que la mayoría de los análisis tienen recuentos cero o muy bajos, y sólo en pocos casos tienen valores altos. Veremos con más detalle cómo tratar estas distribuciones cuando hablemos de las distribuciones de probabilidad, en capítulos posteriores.\nEn los histogramas de los recuentos bacterianos,utilizamos una opción para aumentar el número de barras que queremos en el histograma: breaks= en la función base, bins= en ggplot():\nEn casos de distribuciones muy asimétricas, a veces es conveniente aplicar una transformación a los datos, tal como el logaritmo decimal, mediante la función log10(); esto facilita la interpretación del gráfico:",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#gráficos-de-densidad",
    "href": "040-exploracion.html#gráficos-de-densidad",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.7 Gráficos de densidad",
    "text": "7.7 Gráficos de densidad\nUn gráfico de densidad en R es una representación visual suavizada de la distribución de un conjunto de datos. A diferencia de los histogramas, que dividen los datos en intervalos y cuentan las frecuencias, los gráficos de densidad utilizan técnicas estadísticas no paramétricas para estimar la función de densidad de probabilidad.\nUn gráfico de densidad, también conocido como density plot, es una representación visual suavizada de la distribución de un conjunto de datos.A diferencia de los histogramas, que dividen los datos en intervalos y cuentan las frecuencias, los gráficos de densidad utilizan técnicas estadísticas no paramétricas para estimar la función de densidad de probabilidad de una variable continua. El gráfico de densidad utiliza suavizamiento para proporcionar una estimación más continua de la distribución de los datos.\nExcel no permite la representación de los gráficos de densidad; en R pueden hacerse con la función ggplot()simplemente añadiendo la geometría geom_density()\nPodemos representar simultáneamente el histograma y la función de densidad; hay que tener en cuenta que para representar la densidad y el histograma superpuestos, nos vemos obligados a cambiar la escala del eje Y a los valores de densidad en vez de a las frecuencias, de manera que los dos gráficos puedan solaparse sin problemas.\nLa ventaja de los gráficos de densidad es que como no tenemos que fraccionar los datos en intervalos arbitrarios, no estamos afectados por el efecto visual de la anchura de estos intervalos. También hay otras ventajas desde el punto de vista estadístico, que veremos al hablar de las distribuciones de probabilidad.\nLa flexibilidad de configuración de los gráficos de qqplot()permite la personalización de los gráficos hasta el último detalle. A continuación vemos un gráfico de densidad agrupado por meses, que nos muestra aparentes diferencias en el extracto seco est según los meses.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#diagrama-de-caja-o-boxplot",
    "href": "040-exploracion.html#diagrama-de-caja-o-boxplot",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.8 Diagrama de caja o boxplot",
    "text": "7.8 Diagrama de caja o boxplot\nEste gráfico fue creado por el estadístico John Tukey en 1977, y es una herramienta fundamental en la exploración de datos. Se basa en un grupo de medidas que se utiliza ampliamente en la descripción de conjuntos de datos, el conjunto de cuartiles. Si dividimos un grupo de datos ordenados en cuatro partes iguales, mediante tres puntos de corte, llamamos primer cuartil o \\(Q1\\) al valor que se situa en el 25%; segundo cuartil, o \\(Q2\\), al valor que se sitúa en el centro (50%), y tercer cuartil, o \\(Q3\\), al punto que se situa en el 75% de los datos. A estos tres valores añadimos el mínimo y el máximo, y tenemos un conjunto de cinco números que nos permiten describir la forma de la distribución de datos con cierta precisión. El segundo cuartil (\\(Q2\\)), que corresponde al 50% de los datos, se conoce habitualmente como mediana. El valor resultante de restar \\(Q3-Q1\\) es lo que se conoce como rango intercuartil o \\(IQR\\), y es una medida de la dispersión de la distribución de datos (mide la amplitud de la distribución).\nEl diagrama de caja, también conocido como boxplot, es un gráfico que permite resumir las características principales de un conjunto de datos utilizando estos cinco números, tal como se explica a continuación. Sus ventajas son:\n\nMuestra la mediana y los cuartiles (Q1 y Q3) de los datos.\nPermite identificar la simetría de la distribución: si la mediana no está en el centro, la distribución no es simétrica.\nPermite detectar posibles valores atípicos, representando los valores atípicos (outliers) que están lejos del resto de los datos (un valor es atípico si está más allá de (Q3 + 1.5 IQR) o (Q1 - 1.5 IQR).\n\nLa construcción de un diagrama de caja es como sigue:\n\nMicrosoft Excel no dispone de un diseño de gráficos de caja que sea práctico, por lo que recurriremos siempre a R para realizarlos.\nComo casi siempre, hay una función de base que dibuja un boxplot y también una geometría de ggplot()que lo hace: geom_boxplot(), con muchas más opciones de diseño y formato que la opción de base.\n\\[\nboxplot(df_aula$altura_cm)\n\\]\n#| label: fig-boxplot2 #| fig-cap: “Ejemplos de boxplot” #| fig-subcap: #| - “Boxplot básico de R” #| - “Boxplot utilizando ggplot()” #| layout-ncol: 2\ndf_camembert &lt;- read.csv2(“excel-R/camembert.csv”,fileEncoding=‘latin1’) df_camembert\\(fecha &lt;- as.Date(df_camembert\\)fecha, format(“%d/%m/%Y”))\nboxplot(df_camembert$est)\ndf_camembert |&gt; ggplot(aes(x=““, y=est))+ geom_boxplot() + theme_bw() + theme(axis.text.x = element_blank(), # Oculta las etiquetas del eje x axis.ticks.x = element_blank()) # Oculta las marcas del eje x\nUn uso muy interesante del boxplot en R consiste en agrupar los boxplot de una variable en funcion de otra. En este caso, agrupamos el extracto seco por meses previa agrupación de la fecha. Esta agrupación puede hacerse tanto en los gráficos básicos de R como en ggplot():\nResumen\nEl código realiza las siguientes acciones:\n\nAñade una columna mes al dataframe df_camembert, que contiene solo el mes extraído de la columna fecha.\nCrea un boxplot básico que muestra la distribución de est por mes.\nUtiliza ggplot2 para crear un boxplot más personalizado de est por mes, con etiquetas y un tema minimalista para un diseño limpio.\n\n#| label: fig-boxplot3 #| fig-cap: “Ejemplos de boxplot” #| fig-subcap: #| - “Boxplot básico de R” #| - “Boxplot utilizando ggplot()” #| layout-ncol: 2\ndf_camembert\\(mes &lt;- format(df_camembert\\)fecha, “%m”) boxplot (df_camembert\\(est~df_camembert\\)mes)\ndf_camembert |&gt; mutate (mes = format(fecha, “%m”)) |&gt; ggplot(aes(x = mes, y = est)) + geom_boxplot() + labs(title = “Boxplot de ‘est’ por Meses”, x = “Mes”, y = “Valor de est”) + theme_minimal()\nLa agrupación de los boxplots por meses nos pone claramente de manifiesto las diferencias en el extracto seco estque ya habíamos visto con los gráficos de densidad. Estas diferencias son más claras en el mes de julio.\n\nRelación entre el boxplot y el histograma\nResulta muy útil comprender visualmente la relación entre el boxplot y el histograma para entender la distribución de los datos. En la gráfica siguiente se representan ambos simultáneamente\ndf_camembert |&gt; ggplot(aes(x = est)) + geom_histogram(fill = “lightblue”, color = “black”, bins = 20, alpha = 0.7) + geom_boxplot(width = 2, fill = “darkgrey”, alpha = 0.7, position = position_nudge(y = -2)) + labs(title = “Histograma y Boxplot”, y = “Frecuencias”)\ndf_camembert |&gt; ggplot(aes(x = log10(coliformes+1))) + geom_histogram(fill = “lightblue”, color = “black”, bins = 20, alpha = 0.7) + geom_boxplot(width = 4, fill = “darkgrey”, alpha = 0.7, position = position_nudge(y = -4)) + labs(title = “Histograma y Boxplot”, y = “Frecuencias”)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#gráficos-de-dispersión",
    "href": "040-exploracion.html#gráficos-de-dispersión",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.9 Gráficos de dispersión",
    "text": "7.9 Gráficos de dispersión\nUn gráfico de dispersión, también conocido como diagrama de dispersión o scatter plot, es una representación gráfica que utiliza puntos para mostrar la relación entre dos variables numéricas. Cada punto en el gráfico representa una observación del conjunto de datos y se coloca en el plano cartesiano de acuerdo con sus valores en las dos variables que se están comparando.\nUn gráfico de dispersión se compone mediante puntos:\n\nCada punto en el gráfico representa una observación.\nLa posición del punto en el gráfico está determinada por los valores de las dos variables para esa observación.\n\nLos gráficos de dispersión son útiles para identificar varios aspectos de la relación entre las dos variables:\n\nSi los puntos tienden a agruparse a lo largo de una línea recta ascendente, esto indica una correlación positiva (a medida que una variable aumenta, la otra también lo hace).\nSi los puntos se agrupan a lo largo de una línea descendente, esto indica una correlación negativa (a medida que una variable aumenta, la otra disminuye).\nSi los puntos forman una curva en lugar de una línea recta, esto sugiere una relación no lineal entre las variables.\nLa dispersión de los puntos puede indicar la variabilidad de los datos. Puntos que están muy lejos del patrón general pueden ser valores atípicos.\n\nComo siempre, vemos el gráfico de dispersión en Excel y a continuación en R. Utilizamos la tabla de datos camembert.csv y representamos las variables esty mg.\n\n\n\n\n\n\n\n\n\n\n\n(a) Tabla de origen mostrando una parte de los datos seleccionados\n\n\n\n\n\n\n\n\nGráfico de dispersión\n\n\n\n\n\n\nFigura 7.1: Gráfico de dispersión en Excel\n\n\n\nA continuación, el código R para realizar el gráfico de dispersión, con la función básica y con ggplot().\n#| label: fig-grafdisp #| fig-cap: “Ejemplos de gráfico de dispersión” #| fig-subcap: #| - “Gráfico de dispersion básico de R” #| - “Gráfico de dispersion utilizando ggplot()” #| layout-ncol: 2 #| plot(df_camembert\\(est, df_camembert\\)mg)\ndf_camembert |&gt; ggplot(aes(x=est,y=mg))+ geom_point() + theme_bw()\nLo esperable en nuestros datos es que el valor de est y el de mg estén asociados, y a valores altos de la primera variable correspondan valores altos de la segunda (para el mismo producto y sin cambios de tecnología). Sin embargo, algunos valores parecen no encajar en este modelo; es el caso de un valor de estpor encima de 52% con un valor de mginferior a 23%, o el caso de un valor de mg en torno al 28% con un valor de est por debajo de 47%. Debemos revisar estos valores aparentemente anormales para verificar si ha habido un error en la toma de muestras o un error analítico. En fábricas con productos diferentes cuyos resultados analíticos se recogen en una tabla de datos común, a veces puede haber errores en la introducción de los códigos de producto, de forma que la toma de muestras y los análisis pueden ser correctos, pero estar mal asignados al guardar los datos.\nComo vemos, los gráficos de dispersión son una herramienta esencial en el análisis de datos exploratorio, ya que permiten visualizar relaciones y patrones en los datos, identificar correlaciones y detectar posibles anomalías. Esta información es crucial para realizar análisis estadísticos más profundos y tomar decisiones basadas en datos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#gráficos-de-series-temporales",
    "href": "040-exploracion.html#gráficos-de-series-temporales",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.10 Gráficos de series temporales",
    "text": "7.10 Gráficos de series temporales\nHasta ahora hemos utilizado gráficos y tablas que describen la estructura y forma de una variable, o las relaciones entre dos variables. Hay otros gráficos que tienen en cuenta la forma en la que esos datos cambian con el tiempo. En este caso, será necesario que hayamos recogido en una variable de nuestra tabla los intervalos de tiempo en los que se han producido nuestros valores.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#ejemplos-de-series-temporales",
    "href": "040-exploracion.html#ejemplos-de-series-temporales",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.11 Ejemplos de series temporales",
    "text": "7.11 Ejemplos de series temporales\n¿Se te ocurren algunos ejemplos de series temporales?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "040-exploracion.html#algunos-ejemplos",
    "href": "040-exploracion.html#algunos-ejemplos",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "7.12 Algunos ejemplos",
    "text": "7.12 Algunos ejemplos\n\nproceso de llenado de envases de queso crema: se llena una tarrina cada 15 segundos. Nuestros datos deben recoger el tiempo y el peso.\nnuestro fichero de fabricación de queso camembert recoge los valores analíticos medios diarios del producto fabricado.\nUna fábrica recoge leche diariamente y analiza cada día la composición de la leche que entra en la fábrica.\n\nEn un gráfico de series temporales,\n\nel eje horizontal (X) representa el tiempo. Los puntos de tiempo pueden ser minutos, horas, días, meses, años, etc.\nel eje vertical (Y) representa los valores de la variable que se está estudiando. Estos valores pueden ser medidas como temperatura, ventas, precios, etc.\ncada valor individual corresponde a un punto\nlos valores se conectan mediante una línea que conecta los puntos de datos, mostrando cómo cambian los valores de la variable a lo largo del tiempo.\nnormalmente, en un gráfico de series temporales no suelen representarse los puntos individuales para facilitar la legibilidad del gráfico.\n\nEn nuestro conjunto de datos de fabricación de queso camembert, la primera columna de la tabla recoge la variable fecha, lo que nos permite ordenar nuestros valores en el tiempo.\nCuando representamos valores en el tiempo, nunca usaremos el diagrama de barras, sino el gráfico de líneas.\n\nCómo hacer los gráficos de series temporales en Excel y en R\nPara hacer el gráfico en Excel, seleccionamos la columna este insertamos un gráfico de líneas. A continuación, con el cursor sobre el gráfico, pulsamos el boton derecho y seleccionamos la opción Seleccionar datos. Una vez abierto el cuadro de opciones, editamos las etiquetas del eje X y seleccionamos el rango de la variable fecha desde la fila 2 hasta la última.Aceptamos, y a continuación editamos el formato del eje Y para sustituir el valor mínimo de \\(0\\) por \\(42\\), que es el valor que queremos como mínimo para nuestro gráfico.\n\n\n\nTabla y gráfico de series\n\n\nA continuación, los gráficos en R, como siempre con la opción de gráficos base y con ggplot(). R formatea automáticamente el rango del eje Y y no tenemos que hacer ninguna corrección de formato.\n#| label: fig-graftemp #| fig-cap: “Ejemplos de gráfico de series temporales” #| fig-subcap: #| - “Gráfico de series temporales básico de R” #| - “Gráfico de series temporales utilizando ggplot()” #| layout-ncol: 2\nplot(df_camembert\\(fecha, df_camembert\\)est, type=“l”)\ndf_camembert |&gt; ggplot(aes(x=fecha, y=est))+ geom_line() + theme_bw()\n\n\nResumen\nLos gráficos de series temporales son útiles para:\n\nIdentificar Tendencias:\n\nUna tendencia es una dirección general en la que los datos se mueven a lo largo del tiempo. Puede ser creciente, decreciente o constante.\n\nDetección de Estacionalidad:\n\nLa estacionalidad se refiere a patrones que se repiten en intervalos regulares de tiempo, como las ventas de productos estacionales.\n\nIdentificar Ciclos:\n\nLos ciclos son fluctuaciones que ocurren en intervalos no regulares y pueden deberse a factores económicos o de otra índole.\n\nDetección de Anomalías:\n\nLos picos y caídas repentinas pueden indicar eventos inusuales o errores en los datos.\n\n\nLos gráficos de series temporales son cruciales en diversas áreas:\n\nEconomía y Finanzas:\n\nSeguimiento de precios de acciones, tasas de interés y otros indicadores económicos.\n\nCiencia y Tecnología:\n\nMonitoreo de variables ambientales, datos meteorológicos y medidas científicas.\n\nNegocios:\n\nAnálisis de ventas, demanda de productos y desempeño empresarial a lo largo del tiempo.\n\n\nLos gráficos de series temporales proporcionan una visión clara y concisa de cómo cambian los datos a lo largo del tiempo. Esta visualización es fundamental para el análisis predictivo, la toma de decisiones y la identificación de patrones y anomalías en los datos.\nhttps://tidyplots.org/",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html",
    "href": "050-estad-simple.html",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "",
    "text": "8.1 La media aritmética: un ejemplo de cálculo.\nAl estudiar el diagrama de caja (boxplot) hemos visto que este gráfico se describe mediante los cinco números, que son:\nDe estos valores, el segundo cuartil, que se corresponde con la mitad de los valores, representa una estimación del centro de la distribución, y por eso lo llamamos mediana. La distancia entre el primer y tercer cuartil es lo que se conoce como rango intercuartil (se suele representar por sus siglas en inglés, IQR), y nos da una indicación de la dispersión: cuanto mayor mayor es la dispersión de nuestros valores, más alejados estarán del centro, y por lo tanto habrá mayor distancia entre el primer y el tercer cuartil.\nUna de las mayores ventajas de la mediana y del rango intercuartil es que son estadísticos robustos, es decir, tiene una alta robustez a los valores atípicos. Como en el cálculo del rango intercuartil no se tienen en cuenta los valores extremos, su valor variará muy poco si aparecen nuevas observaciones atípicas (outliers). Como sus valores no dependen de la distribución de los datos, a estos estadísticos se los conoce como no paramétricos.\nExisten otras estadísticos, llamados paramétricos, que, en determinadas condiciones, tienen ventajas frente a los no paramétricos. Los principales son la media aritmética, o simplemente, media, y la varianza.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#la-media-aritmética-un-ejemplo-de-cálculo.",
    "href": "050-estad-simple.html#la-media-aritmética-un-ejemplo-de-cálculo.",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "",
    "text": "Definición\nLa media aritmética de un conjunto de valores es el valor central que se obtiene al dividir la suma de todos los valores por la cantidad de valores. Es una medida de tendencia central que proporciona un punto de referencia para el conjunto de datos.\n\n\nDeducción de la Fórmula:\nSupongamos que tenemos un conjunto de \\(n\\) valores numéricos: \\(x_1, x_2, x_3, \\ldots, x_n\\).\n\nSuma de Todos los Valores: Primero, sumamos todos los valores del conjunto. Matemáticamente, esto se expresa como:\n\\[\nS = x_1 + x_2 + x_3 + \\cdots + x_n\n\\]\nCantidad de Valores: Luego, contamos cuántos valores hay en el conjunto. Este número es \\(n\\).\nDivisión de la Suma por la Cantidad de Valores: Finalmente, dividimos la suma total \\(S\\) por la cantidad de valores \\(n\\) para obtener la media aritmética:\n\\[\n\\text{Media Aritmética} = \\frac{S}{n}\n\\]\nExpresión General: Sustituyendo la suma \\(S\\) en la fórmula, tenemos:\n\\[\n\\text{Media Aritmética} = \\frac{x_1 + x_2 + x_3 + \\cdots + x_n}{n}\n\\]\n\n\n\nEjemplo:\nSupongamos que tenemos los siguientes valores: \\(5, 7, 9\\).\n\nSuma de los Valores: \\[\nS = 5 + 7 + 9 = 21\n\\]\nCantidad de Valores: \\[\nn = 3\n\\]\nCálculo de la Media Aritmética: \\[\n\\text{Media Aritmética} = \\frac{21}{3} = 7\n\\]\n\nEntonces, la media aritmética de los valores \\(5, 7, 9\\) es \\(7\\).\nLa media de una muestra se representa habitualmente mediante el símbolo\n\\[\\bar{x}\\] y, de una manera más formal, su valor se obtiene mediante la fórmula siguiente:\n\\[{\\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}}\\] El signo \\(\\sum\\) se conoce como sumatorio, e indica que ese término consiste en la suma de los \\(x\\) valores desde el primero hasta el valor \\(n\\). Expresado mediante una formulación matemática,\n\\[{\\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}x_{i}={\\frac {x_{1}+x_{2}+\\cdots +x_{n}}{n}}}\\] lo quiere quiere decir: “la suma de todos los valores observados dividido entre el número de estos valores”.\nLa media es lo que conocemos como un valor central, ya que representa el centro de nuestro conjunto de números. Como es el centro de nuestro conjunto de datos, la suma de las distancias de todos los valores a este valor central es \\(cero\\). Más adelante veremos la importancia de este hecho, al hablar de la dispersión y las formas de cálculo de la misma. Como hemos visto, la media de una muestra se representa como \\(\\bar{x}\\), mientras que la media de una población se representa con la letra griega mu: \\(\\mu\\). En ambos casos, el cálculo se realiza de forma idéntica.\nVolvamos a nuestro ejemplo de la altura de un grupo de alumnos, para realizar los cálculos según el modelo que hemos descrito. En nuestro caso, la altura media de nuestros alumnos (la media de nuestro conjunto de números) se calcula como:\n\\[\n\\bar{x} = \\frac{153+135+140+140+175+138+145+154+152+159+154}{11} = 149,54\n\\] Utilicemos una hoja de cálculo para guardar nuestros valores.\n\nLa fórmula para obtener la media en la hoja de cálculo, por ejemplo en la versión en español de Microsoft Excel, es =PROMEDIO(...), donde los puntos suspensivos deben sustituirse por el rango a calcular. En nuestro ejemplo, introduciríamos la fórmula en la celda B13como =PROMEDIO(B2..B12) (Para más detalles, verificar la hoja Excel adjunta).\nPara representar más cómodamente nuestros valores, dibujamos un punto a la altura de cada alumno,\n\ny eliminamos del gráfico los dibujos de nuestros alumnos; así hemos convertido nuestro dibujo en un diagrama de puntos:\n\nPara representar la media, aunque la media es un valor único, necesitamos añadir una columna a la derecha de nuestros datos, que rotulamos en la fila 1, celda C como altura media, e introducimos en cada una de las celdas desde C2hasta C12la fórmula del promedio, con el valor de nuestro rango de datos (Verificar hoja de cálculo). A continuación, designamos nuestro rango de datos para hacer un gráfico de puntos, y hacemos un zoom en los valores de manera que el eje Y se escale mejor entre los valores mínimo y máximo. Por último, hacemos unos ajustes en el formato para dibujar las líneas verticales que nos representan la distancia de cada valor a la media.\n\nSi verificamos el eje \\(Y\\) , veremos que en este gráfico hemos ajustado la escala respecto al gráfico anterior, situando el mínimo en \\(130\\). Esto permite visualizar las diferencias con mucha más claridad. Hemos representado la media \\(\\bar{x}\\) como una línea, y hemos dibujado unas líneas que unen cada valor individual con la media, que se sitúa en el valor \\(149,55\\), tal como calculamos más arriba.\nHemos representado la media como una serie de puntos unidos por una línea amarilla. Representamos en azul nuestros valores, uniendo cada valor con la línea media mediante una línea de puntos vertical. A partir de ahora, por conveniencia, eliminaremos los puntos en la linea media, dejando sólo la línea.\n\n\n\nHoja de cálculo con los valores y el gráfico de puntos\n\n\nEsta línea azul de puntos representa la distancia de cada valor a la media. Usaremos esta distancia para calcular una distancia media, que será una medida de la dispersión de nuestros valores.\nHemos visto que para describir un conjunto de números, en nuestro ejemplo, las medidas de la altura de un grupo de estudiantes, existe un valor, la media de este conjunto, que nos describe el centro de los valores. En nuestro ejemplo, si nuestro grupo tuviese un solo niño, éste tendría \\(149,55{\\ }cm\\) de altura.\n¿Es suficiente con este valor para describir el conjunto de valores? Vamos a ver que no: diferentes conjuntos de valores pueden proporcionar el mismo valor medio, y sin embargo los grupos pueden ser muy diferentes.\nVeamos un caso extremo. Comparemos dos grupos, uno formado por individuos iguales y otro formado por diez individuos iguales y uno distinto. Para ello usaremos nuestra hoja de cálculo:\n\n\n\n\nDos grupos de valores con la misma media\n\n¿Podemos describir adecuadamente los valores de la altura de cada uno de los grupos utilizando el valor medio? Parece evidente que no, ya que a partir de diferentes valores de altura estamos obteniendo el mismo valor medio. Sin embargo, uno de los grupos es más alto que el otro, si no fuera por un sólo individuo que aparentemente distorsiona el cálculo. Podríamos incluir nuestro grupo original, y veremos que los tres grupos son diferentes, aunque su valor medio es idéntico.\n\n\n\n\nTres grupos de valores con la misma media\n\nSi nos ayudamos de un gráfico equivalente al que hemos utilizado antes, vemos estas diferencias con claridad:\n\n\n\nGráfico de tres grupos de valores\n\n\nAunque el valor medio de estos tres grupos de datos es idéntico, parece claro que los tres grupos son muy distintos en su composición, y por lo tanto la media no es suficiente para describir con suficiente precisión cada uno de los grupos. Necesitamos un valor adicional, que nos indique de qué forma los valores se alejan del valor medio. Para ello, vamos a introducir un concepto nuevo: la medida de la dispersión.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#las-medidas-de-dispersión-la-varianza-y-la-desviación-típica",
    "href": "050-estad-simple.html#las-medidas-de-dispersión-la-varianza-y-la-desviación-típica",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "8.2 Las medidas de dispersión: la varianza y la desviación típica",
    "text": "8.2 Las medidas de dispersión: la varianza y la desviación típica\nComo hemos visto en el apartado anterior, diferentes conjuntos de datos pueden tener el mismo valor medio y sin embargo ser muy diferentes. En la última gráfica que hemos visto, el primer grupo se caracteriza por tener todos sus valores idénticos; el segundo tiene todos sus valores idénticos menos uno, que está muy apartado del resto, y el tercero tiene todos sus valores diferentes.\nAhora que conocemos cómo calcular un valor resumen de un conjunto de datos, podríamos utilizar una medida semejante para describir de qué forma en cada caso los valores se separan de la media. Podríamos utilizar una distancia media: calculamos las diferencias entre cada valor y la media, y hacemos su promedio: esto debería darnos una indicación de la magnitud de la separación de los valores en cada uno de los tres grupos.\nUsemos la hoja de cálculo para ello:\n\n\n\nTres grupos de valores en la hoja de cálculo\n\n\nAlgo parece que no está funcionando aquí: el promedio de las diferencias es cero en los tres casos; no podemos usar este cálculo para calcular la dispersión. Pero esto es esperable: ya que la media es un valor central, como hemos visto antes, la suma de las diferencias de todos los valores respecto de su media debe ser forzosamente cero, y esto es lo que estamos obteniendo.\nPara encontrar una solución, vamos a recurrir al viejo teorema de Pitágoras, que si recuerdas, nos dice que, en un triángulo rectángulo, el cuadrado de la hipotenusa es igual a la suma de los cuadrados de los catetos (una explicación gráfica muy divertida en el anexo …): \\[\nh^2= a^2+b^2\n\\] Esta fórmula es la base del cálculo de la distancia entre dos puntos:\n\n\n\n\nDistancia entre dos puntos\n\n\\[\nd(A,B)=\\sqrt{(x_2-x_1)^2+(y_2-y_1)^2}\n\\] Podemos adaptar esta fórmula al cálculo de nuestra distancia media. Como estamos calculando la distancia en una dimensión, sólo necesitamos la coordenada \\(X\\). Si tenemos en cuenta un solo punto, esta distancia \\(d\\) sería: \\[\n(d{\\ }del{\\ }valor{\\ }1{\\ }a{\\ }la{\\ }media)^2=(x_1-\\bar{x})^2\n\\] ¡El hecho de elevar al cuadrado las diferencias nos da la solución! Las diferencias negativas ya no son un problema porque sabemos que al elevar un numero negativo al cuadrado, el resultado es positivo; de esta manera conseguimos que las diferencias no se anulen. Ahora sí podemos calcular una distancia media \\(\\bar{d}\\) entre el conjunto de puntos y su media, calculando el promedio de las diferencias elevadas al cuadrado: \\[\n(\\bar{d}{\\ }de{\\ }los{\\ }n{\\ }valores{\\ }a{\\ }la{\\ }media)^2=\\frac{(x_1-\\bar{x})^2 + (x_2-\\bar{x})^2+\\cdots+(x_n-\\bar{x})^2}{n}\n\\]\ny utilizando la notación que hemos aprendido antes,\n\\[\n(\\bar{d}{\\ }de{\\ }los{\\ }n{\\ }valores{\\ }a{\\ }la{\\ }media)^2={\\frac {1}{n}}\\sum _{i=1}^{n}(x_{i}-\\bar{x})^2\n\\]\nAl igual que en el cálculo de la distancia entre dos puntos, sólo tenemos que extraer la raíz cuadrada de este valor para obtener la distancia media, que es el parámetro que estábamos buscando.\nLa distancia media \\[(\\bar{d}{\\ }de{\\ }los{\\ }n{\\ }valores{\\ }a{\\ }la{\\ }media)^2\\] se conoce en estadística como varianza, y su raíz cuadrada es lo que se conoce como desviación típica. La varianza de una población se representa en estadística con el signo de la letra griega sigma minúscula elevada al cuadrado, \\(\\sigma^2\\), y la desviación típica, mediante la letra \\(\\sigma\\). En el caso de una muestra, la varianza se representa como \\(s_x^2\\), y la desviación típica, como \\(s_x\\).\nEs importante resaltar que la desviación típica es una medida de la distancia media de los valores de una población a su media, y por lo tanto tiene dimensión, la misma que las medidas originales. La varianza, al estar elevada al cuadrado, no tiene una dimensión, o, mejor dicho, tiene la de la medida al cuadrado.\nCon estos nuevos hallazgos, recalculamos nuestra hoja de cálculo:\n\n\n\nTres grupos de valores en la hoja de cálculo, con la misma media y distinta desviación típica\n\n\nVamos a analizar con detalle esta tabla.\nEn la columna J tenemos nuestra población original de 11 alumnos, con las alturas que hemos medido. En la columna B hemos supuesto que todos los alumnos fuesen iguales, con la misma altura del valor medio de los datos originales. En la columna F hemos simulado otro grupo, con todos los valores iguales excepto uno, y con la misma media que los otros dos grupos.\nA la derecha de cada columna de medias, tenemos la columna de diferencias (columnas D, H y L), y en la fila 13, nuestro primer intento de calcular una dispersión media; intento fallido, puesto que obteníamos el valor \\(0\\) para los tres grupos.\nEn la siguiente columna a la derecha, para los tres grupos (columnas E, Iy M), hemos elevado al cuadrado la distancia de cada valor a la media, siguiendo los hallazgos que nos ha proporcionado el teorema de Pitágoras y la fórmula de la distancia entre dos puntos. En la fila 13 de estas columnas, calculamos el promedio de la distancia a la media al cuadrado: esta vez el resultado ya no es cero, sino que obtenemos el valor de la varianza, de acuerdo con la fórmula que hemos deducido más arriba. En la fila 14 (columnas B, F y J)utilizamos la fórmula de la hoja de cálculo para la varianza poblacional (más detalles posteriormente), y vemos que coincide exactamente con el promedio de las diferencias al cuadrado, tal como debe ser, ya que en eso consiste la fórmula que hemos deducido.\nPor último, en la fila 15calculamos la desviación típica de ambas formas, con la fórmula de la hoja de cálculo para la desviación típica poblacional (columnas B, Fy J), que Excel llama desviación estándar, y como la raíz cuadrada del promedio calculado antes (columnas E, Iy M). De nuevo, ambos valores coinciden exactamente, como esperamos.\n\n\n\nGráfico con tres conjuntos de datos con la misma media y diferente desviación típica\n\n\nAhora sí tenemos una forma más completa de describir nuestro conjunto de valores. Aunque el valor medio es el mismo en los tres casos, la dispersión de los valores es muy distinta.\n¿Son suficientes estos dos parámetros que hemos calculado para describir un conjunto de datos? La respuesta a esta pregunta es sí y no. La explicación es que, más allá de los valores numéricos que hemos obtenido, la visualización gráfica de los valores nos debe hacer reflexionar.\nEn el primer grupo, todos los valores son iguales a la media. La variación es cero. Son valores que hemos simulado en nuestra hoja de cálculo, pero difícilmente en el mundo real encontraremos una población en la que todos sus valores, en este caso, la altura de un grupo de alumnos, sean idénticos.\nEn el segundo grupo, todos los valores son idénticos, salvo uno, que se distancia mucho. ¿Debemos aceptar esto como bueno? En realidad, ¿es cierto que el valor medio de este grupo sea el mismo que el del primero? Para responder a esta pregunta debemos recurrir a nuestra experiencia, la estadística no nos da fórmulas mágicas. Pero, con un poco de sentido común, parece que el caso extremo que aparece en este grupo no es coherente con el resto de valores. Es lo que se llama un valor anormal o extraño (en inglés, outlier), y debe hacernos reflexionar sobre si el valor es correcto y realmente pertenece a esta población, o es un error de medida. O, simplemente, un valor que corresponde a otro grupo y que por error hemos situado en éste. La decisión de eliminar o no un valor anormal es una de las decisiones más complejas en estadística, que pueden tener una influencia enorme en la interpretación de los datos, y por lo tanto, hay que hacer con sumo cuidado. En este caso, extremo y artificial, el valor anormal debería ser eliminado, ya que, en realidad, todos los valores restantes son idénticos y más bajos que los del grupo 1. No tiene sentido lógico decir que sus medias son idénticas.\nEn el tercer grupo todos los valores son diferentes, y no podemos decir nada especial sobre sus valores individuales. Hay un valor que se destaca del resto, pero ¿podemos afirmar que es anormal? Seguramente, no con seguridad. De nuevo la experiencia debe indicarnos cómo proceder, aunque en este caso no tendría sentido eliminar este valor. En la situación real, todos conocemos a niños que han pegado el estirón antes que sus compañeros, y en algunos casos, pueden llegar a ser mucho más altos (o más bajos, si han tenido un retraso en este estirón) La experiencia nos dice que no es seguro que este valor sea realmente anormal, y por lo tanto, deberíamos conservarlo.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#las-limitaciones-de-la-media-y-la-desviación-típica",
    "href": "050-estad-simple.html#las-limitaciones-de-la-media-y-la-desviación-típica",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "8.3 Las limitaciones de la media y la desviación típica",
    "text": "8.3 Las limitaciones de la media y la desviación típica\nEn ocasiones nos enfrentamos a conjuntos de datos con valores de media y desviación típica idénticos o muy parecidos, pero que en realidad son muy diferentes. Veamos un ejemplo, semejante a los que hemos visto hasta ahora.\n\n\n\n\nHoja de cálculo con dos conjuntos de datos diferentes, con la misma media y desviación típica\n\n(cambiar a caso 3)\n\n\n\nDiagrama de puntos de las alturas de los alumnos, con indicación del valor medio\n\n\n\n\n\nDiagrama de puntos indicando la variabilidad\n\n\nEn este caso, vemos que tanto la media como la desviación típica son idénticos, y sin embargo los datos son muy diferentes, tal como nos muestra el gráfico de dispersión que hemos estado utilizando:\n\n\n\nGráfico con dos conjuntos de datos diferentes, con la misma media y desviación típica\n\n\nLa existencia de valores anormales o extremos muestra una de las debilidades de la media y la desviación típicas como descriptores de una población: ambos parámetros son muy sensibles a los casos extremos. En realidad, sólo deberíamos utilizar la media y la desviación típica para describir un conjunto de datos cuando estamos seguros de que la distribución de estos datos tienen una forma determinada, la de la campana de Gauss. En otros casos, la mediana y el rango intercuartil son estadísticos más robustos, y por lo tanto más seguros.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#resumen",
    "href": "050-estad-simple.html#resumen",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "8.4 Resumen",
    "text": "8.4 Resumen\nLas medidas paramétricas, como la media y la varianza, y las no paramétricas, como la mediana y el rango intercuartil, tienen diferentes ventajas e inconvenientes según el contexto y los datos con los que se trabaja.\n\nVentajas e inconvenientes de las medidas paramétricas\n\nVentajas\n\nPrecisión y Sensibilidad: La media y la varianza son muy precisas y sensibles a todos los valores del conjunto de datos.\nPropiedades Matemáticas: La media y la varianza tienen propiedades matemáticas deseables, como la facilidad para realizar operaciones algebraicas.\nDistribución Normal: Son especialmente útiles si los datos siguen una distribución normal, ya que permiten aprovechar las propiedades de esta distribución.\n\n\n\nInconvenientes\n\nSensibilidad a Valores Atípicos: La media y la varianza pueden ser distorsionadas significativamente por valores atípicos.\nRequieren Suposiciones: Su uso eficaz a menudo requiere que los datos sigan ciertas suposiciones, como la normalidad y la homogeneidad de la varianza.\n\n\n\n\nVentajas e inconvenientes de las medidas no paramétricas\n\nVentajas\n\nRobustez: La mediana y el rango intercuartil son menos sensibles a valores atípicos y distribuciones asimétricas.\nFlexibilidad: No requieren suposiciones fuertes sobre la distribución de los datos, lo que las hace útiles para una amplia variedad de distribuciones.\nResumir Datos: Son excelentes para resumir datos en situaciones en las que los valores extremos podrían distorsionar la interpretación.\n\n\n\nInconvenientes\n\nMenor Sensibilidad: La mediana y el rango intercuartil no utilizan toda la información de los datos y pueden ser menos sensibles a cambios en los datos.\nMenor Precisión en Ciertos Contextos: En situaciones donde los datos siguen una distribución normal, las medidas no paramétricas pueden ser menos precisas.\n\nLas medidas paramétricas son útiles para datos que siguen suposiciones específicas, como la normalidad, y son precisas y sensibles, pero pueden ser distorsionadas por valores atípicos. Las medidas no paramétricas son robustas y flexibles, ideales para distribuciones no normales y resistentes a valores atípicos, aunque pueden perder sensibilidad y precisión en ciertos contextos.\nModelo: práctica de puntos con un dado, dos dados, tres dados, etc hasta 30\nsuma &lt;- rowSums(replicate(30, sample(6, 10^6, replace=T))) length(suma) hist(suma)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#la-media-aritmética-como-centro-de-gravedad-de-un-grupo-de-datos",
    "href": "050-estad-simple.html#la-media-aritmética-como-centro-de-gravedad-de-un-grupo-de-datos",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "8.5 La media aritmética como centro de gravedad de un grupo de datos",
    "text": "8.5 La media aritmética como centro de gravedad de un grupo de datos\nPara ver cómo la media aritmética equivale al punto de equilibrio o centro de gravedad de un grupo de datos, imaginaremos que tenemos un conjunto de datos cuyos valores se distribuyen en un eje longitudinal X. Podemos hacer equivalentes estos valores a un conjunto de pesos que se distribuyen a lo largo de una barra, y vamos a suponer que existe un punto a una distancia \\(d_i\\) del origen de la barra en el cual dicha barra está en equilibrio. Para encontrar ese valor, empezaremos considerando el principio o ley de la palanca.\nEl Principio de la Palanca o la Ley de la Palanca fue formulada por el científico y matemático griego Arquímedes. Este principio dice que, en equilibrio, el producto de la fuerza aplicada (potencia) por su distancia al punto de apoyo (brazo de la potencia) es igual al producto de la resistencia por su distancia al punto de apoyo (brazo de la resistencia). Matemáticamente, se expresa como: \\[\nP \\cdot d_p = R \\cdot d_r\n\\]\nDonde: - \\(P\\) es la potencia o fuerza aplicada. - \\(d_p\\) es la distancia desde el punto de apoyo hasta el punto donde se aplica la potencia. - \\(R\\) es la resistencia o carga. - \\(d_r\\) es la distancia desde el punto de apoyo hasta el punto donde se aplica la resistencia.\n\nEjemplo\nSi tienes una palanca con una longitud de 5 metros y aplicas una fuerza de 10 Newtons a 1 metro del punto de apoyo, para mantener el equilibrio, la fuerza de resistencia en el otro extremo a 4 metros del punto de apoyo debería ser: \\[\nP \\cdot d_p = R \\cdot d_r\n\\] \\[\n10 \\, \\text{N} \\cdot 1 \\, \\text{m} = R \\cdot 4 \\, \\text{m}\n\\] \\[\nR = \\frac{10 \\cdot 1}{4} = 2,5 \\text{N}\n\\]\nAhora vamos a considerar el punto de equilibrio de una barra de la que se cuelgan diferentes pesos a diferente distancias de su origen.\nCalcular el punto de equilibrio de una barra de la que se cuelgan diferentes pesos a diferentes distancias de su origen es un problema clásico de física que se puede resolver usando el principio de momentos.\n\n\nPaso a Paso\n\nIdentificar las fuerzas: Supongamos que tienes varios pesos \\(W_1, W_2, W_3, \\ldots, W_n\\) colgados a distancias \\(d_1, d_2, d_3, \\ldots, d_n\\) del origen (punto de apoyo).\nCalcular los momentos: El momento (\\(M\\)) de un peso alrededor del punto de apoyo se calcula como el producto de la fuerza (peso) y la distancia al punto de apoyo: \\[\nM_i = W_i \\times d_i\n\\]\nSumar los momentos: Calcula la suma de todos los momentos: \\[\nM_{\\text{total}} = W_1 \\times d_1 + W_2 \\times d_2 + W_3 \\times d_3 + \\ldots + W_n \\times d_n\n\\]\nCalcular el peso total: Suma todos los pesos: \\[\nW_{\\text{total}} = W_1 + W_2 + W_3 + \\ldots + W_n\n\\]\nDeterminar el punto de equilibrio (\\(x\\)): El punto de equilibrio se encuentra dividiendo la suma de los momentos por el peso total: \\[\nx = \\frac{\\sum (W_i \\times d_i)}{\\sum W_i}\n\\]\n\n\n\nEjemplo Práctico\nSupongamos que tenemos tres pesos de 4, 9 y 1 kg, situados, respectivamente, a 2, 1 y 3 m respectivamente del origen de la barra.\n\n\\(W_1 = 4 \\, \\text{kg}\\) a \\(d_1 = 2 \\, \\text{m}\\)\n\\(W_2 = 9 \\, \\text{kg}\\) a \\(d_2 = 1 \\, \\text{m}\\)\n\\(W_3 = 1 \\, \\text{kg}\\) a \\(d_3 = 3 \\, \\text{m}\\)\n\n\nCalcular los momentos: \\[\nM_1 = 4 \\times 2 = 8 \\, \\text{kg·m}\n\\] \\[\nM_2 = 9 \\times 1 = 9 \\, \\text{kg·m}\n\\] \\[\nM_3 = 1 \\times 3 = 3 \\, \\text{kg·m}\n\\]\nSumar los momentos: \\[\nM_{\\text{total}} = 8 + 9 + 3 = 20 \\, \\text{kg·m}\n\\]\nCalcular el peso total: \\[\nW_{\\text{total}} = 4 + 9 + 1 = 14 \\, \\text{kg}\n\\]\nDeterminar el punto de equilibrio:\n\\[\nx = \\frac{20}{14} \\approx 1.43 \\, \\text{m}\n\\]\n\nEl punto de equilibrio se encuentra a aproximadamente 1.43 metros del origen.\n\n\nMedia Ponderada\nEn muchas situaciones, el centro de gravedad de un sistema de masas puede interpretarse como una media ponderada de las posiciones de las masas.\n\nMedia Aritmética\nPara un conjunto de números \\(x_1, x_2, \\ldots, x_n\\), la media aritmética es: \\[\n\\text{Media} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\n\n\nMedia Ponderada\nPara un conjunto de valores \\(x_1, x_2, \\ldots, x_n\\) con pesos asociados \\(w_1, w_2, \\ldots, w_n\\), la media ponderada es: \\[\n\\text{Media ponderada} = \\frac{w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n}{w_1 + w_2 + \\cdots + w_n}\n\\]\n\n\n\nCentro de Gravedad como Media Ponderada\nCuando calculamos el centro de gravedad (\\(x_{\\text{cg}}\\)) de un sistema de masas, estamos esencialmente calculando una media ponderada de las posiciones (\\(x_i\\)) de esas masas (\\(m_i\\)):\n\\[\nx_{\\text{cg}} = \\frac{\\sum (m_i \\cdot x_i)}{\\sum m_i}\n\\]\nAquí, las posiciones \\(x_i\\) son ponderadas por las masas \\(m_i\\).\n\n\nEjemplo Numérico\nEn nuestro ejemplo anterior,\n\nMasa 1: \\(m_1 = 4 \\, \\text{kg}\\) en posición \\(x_1 = 2 \\, \\text{m}\\)\nMasa 2: \\(m_2 = 9 \\, \\text{kg}\\) en posición \\(x_2 = 1 \\, \\text{m}\\)\nMasa 3: \\(m_3 = 1 \\, \\text{kg}\\) en posición \\(x_3 = 3 \\, \\text{m}\\)\n\nLa media aritmética de las posiciones sería: \\[\n\\text{Media} = \\frac{2 + 1 + 3}{3} = 2 \\, \\text{m}\n\\]\nEl punto de equlibrio de la barra, o centro de gravedad, calculado como media ponderada, sería:\n\\[\nx_{\\text{cg}} = \\frac{(4 \\cdot 2) + (9 \\cdot 1) + (1 \\cdot 3)}{4 + 9 + 1} = \\frac{8 + 9 + 3}{14} = \\frac{20}{14} \\approx 1,43 \\, \\text{m}\n\\] que es el mismo resultado que obteníamos formulando el cálculo de los momentos; la fórmula resulta ser idéntica.\n\n\nMedia Aritmética y Centro de Gravedad con Masas Idénticas\nEn los ejemplos anteriores, calculábamos el punto de equilibrio para diferentes pesos colocados a lo largo de una barra. Si en vez de eso suponemos que la distancia al origen de la barra equivale a nuestro eje \\(X\\), que recoge los valores de los que queremos calcular nuestra media aritmética, podemos eliminar el efecto de la masa suponiendo que todas las masas son iguales.\nSi las masas de los diferentes objetos son idénticas, podemos decir que la media aritmética de las posiciones coincide con el centro de gravedad. Esto se debe a que, en este caso, cada masa tiene el mismo peso o influencia en el cálculo del centro de gravedad. Esta es la explicación detallada:\n\nSuposición\nCada objeto tiene la misma masa \\(m\\).\n\n\nMedia Aritmética\nPara un conjunto de posiciones \\(x_1, x_2, \\ldots, x_n\\), la media aritmética es: \\[\n\\text{Media} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\n\n\nCentro de Gravedad\nEl centro de gravedad (\\(x_{\\text{cg}}\\)) para un conjunto de masas idénticas en posiciones \\(x_1, x_2, \\ldots, x_n\\) es: \\[\nx_{\\text{cg}} = \\frac{\\sum (m \\cdot x_i)}{\\sum m}\n\\]\nDado que las masas \\(m\\) son idénticas, el numerador se convierte en: \\[\n\\sum (m \\cdot x_i) = m \\cdot (x_1 + x_2 + \\cdots + x_n)\n\\]\nY el denominador se convierte en: \\[\n\\sum m = n \\cdot m\n\\]\nAl sustituir estos en la fórmula del centro de gravedad, obtenemos: \\[\nx_{\\text{cg}} = \\frac{m \\cdot (x_1 + x_2 + \\cdots + x_n)}{n \\cdot m}\n\\]\nAl simplificar, los términos \\(m\\) se cancelan, y nos queda: \\[\nx_{\\text{cg}} = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\] que es exactamente la fórmula de la media aritmética.\nSi repetimos nuestro ejemplo anterior, suponiendo en este caso masas idénticas\n\nMasa 1: \\(m_1 = 1 \\, \\text{kg}\\) en posición \\(x_1 = 2 \\, \\text{m}\\)\nMasa 2: \\(m_2 = 1 \\, \\text{kg}\\) en posición \\(x_2 = 1 \\, \\text{m}\\)\nMasa 3: \\(m_3 = 1 \\, \\text{kg}\\) en posición \\(x_3 = 3 \\, \\text{m}\\)\n\nLa media aritmética de las posiciones sería: \\[\n\\text{Media} = \\frac{2 + 1 + 3}{3} = 2 \\, \\text{cm}\n\\]\nEl punto de equlibrio de la barra, o centro de gravedad, calculado como media ponderada, sería:\n\\[\nx_{\\text{cg}} = \\frac{(1 \\cdot 2) + (1 \\cdot 1) + (1 \\cdot 3)}{1 + 1 + 1} = \\frac{2 + 1 + 3}{3} = \\frac{6}{3} = 2 \\, \\text{m}\n\\] que es el mismo resultado que obteníamos con la fórmula de la media aritmética de las distancias.\n\n\n\nConclusión\nCuando las masas son idénticas, la media aritmética de las posiciones coincide con el centro de gravedad. Esto nos permite afirmar que en el caso de una dimensión (por ejemplo, el peso) la media aritmética de un conjunto de valores coincide con el centro de gravedad o punto de equilibrio de ese conjunto de valores.Este es un resultado interesante que muestra la conexión entre conceptos estadísticos y físicos.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "050-estad-simple.html#importancia-del-concepto-de-la-media-como-centro-de-gravedad-o-punto-de-equilibrio-de-un-conjunto-de-datos",
    "href": "050-estad-simple.html#importancia-del-concepto-de-la-media-como-centro-de-gravedad-o-punto-de-equilibrio-de-un-conjunto-de-datos",
    "title": "8  Medidas centrales y medidas de dispersión: la media y la varianza.",
    "section": "8.6 Importancia del concepto de la media como centro de gravedad o punto de equilibrio de un conjunto de datos",
    "text": "8.6 Importancia del concepto de la media como centro de gravedad o punto de equilibrio de un conjunto de datos\nEfecto de los outliers: desequilibrio, desplazamiento de la media, incremento de la varianza (gan diferencia), mientras que la mediana y el rango intercuartil no se ven afectados",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Medidas centrales y medidas de dispersión: la media y la varianza.</span>"
    ]
  },
  {
    "objectID": "060-forma-de-los-datos.html",
    "href": "060-forma-de-los-datos.html",
    "title": "9  La forma de los datos",
    "section": "",
    "text": "Introducción\nEn estadística, comprender la forma de los datos es fundamental para interpretar correctamente la distribución de una variable. El análisis de la forma nos permite identificar patrones, tendencias y anomalías en los datos que podrían pasar desapercibidos si solo nos centramos en las estadísticas descriptivas como la media o la mediana.\nLa distribución de los datos describe cómo se distribuyen los valores de una variable en un conjunto de datos. Al analizar la forma de una distribución, podemos observar aspectos clave como la asimetría, la curtosis, y la presencia de valores atípicos. Estos elementos nos dan pistas sobre la simetría de la distribución (si los datos están equilibrados a ambos lados de la mediana), la concentración de los datos (si los datos están agrupados en torno a un valor central), y si hay valores extremos que podrían estar afectando los resultados.\nEn este capítulo, vamos a explorar cómo analizar la forma de los datos utilizando herramientas visuales como histogramas y boxplots, que son fundamentales para identificar las características de la distribución. Para hacerlo, vamos a partir de un ejemplo práctico que ilustra cómo llevar a cabo este análisis paso a paso, usando tanto Excel como R.\n\n\nEjemplo Práctico: Análisis de los Tiempos de Entrega en una Empresa de Reparto\nImaginemos que una empresa de reparto desea analizar los tiempos de entrega de 50 paquetes en un solo día. Los tiempos se han registrado en minutos, y el objetivo es evaluar la eficiencia de las entregas a través de un análisis de la distribución de los datos.\nLos datos que tenemos son los siguientes:\n[30, 28, 32, 35, 31, 29, 27, 33, 34, 36, 40, 45, 50, 42, 38, 37, 39, 41, 60, 62, 65, 29, 28, 30, 33, 35, 55, 58, 32, 31, 30, 30, 31, 27, 28, 29, 34, 35, 33, 32, 70, 72, 75, 29, 28, 30, 65, 68, 40, 41]\nA través de este conjunto de datos, vamos a analizar la forma de los datos y aprender a interpretar las distribuciones. Exploraremos la simetría, la dispersión, la curtosis, y los posibles valores atípicos que podrían estar presentes.\nEste análisis lo realizaremos utilizando dos herramientas principales: Excel y R, las cuales nos permitirán representar visualmente los datos a través de histogramas y boxplots, y extraer conclusiones importantes sobre la distribución de los tiempos de entrega.\n\n\n\nFlujo de trabajo para el análisis:\n\nDistribución de frecuencia y histogramas:\nVeremos cómo usar Excel para calcular la frecuencia de los datos en intervalos y crear un histograma que nos permita identificar patrones de simetría o asimetría.\nBoxplot:\nAprenderemos a usar boxplots en Excel y R para identificar la mediana, los cuartiles y los valores atípicos. Esto nos ayudará a ver si algunos de los tiempos de entrega son inusualmente largos.\nInterpretación de la forma de los datos:\nAl analizar los resultados, podremos determinar si la distribución es asimétrica o simétrica, si hay valores atípicos que podrían estar influyendo en la media y cómo estos factores pueden afectar la interpretación de los tiempos de entrega en la empresa.\n\n\n\n\n1. Análisis con Excel\n\nPaso 1: Crear una Tabla de Frecuencia\nEn Excel, primero introducimos los datos en una columna. A continuación, creamos una tabla de frecuencias agrupando los datos en intervalos o clases, utilizando la función FRECUENCIA(). Por ejemplo, podemos elegir los siguientes intervalos:\n25-30, 30-35, 35-40, 40-45, 45-50, 50-55, 55-60, 60-65, 65-70.\nUna vez que tenemos las frecuencias, podemos representar visualmente los datos en un histograma, lo cual nos permitirá ver la distribución de los tiempos.\n\n\nPaso 2: Crear un Histograma\nPara generar un histograma, seleccionamos los datos y usamos el gráfico de histograma disponible en las opciones de gráficos de Excel. En este histograma, podemos observar la forma general de la distribución, su asimetría (si la cola se extiende más a la derecha o a la izquierda) y si es simétrica.\n\n\nPaso 3: Crear un Boxplot\nLuego, seleccionamos los datos y creamos un boxplot en Excel. Este gráfico muestra la mediana, los cuartiles y los valores atípicos (puntos fuera de los bigotes del gráfico). En este caso, los tiempos superiores a 65 minutos se mostrarían como posibles valores atípicos.\n\n\nInterpretación de los Resultados en Excel:\n\nSimetría: Si el histograma tiene una forma similar a una campana (simétrica), indicaría que los datos siguen una distribución normal. Si el histograma tiene una cola más larga hacia la derecha, los datos tienen asimetría positiva (sesgo a la derecha).\nDispersión y Concentración: El boxplot muestra si los tiempos están muy dispersos o si hay un rango reducido de tiempos de entrega. Si hay muchos valores en la parte inferior del gráfico, indica que la mayoría de las entregas se realizan rápidamente, mientras que los valores atípicos (más de 65 minutos) indican que algunas entregas se están demorando considerablemente.\n\n\n\n\n\n2. Análisis con R\n\nPaso 1: Cargar los Datos en R\nPrimero, cargamos los datos en R, utilizando un vector de los tiempos de entrega:\ntiempos &lt;- c(30, 28, 32, 35, 31, 29, 27, 33, 34, 36, 40, 45, 50, 42, 38, 37, 39, 41, 60, 62, 65, 29, 28, 30, 33, 35, 55, 58, 32, 31, 30, 30, 31, 27, 28, 29, 34, 35, 33, 32, 70, 72, 75, 29, 28, 30, 65, 68, 40, 41)\n\n\nPaso 2: Crear un Histograma\nUsamos el comando hist() para crear el histograma:\nhist(tiempos, main=\"Histograma de Tiempos de Entrega\", xlab=\"Tiempo (minutos)\", col=\"lightblue\", border=\"black\", breaks=10)\nEste histograma nos permite visualizar si los datos están distribuidos de manera simétrica o si presentan un sesgo. En este caso, podemos observar si hay una asimetría positiva, lo que indicaría que la mayoría de las entregas son rápidas, pero algunas se demoran considerablemente.\n\n\nPaso 3: Crear un Boxplot\nPara crear el boxplot en R, utilizamos el comando boxplot():\nboxplot(tiempos, main=\"Boxplot de Tiempos de Entrega\", ylab=\"Tiempo (minutos)\", col=\"lightgreen\")\nEl boxplot nos mostrará la mediana, los cuartiles y los valores atípicos. Los tiempos de entrega superiores a 65 minutos serán identificados como valores atípicos.\n\n\nPaso 4: Evaluar la Simetría y Curtosis\nPara analizar la simetría de la distribución, calculamos la asimetría (skewness) y la curtosis (kurtosis):\nlibrary(e1071)\nskewness(tiempos)   # Asimetría\nkurtosis(tiempos)   # Curtosis\nUn valor de asimetría positiva indica que la distribución está sesgada hacia la derecha (más tiempos rápidos y unos pocos tiempos de entrega muy largos). La curtosis nos dará una idea de cuán concentrados o dispersos están los datos en torno a la mediana.\n\n\nPaso 5: Identificar los Valores Atípicos\nFinalmente, podemos obtener los valores atípicos con el siguiente código:\nboxplot(tiempos)$out\nEsto nos mostrará los valores que se encuentran fuera de los bigotes del boxplot, los cuales son posibles valores atípicos.\n\n\nInterpretación de los Resultados en R:\n\nSimetría: Si la asimetría es positiva, los tiempos de entrega son más rápidos para la mayoría de los paquetes, pero hay algunos muy retrasados. Si la asimetría es cercana a cero, los datos son aproximadamente simétricos.\nCurtosis: Una curtosis alta indicaría que la mayoría de los datos están agrupados cerca de la mediana, con pocos valores extremos. Una curtosis baja indicaría que los datos están más dispersos.\nValores Atípicos: Los valores atípicos (&gt;65 minutos) pueden representar casos extremos que deberían ser investigados, ya que podrían indicar problemas en el proceso de entrega.\n\n\n\n\n\nConclusiones\nEl análisis de la forma de los datos a través de histogramas y boxplots permite identificar patrones clave como la asimetría, la dispersión y la presencia de valores atípicos. Al interpretar estos gráficos, podemos obtener información valiosa sobre la eficiencia del proceso de reparto y detectar áreas que necesiten mejoras, como la gestión de entregas con tiempos excesivos.\n\nEste ejemplo completo, con análisis en Excel y R, ofrece una manera de aplicar herramientas estadísticas para entender la distribución y la forma de los datos en situaciones prácticas.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>La forma de los datos</span>"
    ]
  },
  {
    "objectID": "070-relacion-xy.html",
    "href": "070-relacion-xy.html",
    "title": "10  La relación entre las variables",
    "section": "",
    "text": "10.1 Diagramas de puntos \\((x,y\\))",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>La relación entre las variables</span>"
    ]
  },
  {
    "objectID": "070-relacion-xy.html#correlación",
    "href": "070-relacion-xy.html#correlación",
    "title": "10  La relación entre las variables",
    "section": "10.2 Correlación",
    "text": "10.2 Correlación\n\nCorrelación y causalidad\nAnscombe",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>La relación entre las variables</span>"
    ]
  },
  {
    "objectID": "070-relacion-xy.html#tablas-de-asociación",
    "href": "070-relacion-xy.html#tablas-de-asociación",
    "title": "10  La relación entre las variables",
    "section": "10.3 Tablas de asociación",
    "text": "10.3 Tablas de asociación",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>La relación entre las variables</span>"
    ]
  },
  {
    "objectID": "080-comunicacion.html",
    "href": "080-comunicacion.html",
    "title": "11  La comunicación de los resultados",
    "section": "",
    "text": "quarto markdown PowerPoint Gráficos",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>La comunicación de los resultados</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html",
    "href": "120-control-proc.html",
    "title": "12  El control estadístico de procesos",
    "section": "",
    "text": "12.1 La mejora de la calidad y el control estadístico de procesos\nEl control estadístico de la calidad empezó con W.E. Deming a mediados del pasado siglo XX, y fueron las empresas japonesas, sobre todo las automovilísticas, las que inicialmente recogieron el testigo de Deming y aplicaron estos principios a la mejora de la producción industrial, difundiendo su conocimiento a todos los niveles jerárquicos de las organizaciones, desde los operarios de línea hasta los más altos directivos. Desde ese momento hasta la difusión actual de los métodos Six Sigma gracias a General Electric y Motorola, la mejora industrial de los procesos ha estado siempre apoyada en la correcta utilización de estas metodologías.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html#introducción-a-los-gráficos-de-control",
    "href": "120-control-proc.html#introducción-a-los-gráficos-de-control",
    "title": "12  El control estadístico de procesos",
    "section": "12.2 Introducción a los gráficos de control",
    "text": "12.2 Introducción a los gráficos de control\n\nCausas comunes y causas especiales de variación\n\n\nVariación a corto plazo y variación a largo plazo",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html#la-capacidad-de-un-proceso",
    "href": "120-control-proc.html#la-capacidad-de-un-proceso",
    "title": "12  El control estadístico de procesos",
    "section": "12.3 La capacidad de un proceso",
    "text": "12.3 La capacidad de un proceso",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "120-control-proc.html#ejemplo-establecer-las-especificaciones-de-un-producto",
    "href": "120-control-proc.html#ejemplo-establecer-las-especificaciones-de-un-producto",
    "title": "12  El control estadístico de procesos",
    "section": "12.4 Ejemplo: establecer las especificaciones de un producto",
    "text": "12.4 Ejemplo: establecer las especificaciones de un producto",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>El control estadístico de procesos</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html",
    "href": "130-sistema-medicion.html",
    "title": "13  El análisis del sistema de medición",
    "section": "",
    "text": "13.1 ¿Qué es una medida?\nUna medida es el resultado de la acción de medir. Normalmente, medir quiere decir comparar lo que va a ser medido con un patrón de referencia; esta comparación es realizada por una o varias personas que llamaremos analistas, los cuales utilizarán un método analítico, siguiendo un procedimiento de medida. Por ejemplo, en la imagen a continuación, la analista está midiendo la altura de un niño utilizando un instrumento de medida, una cinta métrica.\nHasta aquí parece que todo está suficientemente claro, por lo que el resultado de la medida debe ser un valor que no nos ofrecerá dudas sobre su veracidad. Sin embargo, si analizamos el proceso con atención, veremos que hay algunos elementos que pueden hacer que nuestro resultado no sea todo lo preciso que habíamos pensado. Por ejemplo, en la imagen no vemos si el niño está calzado o no. Es evidente que deberíamos decir al analista que el niño debe estar descalzo, ya que diferentes tipos de calzado podrían alterar el resultado de formas diferentes. EL niño podría estar ligeramente encorvado, o sus rodillas dobladas, y entonces la altura que estamos midiendo será menor de la altura real. Además de esto, la forma de realizar la evaluación de la altura se ve influenciada por la posición de los ojos del analista: si están demasiado bajos, no verá correctamente la parte superior de la cabeza y tenderá a sobreestimar el verdadero valor por un efecto de perspectiva. Por otra parte, no sabemos si el instrumento utilizado tiene una escala de medida construida de forma fiable o sólo aproximada. Si nuestro instrumento (la cinta métrica) no es fiable, o su escala difiere de la de otros instrumentos semejantes, es posible que el valor de la medida varíe.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#qué-es-una-medida",
    "href": "130-sistema-medicion.html#qué-es-una-medida",
    "title": "13  El análisis del sistema de medición",
    "section": "",
    "text": "Cinta métrica de sastre\n\n\n\n\nMetro de albañilería",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#los-estándares-de-medida",
    "href": "130-sistema-medicion.html#los-estándares-de-medida",
    "title": "13  El análisis del sistema de medición",
    "section": "13.2 Los estándares de medida",
    "text": "13.2 Los estándares de medida\nPor lo que hemos visto, cuando hacemos una medida, debemos establecer un procedimiento de medida, que debe indicar al analista cual es la forma correcta de realizar los pasos para hacer que la medida sea veraz. Deberemos definir también el instrumento de medida, de manera que cuando se repita el procedimiento no se introduzca un factor de variación debido al uno de un instrumento inapropiado. LO mejor es que este instrumento disponga de una homologación por un servicio de homologación externo, que nos asegure, por ejemplo, que los intervalos de medida de que dispone se corresponden con valores de referencia, en este caso, centímetros y milímetros. El procedimiento debe establecer también con claridad las condiciones en que las debe estar el objeto a medir (la persona, en este caso): descalzo, perfectamente estirado, con sus rodillas rectas, etc. Seguramente, el procedimiento incluirá un dibujo para que el analista visualice con claridad los puntos claves que debe revisar para hacer una buena medida. En el dibujo a continuación, se indican algunos de estos puntos claves, incluyendo la necesidad de que el niño se apoye en un plano vertical (la pared) y que el analista se situe correctamente para que su vista sea perpendicular al plano, utilizando una guía para la valoración correcta de la altura medida.\n\nPero ¿y si la niña tiene una altura tal que el analista no puede mantener una posición estable? El procedimiento real puede llegar a ser mucho más complejo, tal como vemos en el último gráfico, que proviene de un documento médico de la Organización Mundial de la Salud, en donde la correcta estimación del peso y altura de los niños es fundamental para determinar su estado de salud nutricional, y por lo tanto es necesario minimizar el riesgo de errores de medida, garantizando que todos los analistas realizan correctamente el mismo procedimiento aunque estén en diferentes ubicaciones y en momentos diferentes:\n\nEL procedimiento de la OMS ha estimado que es necesario que sean dos los analistas que realizan la medida, utilizando un aparato especialmente diseñado para ello, y que utiliza una pieza para ajustar a la cabeza de manera que la posición de lectura no esté sometida al error de la posición del analista que realiza la medida.\nLa descripción del procedimiento de análisis, junto con el detalle de los instrumentos necesarios y sus homologaciones requeridas, constituye lo que se conoce como método analítico. El analista o analistas deberán estudiar este método analítico para estar seguros de que son capaces de llevarlo a la práctica sin error.\nVeremos en un apartado posterior que cada uno de estos elementos dan lugar a un tipo de error concreto, que son principalmente dos: los debidos al analista y los debidos al procedimiento (incluyendo aquí los errores instrumentales) Veremos cómo evaluar la magnitud de cada uno de estos errores y la forma de establecer un plan de trabajo para reducirlos, mejorando así la calidad de nuestras medidas.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#la-precisión-analítica",
    "href": "130-sistema-medicion.html#la-precisión-analítica",
    "title": "13  El análisis del sistema de medición",
    "section": "13.3 La precisión analítica",
    "text": "13.3 La precisión analítica",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#análisis-de-repetibilidad-y-reproducibilidad-grr-analysis",
    "href": "130-sistema-medicion.html#análisis-de-repetibilidad-y-reproducibilidad-grr-analysis",
    "title": "13  El análisis del sistema de medición",
    "section": "13.4 Análisis de repetibilidad y reproducibilidad (GR&R analysis)",
    "text": "13.4 Análisis de repetibilidad y reproducibilidad (GR&R analysis)\n\nRound only on the final calculation result[edit]\nWhen performing multiple stage calculations, do not round intermediate stage calculation results; keep as many digits as is practical (at least one more digit than the rounding rule allows per stage) until the end of all the calculations to avoid cumulative rounding errors while tracking or recording the significant figures in each intermediate result. Then, round the final result, for example, to the fewest number of significant figures (for multiplication or division) or leftmost last significant digit position (for addition or subtraction) among the inputs in the final calculation.[15]\n\n(2.3494 + 1.345) × 1.2 = 3.6944 × 1.2 = 4.43328 ≈ 4.4.\n(2.3494 × 1.345) + 1.2 = 3.159943 + 1.2 = 4.359943 ≈ 4.4.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "130-sistema-medicion.html#precision-of-measuring-tools-and-significant-figures",
    "href": "130-sistema-medicion.html#precision-of-measuring-tools-and-significant-figures",
    "title": "13  El análisis del sistema de medición",
    "section": "13.5 Precision of Measuring Tools and Significant Figures",
    "text": "13.5 Precision of Measuring Tools and Significant Figures\nAccuracy, Precision, and Significant Figures | Physics (lumenlearning.com)\nAn important factor in the accuracy and precision of measurements involves the precision of the measuring tool. In general, a precise measuring tool is one that can measure values in very small increments. For example, a standard ruler can measure length to the nearest millimeter, while a caliper can measure length to the nearest 0.01 millimeter. The caliper is a more precise measuring tool because it can measure extremely small differences in length. The more precise the measuring tool, the more precise and accurate the measurements can be.\nWhen we express measured values, we can only list as many digits as we initially measured with our measuring tool. For example, if you use a standard ruler to measure the length of a stick, you may measure it to be 36.7 cm. You could not express this value as 36.71 cm because your measuring tool was not precise enough to measure a hundredth of a centimeter. It should be noted that the last digit in a measured value has been estimated in some way by the person performing the measurement. For example, the person measuring the length of a stick with a ruler notices that the stick length seems to be somewhere in between 36.6 cm and 36.7 cm, and he or she must estimate the value of the last digit. Using the method of significant figures, the rule is that the last digit written down in a measurement is the first digit with some uncertainty. In order to determine the number of significant digits in a value, start with the first measured value at the left and count the number of digits through the last digit written on the right. For example, the measured value 36.7cm has three digits, or significant figures. Significant figures indicate the precision of a measuring tool that was used to measure a value.\n\nLa “mano del quesero”\nComparar pros y contras de la práctica basada en la experiencia con la práctica basada ne el método y la cuantificación\n\nsubjetividad\npérdida de conocimiento si el experto deja la empresa\n\n\n\nMétodo cientifico\nAlgoritmos - recetas cocina- DMAIC - método cientifico\nMontgomery 1.1\nLa reproducibilidad de los análisis de datos\nLiterate programming - Wikipedia\nReproducible Research (hbiostat.org)\nrr (hbiostat.org)\nEn el mundo científico y técnico cada vez cobra más importancia el concepto de reproducibilidad de los análisis, sobre todo cuando se trata de comunicar o publicar el resultado de un trabajo o de una investigación. Medios, como la prestigiosa revista Science, se han hecho eco de ello (Buck 2015). Por otra parte, la utilización de un flujo de trabajo basado en hojas de cálculo hace difícil garantizar esta reproducibilidad, y a veces puede llevar a cometer errores de consecuencias graves (Ferrero 2018; Ryssdal 2013).\nJesse Sadler (Sadler 2017) lo explica así:\nEl peligro de la hoja de cálculo deriva de su propia estructura. La mezcla de entrada de datos, análisis y visualización hace que sea fácil confundir las celdas que contienen datos sin procesar con las que son el resultado del análisis. La forma de definir la lógica programática, tal como la selección de qué celdas se van a sumar, mediante clics del mouse, significa que una acción errónea de clic o arrastre puede provocar errores o la sobreescritura de datos. Solo hace falta pensar en el pavor del momento en el que vas a cerrar una hoja de cálculo y el programa te pregunta si te gustaría guardar los cambios. Te hace preguntarte. ¿Quiero guardar? ¿Qué cambios hice? Debido a que la lógica en una hoja de cálculo se realiza a través de clics del mouse, no hay forma de rastrear de manera efectiva qué cambios se han realizado en una sesión o en la producción de un gráfico. Los errores cometidos con Excel pueden tener consecuencias graves, como se puso de manifiesto tras la controversia alrededor del artículo de Carmen Reinhart y Kenneth Rogoff sobre la deuda nacional de los EEUU.\nCiertamente hay razones legítimas por las que las personas usan por defecto hojas de cálculo para el análisis de datos en lugar de usar un lenguaje de programación como R. Las hojas de cálculo son mucho más atractivas y confortables de lo que cualquier lenguaje de programación podría ser para un recién llegado. Aprender a programar es intimidante y no es algo que se pueda hacer rápida o fácilmente. Las aplicaciones de interfaz gráfica de usuario (GUI) son mucho menos desalentadoras que una interfaz de línea de comandos. En segundo lugar, las hojas de cálculo son una buena herramienta para la entrada de datos, y es tentador pasar directamente al análisis de datos, manteniendo todo en el mismo documento. Finalmente, la naturaleza interactiva de las hojas de cálculo y la capacidad de crear gráficos que cambian en función de las entradas es muy atractiva, incluso si desbloquear completamente este potencial implica un conocimiento bastante complejo sobre cómo funciona el programa. La primera ventaja de las hojas de cálculo sobre la programación no se supera fácilmente, pero las dos últimas se basan en lo que creo que es un flujo de trabajo problemático. En lugar de usar un par de aplicaciones monolíticas, a menudo un conjunto de aplicaciones de oficina, para hacer todo, creo que es mejor dividir el flujo de trabajo entre varias aplicaciones que hacen una cosa bien.\nCrear una división clara entre la entrada y el análisis de datos es una de las principales razones por las que el análisis de datos en un lenguaje de programación es preferible al software de hoja de cálculo. Todavía uso hojas de cálculo, pero su limito su uso estrictamente a la entrada de datos. En un programa de hoja de cálculo, el análisis manipula directamente la única copia de los datos sin procesar. Por el contrario, con R se importan los datos, creando un objeto que es una copia de los datos sin procesar. Todas las manipulaciones de los datos se realizan en esta copia, y los datos originales nunca se alteran de ninguna manera. Esto significa que no hay forma de estropear los datos sin procesar. La manipulación de una copia de los datos le permite experimentar más libremente. Los errores son intrascendentes, incluso aunque a veces puedan llegar a ser frustrantes. Una línea de código que devuelve un error se puede ajustar y volver a ejecutar, repitiendo el proceso las veces necesarias hasta que se devuelva el resultado esperado.\nTrabajar en una copia de los datos sin procesar puede incluso simplificar el proceso de entrada de datos. El análisis de datos tabulares en R da como resultado la creación de múltiples objetos, que se conocen como data frames y pueden considerarse equivalentes a tablas en una hoja de cálculo. La capacidad de dividir, muestrear y transformar el conjunto de datos original en muchos data frames diferentes tiene la ventaja de reducir drásticamente la complejidad de la entrada de datos. En lugar de necesitar hojas de cálculo a medida con múltiples hojas y tablas interrelacionadas, cada pieza de datos solo debe ingresarse una vez y todas las manipulaciones se pueden realizar en el código. Los diferentes data frames que se crean en el proceso de análisis ni siquiera tienen que ser guardados, porque son muy fácilmente reproducidos por el script de código.\nLa separación de la entrada y el análisis de los datos reduce en gran manera el potencial de errores, pero tal vez aún más significativamente, el uso de código para el análisis de datos permite la creación de investigaciones reproducibles que no son posibles en hojas de cálculo. […] Con un lenguaje de programación, los pasos del análisis se pueden establecer claramente en el código […] Guardar el análisis en código tiene el beneficio inmediato de que se puede volver a ejecutar fácilmente en cualquier momento que se agreguen nuevos datos. El código también se puede aplicar a un conjunto de datos completamente nuevo de una manera mucho más transparente que con las hojas de cálculo. El beneficio a largo plazo es que con el código todo el análisis se documenta en lugar de ocultarse detrás de los clics del mouse. Esto hace que sea más fácil revisar los propios análisis mucho después de haber terminado con ellos, así como que otros entiendan lo que se ha hecho y comprueben si hay errores.\n\n\n\n\nBuck, Stuart. 2015. “Solving Reproducibility.” Science 348 (6242): 1403. https://www.science.org/doi/full/10.1126/science.aac8041.\n\n\nFerrero, Rosana. 2018. “Los Errores de Reinhart & Rogoff: R y La Reproducibilidad.” 2018. https://www.maximaformacion.es/blog-dat/los-errores-de-reinhart-rogo/.\n\n\nRyssdal, Karl. 2013. “The Excel Mistake Heard Round the World.” 2013. https://www.marketplace.org/2013/04/17/economy/excel-mistake-heard-round-world/.\n\n\nSadler, Jesse. 2017. “Excel Vs r: A Brief Introduction to r.” 2017. https://www.jessesadler.com/post/excel-vs-r/.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>El análisis del sistema de medición</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html",
    "href": "140-mejora-procesos.html",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "",
    "text": "14.1 Six Sigma y mejora de la calidad",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#definir-un-problema-opex-lean-sixsigma-16",
    "href": "140-mejora-procesos.html#definir-un-problema-opex-lean-sixsigma-16",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.2 Definir un problema [OPEX Lean SixSigma #16]",
    "text": "14.2 Definir un problema [OPEX Lean SixSigma #16]\n\n¿Cómo es una buena definición de un problema?\n\nBreve\nEvitar lenguaje técnico: debes ser capaz de explicarlo a cualquier persona de la organización usando términos sencillos\nCuantificar el problema, usando los datos disponibles\nIntegra y explica el coste real del problema, para justificar la necesidad del análisis. Puedes relacionarlo con los costes de no calidad\nDefine el ámbito del problema: usa los términos que sean necesarios para delimitarlo con precisión.\nLa definición de un problema debe conseguir formularlo de forma que sea específico, medible, realizable, relevante y acotado en el tiempo (plazo)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#estrategia-de-resolucion-de-problemas",
    "href": "140-mejora-procesos.html#estrategia-de-resolucion-de-problemas",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.3 Estrategia de resolucion de problemas",
    "text": "14.3 Estrategia de resolucion de problemas",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#dmaic---sixsigma",
    "href": "140-mejora-procesos.html#dmaic---sixsigma",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.4 DMAIC - SixSigma",
    "text": "14.4 DMAIC - SixSigma",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "140-mejora-procesos.html#el-papel-de-los-métodos-estadísticos-en-la-mejora-six-sigma",
    "href": "140-mejora-procesos.html#el-papel-de-los-métodos-estadísticos-en-la-mejora-six-sigma",
    "title": "14  Six Sigma y la mejora de los procesos.",
    "section": "14.5 El papel de los métodos estadísticos en la mejora Six Sigma",
    "text": "14.5 El papel de los métodos estadísticos en la mejora Six Sigma",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>*Six Sigma* y la mejora de los procesos.</span>"
    ]
  },
  {
    "objectID": "160-bibliografia.html",
    "href": "160-bibliografia.html",
    "title": "Bibliografía",
    "section": "",
    "text": "Buck, Stuart. 2015. “Solving Reproducibility.”\nScience 348 (6242): 1403. https://www.science.org/doi/full/10.1126/science.aac8041.\n\n\nFerrero, Rosana. 2018. “Los Errores de Reinhart & Rogoff: R y\nLa Reproducibilidad.” 2018. https://www.maximaformacion.es/blog-dat/los-errores-de-reinhart-rogo/.\n\n\nHadley Wickham, Garret Grolemund. 2023. “R Para Ciencia de\nDatos.” 2023. https://es.r4ds.hadley.nz/.\n\n\nHadley Wickham, Garret Grolemund, Mine Çetinkaya-Rundel. 2023. R for\nData Science, 2nd Ed. 1005 Gravenstein Highway North, Sebastopol,\nCA95472: O’Reilly Media Inc. https://r4ds.hadley.nz/.\n\n\nRyssdal, Karl. 2013. “The Excel Mistake Heard Round the\nWorld.” 2013. https://www.marketplace.org/2013/04/17/economy/excel-mistake-heard-round-world/.\n\n\nSadler, Jesse. 2017. “Excel Vs r: A Brief Introduction to\nr.” 2017. https://www.jessesadler.com/post/excel-vs-r/.\n\n\nWorld Economic Forum. 2025. “Future of Jobs Report, 2025.”\n91-93 route de la Capite,CH-1223 Cologny/Geneva, Switzerland: World\nEconomic Forum. https://www.weforum.org/publications/the-future-of-jobs-report-2025/.",
    "crumbs": [
      "Anexos",
      "Bibliografía"
    ]
  },
  {
    "objectID": "040-exploracion.html#explorando-los-datos",
    "href": "040-exploracion.html#explorando-los-datos",
    "title": "7  La exploración de los datos mediante gráficos.",
    "section": "",
    "text": "El diagrama de tallo y hojas (stem and leaf plot o stemplot)\nEl diagrama de tallo y hojas, también conocido como stemplot, es una herramienta gráfica utilizada en estadística para representar la distribución de un conjunto de datos. Es especialmente útil para conjuntos de datos pequeños y proporciona una forma rápida y efectiva de visualizar la forma de los datos y su dispersión. El stemplot recibe este nombre porque el dibujo que resulta se asemeja a un tallo el que le salen las hojas que son los datos individuales.\nLos componentes de un stemplot son:\n\nTallo: Representa el grupo principal de los valores de los datos. Generalmente, se usa la parte más significativa del número. Por ejemplo, en el número 43, el tallo podría ser 4.\nHojas: Representan los dígitos finales o menos significativos de los valores de los datos. Siguiendo el ejemplo anterior, la hoja sería 3.\n\n\nConstrucción del diagrama en la hoja de cálculo\nSupongamos que queremos medir la altura de un grupo de alumnos de nuestra clase. Éste es nuestro grupo:\n\nRealizamos la medida de altura de cada persona y registramos los valores en una hoja de cálculo, siguiendo las buenas prácticas que hemos visto al estudiar los datos ordenados.\n\nVamos a utilizar los datos de medidas de altura de nuestro grupo de alumnos. Quitamos el último dígito a la derecha de nuestros valores y colocamos verticalmente los valores resultantes ordenándolos de menor a mayor, y evitando las repeticiones. Para evitar errores en la escala, debemos incluir los valores intermedios aunque no haya ninguno en nuestros datos (en el ejemplo, el valor 16 que correspondería a los 160). Esto forma el “tallo” de nuestro diagrama:\n\nA continuación añadimos las “hojas” en la celda a la derecha, que consisten en los valores que hemos “cortado” de nuestro árbol, uno al lado de otro, incluyendo esta vez los valores repetidos, en orden de menor a mayor. Por ejemplo, para el valor 135, descartamos 13 y utlizamos 5; para el valor 138, descartamos 13 y utilizamos 8, y así sucesivamente para todos los valores.\n\n\n\n\nInterpretación del diagrama de tallo y hojas\n\nTallo: Los números a la izquierda del símbolo | representan los valores base (o tallos), en este caso, las decenas de las alturas.\nHojas: Los números a la derecha del símbolo | representan los dígitos adicionales (o hojas). Por ejemplo, en la línea 13 | 58, el tallo es 13 (130), y las hojas son 5 y 8, que corresponden a los datos 135 y 138.\n\nEl diagrama nos dice que los valores en torno a 150 cm son los más frecuentes, y que hay un valor alto (175) que se separa un poco del resto.\n\n\nResumen\nEl stemplot es muy sencillo de hacer y nos da una visión rápida y compacta de la distribución de nuestros valores, así como de la posible existencia de valores que se separan del conjunto. Estos valores alejados, que se conocen en inglés como outliers, tienen mucha importancia en el analisis e interpretación de los datos, como veremos más adelante.\nLa ventaja principal del stemplot es que mantiene los valores originales de las observaciones, y puede hacerse fácilmente con bolígrafo y papel, sin necesidad de más herramientas.\nSu principal inconveniente es la elaboración manual (aunque lenguajes como R tienen funciones que lo contruyen de forma automática), y por lo tanto, la dificultad de aplicarlo a volumenes de datos medios o grandes. El uso generalizado de los ordenadores ha hecho que actualmente esta herramienta tenga muy poco uso, y se utilicen en su lugar otras más gráficas y de construcción automática, como el histograma.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>La exploración de los datos mediante gráficos.</span>"
    ]
  }
]